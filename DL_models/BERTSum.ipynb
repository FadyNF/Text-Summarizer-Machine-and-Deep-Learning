{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCSG1-4rVb4U",
        "outputId": "48756856-91ac-414c-c2f8-63c9b7f13431"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "to8AGGJEVt7X"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import pandas as pd"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "Sfn9uf6jVu0_"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import re\n",
        "import nltk\n",
        "import spacy\n",
        "from tqdm import tqdm\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords, wordnet\n",
        "from nltk.tokenize import sent_tokenize\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk import pos_tag\n",
        "from difflib import SequenceMatcher\n",
        "\n",
        "# Download NLTK resources\n",
        "nltk.download(\"punkt\", quiet=True)\n",
        "nltk.download(\"stopwords\", quiet=True)\n",
        "nltk.download(\"wordnet\", quiet=True)\n",
        "nltk.download(\"averaged_perceptron_tagger\", quiet=True)\n",
        "nltk.download(\"averaged_perceptron_tagger_eng\", quiet=True)\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "\n",
        "def to_lowercase(text: str) -> str:\n",
        "    return text.lower()\n",
        "\n",
        "\n",
        "def remove_control_characters(text: str) -> str:\n",
        "    return re.sub(r\"[\\x00-\\x1f\\x7f-\\x9f\\u200b]\", \"\", text)\n",
        "\n",
        "\n",
        "def normalize_whitespace(text: str) -> str:\n",
        "    return re.sub(r\"\\s+\", \" \", text).strip()\n",
        "\n",
        "\n",
        "def tokenize_text(text: str) -> list:\n",
        "    return word_tokenize(text)\n",
        "\n",
        "\n",
        "def clean_tokens(tokens: list) -> list:\n",
        "    return [\n",
        "        t\n",
        "        for t in tokens\n",
        "        if t.isalpha() or (\".\" in t and len(t) > 1 and not t.strip(\".\") == \"\")\n",
        "    ]\n",
        "\n",
        "\n",
        "def remove_stopwords(tokens: list) -> list:\n",
        "    stop_words = set(stopwords.words(\"english\"))\n",
        "    return [t for t in tokens if t.lower() not in stop_words]\n",
        "\n",
        "\n",
        "def get_wordnet_pos(treebank_tag: str) -> str:\n",
        "    if treebank_tag.startswith(\"J\"):\n",
        "        return wordnet.ADJ\n",
        "    elif treebank_tag.startswith(\"V\"):\n",
        "        return wordnet.VERB\n",
        "    elif treebank_tag.startswith(\"N\"):\n",
        "        return wordnet.NOUN\n",
        "    elif treebank_tag.startswith(\"R\"):\n",
        "        return wordnet.ADV\n",
        "    return wordnet.NOUN\n",
        "\n",
        "\n",
        "def lemmatize_tokens(tokens: list) -> list:\n",
        "    lemmatizer = WordNetLemmatizer()\n",
        "    pos_tags = pos_tag(tokens)\n",
        "    return [lemmatizer.lemmatize(t, get_wordnet_pos(tag)) for t, tag in pos_tags]\n",
        "\n",
        "\n",
        "def join_tokens(tokens: list) -> str:\n",
        "    return \" \".join(tokens)\n",
        "\n",
        "\n",
        "def basic_preprocess(text: str) -> str:\n",
        "    if not isinstance(text, str):\n",
        "        return \"\"\n",
        "\n",
        "    text = to_lowercase(text)\n",
        "    text = remove_control_characters(text)\n",
        "    text = normalize_whitespace(text)\n",
        "    tokens = tokenize_text(text)\n",
        "    tokens = clean_tokens(tokens)\n",
        "    tokens = lemmatize_tokens(tokens)\n",
        "    tokens = remove_stopwords(tokens)\n",
        "\n",
        "    return join_tokens(tokens)\n",
        "\n",
        "\n",
        "def spacy_sent_tokenize(text: str) -> list:\n",
        "    \"\"\"\n",
        "    Splits text into sentences using spaCy.\n",
        "    Normalizes line breaks first.\n",
        "    \"\"\"\n",
        "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
        "    doc = nlp(cleaned_text)\n",
        "    return [sent.text.strip() for sent in doc.sents if sent.text.strip()]\n",
        "\n",
        "\n",
        "def is_summary_sentence(sent: str, summary_sents: list, threshold: float = 0.7) -> bool:\n",
        "    \"\"\"\n",
        "    Checks whether a sentence closely matches any sentence in the summary using SequenceMatcher.\n",
        "    \"\"\"\n",
        "    for summ_sent in summary_sents:\n",
        "        similarity = SequenceMatcher(None, sent, summ_sent).ratio()\n",
        "        if similarity >= threshold:\n",
        "            return True\n",
        "    return False\n",
        "\n",
        "\n",
        "def prepare_labeled_sentences_spacy(df) -> list:\n",
        "    all_data = []\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Preprocessing articles\"):\n",
        "        article = row[\"Article\"]\n",
        "        summary = row[\"Summary\"]\n",
        "\n",
        "        article_sents = spacy_sent_tokenize(article)\n",
        "        summary_sents = spacy_sent_tokenize(summary)\n",
        "\n",
        "        preprocessed_summary_sents = [basic_preprocess(s) for s in summary_sents]\n",
        "\n",
        "        for sent in article_sents:\n",
        "            raw_sentence = sent\n",
        "            preprocessed = basic_preprocess(sent)\n",
        "\n",
        "            label = int(\n",
        "                is_summary_sentence(\n",
        "                    preprocessed, preprocessed_summary_sents, threshold=0.6\n",
        "                )\n",
        "            )\n",
        "\n",
        "            all_data.append(\n",
        "                {\n",
        "                    \"article_id\": idx,\n",
        "                    \"raw_sentence\": raw_sentence,\n",
        "                    \"preprocessed_sentence\": preprocessed,\n",
        "                    \"label\": label,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return all_data\n",
        "\n",
        "\n",
        "def prepare_labeled_sentences(df) -> list:\n",
        "    all_data = []\n",
        "\n",
        "    for idx, row in tqdm(df.iterrows(), total=len(df), desc=\"Preprocessing articles\"):\n",
        "        article = row[\"Article\"]\n",
        "        summary = row[\"Summary\"]\n",
        "\n",
        "        article_sents = sent_tokenize(article)\n",
        "        summary_sents = sent_tokenize(summary)\n",
        "\n",
        "        preprocessed_summary_sents = [basic_preprocess(s) for s in summary_sents]\n",
        "\n",
        "        for sent in article_sents:\n",
        "            raw_sentence = sent\n",
        "            preprocessed = basic_preprocess(sent)\n",
        "\n",
        "            label = int(\n",
        "                is_summary_sentence(\n",
        "                    preprocessed, preprocessed_summary_sents, threshold=0.6\n",
        "                )\n",
        "            )\n",
        "\n",
        "            all_data.append(\n",
        "                {\n",
        "                    \"article_id\": idx,\n",
        "                    \"raw_sentence\": raw_sentence,\n",
        "                    \"preprocessed_sentence\": preprocessed,\n",
        "                    \"label\": label,\n",
        "                }\n",
        "            )\n",
        "\n",
        "    return all_data\n",
        "\n",
        "\n",
        "# ========== FILE HANDLING ==========\n",
        "\n",
        "\n",
        "def preprocess_document(input_path: str, output_path: str) -> bool:\n",
        "    if not os.path.exists(input_path):\n",
        "        print(f\"[Error] File not found at: {os.path.abspath(input_path)}\")\n",
        "        return False\n",
        "\n",
        "    try:\n",
        "        with open(input_path, \"r\", encoding=\"utf-8\") as f:\n",
        "            raw_text = f.read()\n",
        "\n",
        "        clean_text = basic_preprocess(raw_text)\n",
        "\n",
        "        with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
        "            f.write(clean_text)\n",
        "\n",
        "        print(f\"[Success] Output saved to: {output_path}\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"[Error] {e}\")\n",
        "        return False\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "id": "tZKRQq98Vxqk"
      },
      "outputs": [],
      "source": [
        "# BBC Dataset\n",
        "bbc_df = pd.read_csv(\"/content/drive/MyDrive/data/bbc/bbc_dataset.csv\")\n",
        "\n",
        "#IMDB Dataset\n",
        "imdb_df = pd.read_csv(\"/content/drive/MyDrive/data/imdb/imdb.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SZHva4XIVxoW",
        "outputId": "8e936ede-c929-4dc8-8e7a-cf6a3fa2b585"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading collection 'all'\n",
            "[nltk_data]    | \n",
            "[nltk_data]    | Downloading package abc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/abc.zip.\n",
            "[nltk_data]    | Downloading package alpino to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/alpino.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
            "[nltk_data]    |       to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_eng to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Package averaged_perceptron_tagger_eng is already\n",
            "[nltk_data]    |       up-to-date!\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_ru to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_ru.zip.\n",
            "[nltk_data]    | Downloading package averaged_perceptron_tagger_rus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/averaged_perceptron_tagger_rus.zip.\n",
            "[nltk_data]    | Downloading package basque_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/basque_grammars.zip.\n",
            "[nltk_data]    | Downloading package bcp47 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package biocreative_ppi to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/biocreative_ppi.zip.\n",
            "[nltk_data]    | Downloading package bllip_wsj_no_aux to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/bllip_wsj_no_aux.zip.\n",
            "[nltk_data]    | Downloading package book_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/book_grammars.zip.\n",
            "[nltk_data]    | Downloading package brown to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown.zip.\n",
            "[nltk_data]    | Downloading package brown_tei to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/brown_tei.zip.\n",
            "[nltk_data]    | Downloading package cess_cat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_cat.zip.\n",
            "[nltk_data]    | Downloading package cess_esp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cess_esp.zip.\n",
            "[nltk_data]    | Downloading package chat80 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/chat80.zip.\n",
            "[nltk_data]    | Downloading package city_database to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/city_database.zip.\n",
            "[nltk_data]    | Downloading package cmudict to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/cmudict.zip.\n",
            "[nltk_data]    | Downloading package comparative_sentences to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/comparative_sentences.zip.\n",
            "[nltk_data]    | Downloading package comtrans to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package conll2000 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2000.zip.\n",
            "[nltk_data]    | Downloading package conll2002 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/conll2002.zip.\n",
            "[nltk_data]    | Downloading package conll2007 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package crubadan to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/crubadan.zip.\n",
            "[nltk_data]    | Downloading package dependency_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dependency_treebank.zip.\n",
            "[nltk_data]    | Downloading package dolch to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/dolch.zip.\n",
            "[nltk_data]    | Downloading package english_wordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/english_wordnet.zip.\n",
            "[nltk_data]    | Downloading package europarl_raw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/europarl_raw.zip.\n",
            "[nltk_data]    | Downloading package extended_omw to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package floresta to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/floresta.zip.\n",
            "[nltk_data]    | Downloading package framenet_v15 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v15.zip.\n",
            "[nltk_data]    | Downloading package framenet_v17 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/framenet_v17.zip.\n",
            "[nltk_data]    | Downloading package gazetteers to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gazetteers.zip.\n",
            "[nltk_data]    | Downloading package genesis to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/genesis.zip.\n",
            "[nltk_data]    | Downloading package gutenberg to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/gutenberg.zip.\n",
            "[nltk_data]    | Downloading package ieer to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ieer.zip.\n",
            "[nltk_data]    | Downloading package inaugural to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/inaugural.zip.\n",
            "[nltk_data]    | Downloading package indian to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/indian.zip.\n",
            "[nltk_data]    | Downloading package jeita to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package kimmo to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/kimmo.zip.\n",
            "[nltk_data]    | Downloading package knbc to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package large_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/large_grammars.zip.\n",
            "[nltk_data]    | Downloading package lin_thesaurus to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/lin_thesaurus.zip.\n",
            "[nltk_data]    | Downloading package mac_morpho to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mac_morpho.zip.\n",
            "[nltk_data]    | Downloading package machado to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package masc_tagged to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker.zip.\n",
            "[nltk_data]    | Downloading package maxent_ne_chunker_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping chunkers/maxent_ne_chunker_tab.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/maxent_treebank_pos_tagger.zip.\n",
            "[nltk_data]    | Downloading package maxent_treebank_pos_tagger_tab to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping\n",
            "[nltk_data]    |       taggers/maxent_treebank_pos_tagger_tab.zip.\n",
            "[nltk_data]    | Downloading package moses_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/moses_sample.zip.\n",
            "[nltk_data]    | Downloading package movie_reviews to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/movie_reviews.zip.\n",
            "[nltk_data]    | Downloading package mte_teip5 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/mte_teip5.zip.\n",
            "[nltk_data]    | Downloading package mwa_ppdb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/mwa_ppdb.zip.\n",
            "[nltk_data]    | Downloading package names to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/names.zip.\n",
            "[nltk_data]    | Downloading package nombank.1.0 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package nonbreaking_prefixes to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nonbreaking_prefixes.zip.\n",
            "[nltk_data]    | Downloading package nps_chat to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/nps_chat.zip.\n",
            "[nltk_data]    | Downloading package omw to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package omw-1.4 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package opinion_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/opinion_lexicon.zip.\n",
            "[nltk_data]    | Downloading package panlex_swadesh to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package paradigms to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/paradigms.zip.\n",
            "[nltk_data]    | Downloading package pe08 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pe08.zip.\n",
            "[nltk_data]    | Downloading package perluniprops to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping misc/perluniprops.zip.\n",
            "[nltk_data]    | Downloading package pil to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pil.zip.\n",
            "[nltk_data]    | Downloading package pl196x to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pl196x.zip.\n",
            "[nltk_data]    | Downloading package porter_test to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/porter_test.zip.\n",
            "[nltk_data]    | Downloading package ppattach to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ppattach.zip.\n",
            "[nltk_data]    | Downloading package problem_reports to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/problem_reports.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_1 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_1.zip.\n",
            "[nltk_data]    | Downloading package product_reviews_2 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/product_reviews_2.zip.\n",
            "[nltk_data]    | Downloading package propbank to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package pros_cons to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/pros_cons.zip.\n",
            "[nltk_data]    | Downloading package ptb to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ptb.zip.\n",
            "[nltk_data]    | Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]    |   Package punkt is already up-to-date!\n",
            "[nltk_data]    | Downloading package punkt_tab to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping tokenizers/punkt_tab.zip.\n",
            "[nltk_data]    | Downloading package qc to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/qc.zip.\n",
            "[nltk_data]    | Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package rslp to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping stemmers/rslp.zip.\n",
            "[nltk_data]    | Downloading package rte to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/rte.zip.\n",
            "[nltk_data]    | Downloading package sample_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/sample_grammars.zip.\n",
            "[nltk_data]    | Downloading package semcor to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package senseval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/senseval.zip.\n",
            "[nltk_data]    | Downloading package sentence_polarity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentence_polarity.zip.\n",
            "[nltk_data]    | Downloading package sentiwordnet to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sentiwordnet.zip.\n",
            "[nltk_data]    | Downloading package shakespeare to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/shakespeare.zip.\n",
            "[nltk_data]    | Downloading package sinica_treebank to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/sinica_treebank.zip.\n",
            "[nltk_data]    | Downloading package smultron to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/smultron.zip.\n",
            "[nltk_data]    | Downloading package snowball_data to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package spanish_grammars to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping grammars/spanish_grammars.zip.\n",
            "[nltk_data]    | Downloading package state_union to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/state_union.zip.\n",
            "[nltk_data]    | Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]    |   Package stopwords is already up-to-date!\n",
            "[nltk_data]    | Downloading package subjectivity to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/subjectivity.zip.\n",
            "[nltk_data]    | Downloading package swadesh to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/swadesh.zip.\n",
            "[nltk_data]    | Downloading package switchboard to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/switchboard.zip.\n",
            "[nltk_data]    | Downloading package tagsets to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets.zip.\n",
            "[nltk_data]    | Downloading package tagsets_json to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping help/tagsets_json.zip.\n",
            "[nltk_data]    | Downloading package timit to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/timit.zip.\n",
            "[nltk_data]    | Downloading package toolbox to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/toolbox.zip.\n",
            "[nltk_data]    | Downloading package treebank to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/treebank.zip.\n",
            "[nltk_data]    | Downloading package twitter_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/twitter_samples.zip.\n",
            "[nltk_data]    | Downloading package udhr to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr.zip.\n",
            "[nltk_data]    | Downloading package udhr2 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/udhr2.zip.\n",
            "[nltk_data]    | Downloading package unicode_samples to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/unicode_samples.zip.\n",
            "[nltk_data]    | Downloading package universal_tagset to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping taggers/universal_tagset.zip.\n",
            "[nltk_data]    | Downloading package universal_treebanks_v20 to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package vader_lexicon to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    | Downloading package verbnet to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet.zip.\n",
            "[nltk_data]    | Downloading package verbnet3 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/verbnet3.zip.\n",
            "[nltk_data]    | Downloading package webtext to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/webtext.zip.\n",
            "[nltk_data]    | Downloading package wmt15_eval to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/wmt15_eval.zip.\n",
            "[nltk_data]    | Downloading package word2vec_sample to\n",
            "[nltk_data]    |     /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping models/word2vec_sample.zip.\n",
            "[nltk_data]    | Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]    |   Package wordnet is already up-to-date!\n",
            "[nltk_data]    | Downloading package wordnet2021 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet2022 to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet2022.zip.\n",
            "[nltk_data]    | Downloading package wordnet31 to /root/nltk_data...\n",
            "[nltk_data]    | Downloading package wordnet_ic to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/wordnet_ic.zip.\n",
            "[nltk_data]    | Downloading package words to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/words.zip.\n",
            "[nltk_data]    | Downloading package ycoe to /root/nltk_data...\n",
            "[nltk_data]    |   Unzipping corpora/ycoe.zip.\n",
            "[nltk_data]    | \n",
            "[nltk_data]  Done downloading collection all\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import nltk\n",
        "nltk.download('all')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zfFg5yfsi3Y",
        "outputId": "9461d78c-5309-402c-c40c-a361a22778b8"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preprocessing articles: 100%|██████████| 2225/2225 [05:54<00:00,  6.27it/s]\n"
          ]
        }
      ],
      "source": [
        "# Then process the BBC dataset\n",
        "bbc_labeled_data = prepare_labeled_sentences_spacy(bbc_df)\n",
        "\n",
        "# Convert to DataFrame for modeling\n",
        "bbc_processed_df = pd.DataFrame(\n",
        "    [\n",
        "        {\n",
        "            \"article_id\": item[\"article_id\"],\n",
        "            \"article_sentences\": item[\"raw_sentence\"],\n",
        "            \"preprocessed_sentence\": item[\"preprocessed_sentence\"],\n",
        "            \"label\": item[\"label\"],\n",
        "        }\n",
        "        for item in bbc_labeled_data\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "nhgFAlSXVxjt",
        "outputId": "7f9a40b2-7b74-47f9-afd9-29b419b674c9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary sentences: 16543 out of 41677 (39.69%)\n",
            "\n",
            "Example summary sentences:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"display(bbc_processed_df[bbc_processed_df['label'] == 1]\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"article_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 0,\n        \"max\": 0,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          0\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"article_sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"Ad sales boost Time Warner profit  Quarterly profits at US media giant TimeWarner jumped 76% to $1.13bn (\\u00c2\\u00a3600m) for the three months to December, from $639m year-earlier.\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"preprocessed_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"ad sale boost time warner profit quarterly profit u medium giant timewarner jump 1.13bn three month december\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-fc8472fd-5ae6-4b55-9d96-c137faed66ec\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_id</th>\n",
              "      <th>article_sentences</th>\n",
              "      <th>preprocessed_sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>Ad sales boost Time Warner profit  Quarterly p...</td>\n",
              "      <td>ad sale boost time warner profit quarterly pro...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>TimeWarner said fourth quarter sales rose 2% t...</td>\n",
              "      <td>timewarner say fourth quarter sale rise 11.1bn...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>0</td>\n",
              "      <td>It lost 464,000 subscribers in the fourth quar...</td>\n",
              "      <td>lose subscriber fourth quarter profit low prec...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-fc8472fd-5ae6-4b55-9d96-c137faed66ec')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-fc8472fd-5ae6-4b55-9d96-c137faed66ec button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-fc8472fd-5ae6-4b55-9d96-c137faed66ec');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-831c2065-344e-4ba3-8529-cf5e053f064e\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-831c2065-344e-4ba3-8529-cf5e053f064e')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-831c2065-344e-4ba3-8529-cf5e053f064e button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "   article_id                                  article_sentences  \\\n",
              "0           0  Ad sales boost Time Warner profit  Quarterly p...   \n",
              "2           0  TimeWarner said fourth quarter sales rose 2% t...   \n",
              "6           0  It lost 464,000 subscribers in the fourth quar...   \n",
              "\n",
              "                               preprocessed_sentence  label  \n",
              "0  ad sale boost time warner profit quarterly pro...      1  \n",
              "2  timewarner say fourth quarter sale rise 11.1bn...      1  \n",
              "6  lose subscriber fourth quarter profit low prec...      1  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Count how many sentences are labeled as summary sentences\n",
        "summary_count = bbc_processed_df['label'].sum()\n",
        "total_count = len(bbc_processed_df)\n",
        "print(f\"Summary sentences: {summary_count} out of {total_count} ({summary_count/total_count:.2%})\")\n",
        "\n",
        "# Show some examples of sentences included in summaries\n",
        "print(\"\\nExample summary sentences:\")\n",
        "display(bbc_processed_df[bbc_processed_df['label'] == 1].head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eIF4da8oVxhR",
        "outputId": "ec38795a-de5a-430b-ee01-2ff923ac7489"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preprocessing articles: 100%|██████████| 4000/4000 [04:11<00:00, 15.94it/s]\n"
          ]
        }
      ],
      "source": [
        "# Process the BBC dataset\n",
        "imdb_labeled_df = prepare_labeled_sentences_spacy(imdb_df[:4000])\n",
        "\n",
        "# Convert to DataFrame for modeling\n",
        "imdb_processed_df = pd.DataFrame(\n",
        "    [\n",
        "        {\n",
        "            \"article_id\": item[\"article_id\"],\n",
        "            \"article_sentences\": item[\"raw_sentence\"],\n",
        "            \"preprocessed_sentence\": item[\"preprocessed_sentence\"],\n",
        "            \"label\": item[\"label\"],\n",
        "        }\n",
        "        for item in imdb_labeled_df\n",
        "    ]\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 195
        },
        "id": "x5MojMfUVxex",
        "outputId": "c3383e4c-84fa-4c49-8290-5828a213c6b4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Summary sentences: 2934 out of 13024 (22.53%)\n",
            "\n",
            "Example summary sentences:\n"
          ]
        },
        {
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "summary": "{\n  \"name\": \"display(imdb_processed_df[imdb_processed_df['label'] == 1]\",\n  \"rows\": 3,\n  \"fields\": [\n    {\n      \"column\": \"article_id\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 1,\n        \"min\": 1,\n        \"max\": 4,\n        \"num_unique_values\": 3,\n        \"samples\": [\n          1,\n          3,\n          4\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"article_sentences\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"A wonderful little production The filming technique is very unassuming very old time BBC fashion and gives comforting and sometimes discomforting sense of realism to the entire piece The actors are extremely well chosen Michael Sheen not only has got all the polari\",\n          \"Basically there a family where little boy Jake thinks there a zombie in his closet his parents are fighting all the time This movie is slower than soap opera and suddenly Jake decides to become Rambo and kill the zombie OK first of all when you re going to make film you must Decide if its thriller or drama As drama the movie is watchable Parents are divorcing arguing like in real life\",\n          \"Petter Mattei Love in the Time of Money is visually stunning film to watch Mr Mattei offers us vivid portrait about human relations\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"preprocessed_sentence\",\n      \"properties\": {\n        \"dtype\": \"string\",\n        \"num_unique_values\": 3,\n        \"samples\": [\n          \"wonderful little production filming technique unassuming old time bbc fashion give comfort sometimes discomforting sense realism entire piece actor extremely well choose michael sheen get polari\",\n          \"basically family little boy jake think zombie closet parent fight time movie slow soap opera suddenly jake decides become rambo kill zombie ok first go make film must decide thriller drama drama movie watchable parent divorce argue like real life\",\n          \"petter mattei love time money visually stunning film watch mr mattei offer u vivid portrait human relation\"\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    },\n    {\n      \"column\": \"label\",\n      \"properties\": {\n        \"dtype\": \"number\",\n        \"std\": 0,\n        \"min\": 1,\n        \"max\": 1,\n        \"num_unique_values\": 1,\n        \"samples\": [\n          1\n        ],\n        \"semantic_type\": \"\",\n        \"description\": \"\"\n      }\n    }\n  ]\n}",
              "type": "dataframe"
            },
            "text/html": [
              "\n",
              "  <div id=\"df-7988bdfa-20a6-4810-b339-59c133036819\" class=\"colab-df-container\">\n",
              "    <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>article_id</th>\n",
              "      <th>article_sentences</th>\n",
              "      <th>preprocessed_sentence</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>1</td>\n",
              "      <td>A wonderful little production The filming tech...</td>\n",
              "      <td>wonderful little production filming technique ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>3</td>\n",
              "      <td>Basically there a family where little boy Jake...</td>\n",
              "      <td>basically family little boy jake think zombie ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>4</td>\n",
              "      <td>Petter Mattei Love in the Time of Money is vis...</td>\n",
              "      <td>petter mattei love time money visually stunnin...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "    <div class=\"colab-df-buttons\">\n",
              "\n",
              "  <div class=\"colab-df-container\">\n",
              "    <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-7988bdfa-20a6-4810-b339-59c133036819')\"\n",
              "            title=\"Convert this dataframe to an interactive table.\"\n",
              "            style=\"display:none;\">\n",
              "\n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\" viewBox=\"0 -960 960 960\">\n",
              "    <path d=\"M120-120v-720h720v720H120Zm60-500h600v-160H180v160Zm220 220h160v-160H400v160Zm0 220h160v-160H400v160ZM180-400h160v-160H180v160Zm440 0h160v-160H620v160ZM180-180h160v-160H180v160Zm440 0h160v-160H620v160Z\"/>\n",
              "  </svg>\n",
              "    </button>\n",
              "\n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    .colab-df-buttons div {\n",
              "      margin-bottom: 4px;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "    <script>\n",
              "      const buttonEl =\n",
              "        document.querySelector('#df-7988bdfa-20a6-4810-b339-59c133036819 button.colab-df-convert');\n",
              "      buttonEl.style.display =\n",
              "        google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "      async function convertToInteractive(key) {\n",
              "        const element = document.querySelector('#df-7988bdfa-20a6-4810-b339-59c133036819');\n",
              "        const dataTable =\n",
              "          await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                    [key], {});\n",
              "        if (!dataTable) return;\n",
              "\n",
              "        const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "          '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "          + ' to learn more about interactive tables.';\n",
              "        element.innerHTML = '';\n",
              "        dataTable['output_type'] = 'display_data';\n",
              "        await google.colab.output.renderOutput(dataTable, element);\n",
              "        const docLink = document.createElement('div');\n",
              "        docLink.innerHTML = docLinkHtml;\n",
              "        element.appendChild(docLink);\n",
              "      }\n",
              "    </script>\n",
              "  </div>\n",
              "\n",
              "\n",
              "    <div id=\"df-02444cbc-0be9-460c-a374-1d73ae8b5680\">\n",
              "      <button class=\"colab-df-quickchart\" onclick=\"quickchart('df-02444cbc-0be9-460c-a374-1d73ae8b5680')\"\n",
              "                title=\"Suggest charts\"\n",
              "                style=\"display:none;\">\n",
              "\n",
              "<svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "     width=\"24px\">\n",
              "    <g>\n",
              "        <path d=\"M19 3H5c-1.1 0-2 .9-2 2v14c0 1.1.9 2 2 2h14c1.1 0 2-.9 2-2V5c0-1.1-.9-2-2-2zM9 17H7v-7h2v7zm4 0h-2V7h2v10zm4 0h-2v-4h2v4z\"/>\n",
              "    </g>\n",
              "</svg>\n",
              "      </button>\n",
              "\n",
              "<style>\n",
              "  .colab-df-quickchart {\n",
              "      --bg-color: #E8F0FE;\n",
              "      --fill-color: #1967D2;\n",
              "      --hover-bg-color: #E2EBFA;\n",
              "      --hover-fill-color: #174EA6;\n",
              "      --disabled-fill-color: #AAA;\n",
              "      --disabled-bg-color: #DDD;\n",
              "  }\n",
              "\n",
              "  [theme=dark] .colab-df-quickchart {\n",
              "      --bg-color: #3B4455;\n",
              "      --fill-color: #D2E3FC;\n",
              "      --hover-bg-color: #434B5C;\n",
              "      --hover-fill-color: #FFFFFF;\n",
              "      --disabled-bg-color: #3B4455;\n",
              "      --disabled-fill-color: #666;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart {\n",
              "    background-color: var(--bg-color);\n",
              "    border: none;\n",
              "    border-radius: 50%;\n",
              "    cursor: pointer;\n",
              "    display: none;\n",
              "    fill: var(--fill-color);\n",
              "    height: 32px;\n",
              "    padding: 0;\n",
              "    width: 32px;\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart:hover {\n",
              "    background-color: var(--hover-bg-color);\n",
              "    box-shadow: 0 1px 2px rgba(60, 64, 67, 0.3), 0 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "    fill: var(--button-hover-fill-color);\n",
              "  }\n",
              "\n",
              "  .colab-df-quickchart-complete:disabled,\n",
              "  .colab-df-quickchart-complete:disabled:hover {\n",
              "    background-color: var(--disabled-bg-color);\n",
              "    fill: var(--disabled-fill-color);\n",
              "    box-shadow: none;\n",
              "  }\n",
              "\n",
              "  .colab-df-spinner {\n",
              "    border: 2px solid var(--fill-color);\n",
              "    border-color: transparent;\n",
              "    border-bottom-color: var(--fill-color);\n",
              "    animation:\n",
              "      spin 1s steps(1) infinite;\n",
              "  }\n",
              "\n",
              "  @keyframes spin {\n",
              "    0% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "      border-left-color: var(--fill-color);\n",
              "    }\n",
              "    20% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    30% {\n",
              "      border-color: transparent;\n",
              "      border-left-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    40% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-top-color: var(--fill-color);\n",
              "    }\n",
              "    60% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "    }\n",
              "    80% {\n",
              "      border-color: transparent;\n",
              "      border-right-color: var(--fill-color);\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "    90% {\n",
              "      border-color: transparent;\n",
              "      border-bottom-color: var(--fill-color);\n",
              "    }\n",
              "  }\n",
              "</style>\n",
              "\n",
              "      <script>\n",
              "        async function quickchart(key) {\n",
              "          const quickchartButtonEl =\n",
              "            document.querySelector('#' + key + ' button');\n",
              "          quickchartButtonEl.disabled = true;  // To prevent multiple clicks.\n",
              "          quickchartButtonEl.classList.add('colab-df-spinner');\n",
              "          try {\n",
              "            const charts = await google.colab.kernel.invokeFunction(\n",
              "                'suggestCharts', [key], {});\n",
              "          } catch (error) {\n",
              "            console.error('Error during call to suggestCharts:', error);\n",
              "          }\n",
              "          quickchartButtonEl.classList.remove('colab-df-spinner');\n",
              "          quickchartButtonEl.classList.add('colab-df-quickchart-complete');\n",
              "        }\n",
              "        (() => {\n",
              "          let quickchartButtonEl =\n",
              "            document.querySelector('#df-02444cbc-0be9-460c-a374-1d73ae8b5680 button');\n",
              "          quickchartButtonEl.style.display =\n",
              "            google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "        })();\n",
              "      </script>\n",
              "    </div>\n",
              "\n",
              "    </div>\n",
              "  </div>\n"
            ],
            "text/plain": [
              "    article_id                                  article_sentences  \\\n",
              "2            1  A wonderful little production The filming tech...   \n",
              "9            3  Basically there a family where little boy Jake...   \n",
              "11           4  Petter Mattei Love in the Time of Money is vis...   \n",
              "\n",
              "                                preprocessed_sentence  label  \n",
              "2   wonderful little production filming technique ...      1  \n",
              "9   basically family little boy jake think zombie ...      1  \n",
              "11  petter mattei love time money visually stunnin...      1  "
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Count how many sentences are labeled as summary sentences\n",
        "summary_count = imdb_processed_df['label'].sum()\n",
        "total_count = len(imdb_processed_df)\n",
        "print(f\"Summary sentences: {summary_count} out of {total_count} ({summary_count/total_count:.2%})\")\n",
        "\n",
        "# Show some examples of sentences included in summaries\n",
        "print(\"\\nExample summary sentences:\")\n",
        "display(imdb_processed_df[imdb_processed_df['label'] == 1].head(3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "metadata": {
        "id": "K7MkyubmVxaB"
      },
      "outputs": [],
      "source": [
        "from sklearn.utils import resample\n",
        "\n",
        "def balance_dataset(df):\n",
        "    df_majority = df[df.label == 0]\n",
        "    df_minority = df[df.label == 1]\n",
        "\n",
        "    df_minority_upsampled = resample(\n",
        "        df_minority, replace=True, n_samples=len(df_majority), random_state=42\n",
        "    )\n",
        "\n",
        "    return pd.concat([df_majority, df_minority_upsampled])\n",
        "\n",
        "\n",
        "# Balance both datasets\n",
        "bbc_balanced = balance_dataset(bbc_processed_df)\n",
        "imdb_balanced = balance_dataset(imdb_processed_df)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MnoDqWGpVxXj",
        "outputId": "33a6fe03-717b-4812-e304-0b687e1078a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting rouge_score\n",
            "  Downloading rouge_score-0.1.2.tar.gz (17 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.4.0)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.11/dist-packages (from rouge_score) (3.9.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from rouge_score) (2.0.2)\n",
            "Requirement already satisfied: six>=1.14.0 in /usr/local/lib/python3.11/dist-packages (from rouge_score) (1.17.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (8.2.0)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (1.5.0)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (2024.11.6)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from nltk->rouge_score) (4.67.1)\n",
            "Building wheels for collected packages: rouge_score\n",
            "  Building wheel for rouge_score (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for rouge_score: filename=rouge_score-0.1.2-py3-none-any.whl size=24934 sha256=eab35eb9216ce3b377d87f6996f278360813d68cd61d00104467e0de9ca2956f\n",
            "  Stored in directory: /root/.cache/pip/wheels/1e/19/43/8a442dc83660ca25e163e1bd1f89919284ab0d0c1475475148\n",
            "Successfully built rouge_score\n",
            "Installing collected packages: rouge_score\n",
            "Successfully installed rouge_score-0.1.2\n"
          ]
        }
      ],
      "source": [
        "# !pip install rouge_score"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "metadata": {
        "id": "l09f_knJVxU9"
      },
      "outputs": [],
      "source": [
        "# bertsum.py\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from transformers import BertTokenizer, BertModel\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from tqdm import tqdm\n",
        "from rouge_score import rouge_scorer\n",
        "\n",
        "class BertSumDataset(Dataset):\n",
        "    def __init__(self, df, tokenizer, max_len=128):\n",
        "        self.sentences = df['preprocessed_sentence'].tolist()\n",
        "        self.labels = df['label'].tolist()\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_len = max_len\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentences)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sentence = self.sentences[idx]\n",
        "        label = self.labels[idx]\n",
        "        encoding = self.tokenizer(\n",
        "            sentence,\n",
        "            padding='max_length',\n",
        "            truncation=True,\n",
        "            max_length=self.max_len,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = encoding['input_ids'].squeeze(0)\n",
        "        attention_mask = encoding['attention_mask'].squeeze(0)\n",
        "        return input_ids, attention_mask, torch.tensor(label, dtype=torch.float)\n",
        "\n",
        "class BertSumModel(nn.Module):\n",
        "    def __init__(self, dropout=0.3):\n",
        "        super(BertSumModel, self).__init__()\n",
        "        self.bert = BertModel.from_pretrained('bert-base-uncased')\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.classifier = nn.Linear(self.bert.config.hidden_size, 1)\n",
        "        self.sigmoid = nn.Sigmoid()\n",
        "\n",
        "    def forward(self, input_ids, attention_mask):\n",
        "        outputs = self.bert(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        pooled_output = outputs.pooler_output\n",
        "        x = self.dropout(pooled_output)\n",
        "        x = self.classifier(x)\n",
        "        return self.sigmoid(x)\n",
        "\n",
        "def train_bertsum(train_df, val_df, batch_size=4, epochs=2):\n",
        "    tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
        "    train_dataset = BertSumDataset(train_df, tokenizer)\n",
        "    val_dataset = BertSumDataset(val_df, tokenizer)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_dataset, batch_size=batch_size)\n",
        "\n",
        "    model = BertSumModel()\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model.to(device)\n",
        "\n",
        "    optimizer = torch.optim.Adam(model.parameters(), lr=2e-5)\n",
        "    loss_fn = nn.BCELoss()\n",
        "\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        total_loss = 0\n",
        "        for input_ids, attention_mask, labels in tqdm(train_loader, desc=f\"Epoch {epoch + 1}\"):\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            loss = loss_fn(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            total_loss += loss.item()\n",
        "        print(f\"Epoch {epoch + 1} Loss: {total_loss:.4f}\")\n",
        "\n",
        "    return model, tokenizer\n",
        "\n",
        "class BertSumTrainer:\n",
        "    def __init__(self, model, tokenizer, threshold=0.5):\n",
        "        self.model = model\n",
        "        self.tokenizer = tokenizer\n",
        "        self.threshold = threshold\n",
        "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "        self.model.to(self.device)\n",
        "        self.model.eval()\n",
        "\n",
        "    def summarize(self, sentences):\n",
        "        inputs = self.tokenizer(\n",
        "            sentences,\n",
        "            padding=True,\n",
        "            truncation=True,\n",
        "            return_tensors='pt'\n",
        "        )\n",
        "        input_ids = inputs['input_ids'].to(self.device)\n",
        "        attention_mask = inputs['attention_mask'].to(self.device)\n",
        "\n",
        "        with torch.no_grad():\n",
        "            probs = self.model(input_ids, attention_mask).squeeze(1)\n",
        "\n",
        "        selected_indices = (probs >= self.threshold).nonzero(as_tuple=True)[0].tolist()\n",
        "        summary_sentences = [sentences[i] for i in selected_indices]\n",
        "        return \" \".join(summary_sentences), probs.cpu().numpy()\n",
        "\n",
        "def evaluate_model(model, data_loader, tokenizer, original_sentences, threshold=0.5):\n",
        "    model.eval()\n",
        "    device = next(model.parameters()).device\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    scorer = rouge_scorer.RougeScorer(['rouge1', 'rouge2', 'rougeL'], use_stemmer=True)\n",
        "    rouge1_total = 0\n",
        "    rouge2_total = 0\n",
        "    rougeL_total = 0\n",
        "\n",
        "    idx = 0\n",
        "    with torch.no_grad():\n",
        "        for input_ids, attention_mask, labels in data_loader:\n",
        "            input_ids = input_ids.to(device)\n",
        "            attention_mask = attention_mask.to(device)\n",
        "            labels = labels.to(device).unsqueeze(1)\n",
        "\n",
        "            outputs = model(input_ids, attention_mask)\n",
        "            preds = (outputs >= threshold).float()\n",
        "            correct += (preds == labels).sum().item()\n",
        "            total += labels.size(0)\n",
        "\n",
        "            for i in range(len(labels)):\n",
        "                if idx >= len(original_sentences):\n",
        "                    break\n",
        "                original = original_sentences[idx]\n",
        "                predicted = original if preds[i].item() == 1.0 else \"\"\n",
        "                reference = original if labels[i].item() == 1.0 else \"\"\n",
        "                scores = scorer.score(reference, predicted)\n",
        "                rouge1_total += scores['rouge1'].fmeasure\n",
        "                rouge2_total += scores['rouge2'].fmeasure\n",
        "                rougeL_total += scores['rougeL'].fmeasure\n",
        "                idx += 1\n",
        "\n",
        "    accuracy = correct / total\n",
        "    n = min(idx, len(original_sentences))\n",
        "    avg_rouge1 = rouge1_total / n if n > 0 else 0\n",
        "    avg_rouge2 = rouge2_total / n if n > 0 else 0\n",
        "    avg_rougeL = rougeL_total / n if n > 0 else 0\n",
        "\n",
        "    return {\n",
        "        \"accuracy\": accuracy,\n",
        "        \"rouge1\": avg_rouge1,\n",
        "        \"rouge2\": avg_rouge2,\n",
        "        \"rougeL\": avg_rougeL\n",
        "    }\n",
        "\n",
        "def save_model(model, tokenizer, path_prefix=\"bertsum_model\"):\n",
        "    model_path = f\"{path_prefix}.pt\"\n",
        "    tokenizer_path = f\"{path_prefix}_tokenizer\"\n",
        "    torch.save(model.state_dict(), model_path)\n",
        "    tokenizer.save_pretrained(tokenizer_path)\n",
        "    print(f\"Model saved to {model_path}\")\n",
        "    print(f\"Tokenizer saved to {tokenizer_path}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DEVa58nHV_x6",
        "outputId": "e1cb5451-02a0-4bfb-cbda-672961ce1c7c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 1: 100%|██████████| 5027/5027 [16:12<00:00,  5.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 1 Loss: 3102.8417\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 2: 100%|██████████| 5027/5027 [16:12<00:00,  5.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 2 Loss: 2139.4816\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 3: 100%|██████████| 5027/5027 [16:12<00:00,  5.17it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 3 Loss: 1090.1288\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 4: 100%|██████████| 5027/5027 [16:11<00:00,  5.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 4 Loss: 597.3660\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Epoch 5: 100%|██████████| 5027/5027 [16:11<00:00,  5.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Epoch 5 Loss: 404.3644\n",
            "Train Accuracy: 0.9948, ROUGE-1: 0.4974, ROUGE-L: 0.4974\n",
            "Val Accuracy:   0.8297, ROUGE-1: 0.4358, ROUGE-L: 0.4358\n",
            "Test Accuracy:  0.8251, ROUGE-1: 0.4219, ROUGE-L: 0.4219\n"
          ]
        }
      ],
      "source": [
        "# === Split BBC into train, val, and test sets (80/10/10) ===\n",
        "from sklearn.model_selection import train_test_split\n",
        "from torch.utils.data import DataLoader\n",
        "\n",
        "# First: split into train (80%) and temp (20%)\n",
        "bbc_train, bbc_temp = train_test_split(bbc_balanced, test_size=0.2, random_state=42)\n",
        "\n",
        "# Then: split temp into val (10%) and test (10%)\n",
        "bbc_val, bbc_test = train_test_split(bbc_temp, test_size=0.5, random_state=42)\n",
        "\n",
        "# === Train the model ===\n",
        "bbc_model, bbc_tokenizer = train_bertsum(bbc_train, bbc_val, batch_size=8, epochs=5)\n",
        "\n",
        "# === Wrap in a trainer (optional for inference) ===\n",
        "bbc_trainer = BertSumTrainer(bbc_model, bbc_tokenizer)\n",
        "\n",
        "# === Create Datasets & Loaders ===\n",
        "train_dataset = BertSumDataset(bbc_train, bbc_tokenizer)\n",
        "val_dataset = BertSumDataset(bbc_val, bbc_tokenizer)\n",
        "test_dataset = BertSumDataset(bbc_test, bbc_tokenizer)\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=8, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=8)\n",
        "test_loader = DataLoader(test_dataset, batch_size=8)\n",
        "\n",
        "# === Evaluate ===\n",
        "train_metrics = evaluate_model(bbc_model, train_loader, bbc_tokenizer, bbc_train['preprocessed_sentence'].tolist())\n",
        "val_metrics = evaluate_model(bbc_model, val_loader, bbc_tokenizer, bbc_val['preprocessed_sentence'].tolist())\n",
        "test_metrics = evaluate_model(bbc_model, test_loader, bbc_tokenizer, bbc_test['preprocessed_sentence'].tolist())\n",
        "\n",
        "# === Print Results ===\n",
        "print(f\"Train Accuracy: {train_metrics['accuracy']:.4f}, ROUGE-1: {train_metrics['rouge1']:.4f}, ROUGE-L: {train_metrics['rougeL']:.4f}\")\n",
        "print(f\"Val Accuracy:   {val_metrics['accuracy']:.4f}, ROUGE-1: {val_metrics['rouge1']:.4f}, ROUGE-L: {val_metrics['rougeL']:.4f}\")\n",
        "print(f\"Test Accuracy:  {test_metrics['accuracy']:.4f}, ROUGE-1: {test_metrics['rouge1']:.4f}, ROUGE-L: {test_metrics['rougeL']:.4f}\")\n",
        "\n",
        "# === Save model and tokenizer ===\n",
        "save_model(bbc_model, bbc_tokenizer, path_prefix=\"bertsum_bbc\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SQt3OuHlV_v7",
        "outputId": "ca11c2ce-5d2a-4b4e-901a-6912a73b5286"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preprocessing articles: 100%|██████████| 1/1 [00:00<00:00,  7.47it/s]\n"
          ]
        }
      ],
      "source": [
        "# Alternative using your preprocessing pipeline\n",
        "sample_row = bbc_df.iloc[[1]]  # Get as DataFrame to match your processing function\n",
        "processed = prepare_labeled_sentences_spacy(sample_row)\n",
        "sample_sentences = [item['raw_sentence'] for item in processed]\n",
        "\n",
        "summary, probs = bbc_trainer.summarize(sample_sentences)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KkaaZ0l6V_t8",
        "outputId": "9361a41c-fbdc-49f1-b0b9-cb504b76426f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preprocessing articles: 100%|██████████| 1/1 [00:00<00:00,  6.54it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "=== ORIGINAL ARTICLE SENTENCES ===\n",
            "\n",
            "Sentence 1 (Score: 0.001):\n",
            "Dollar gains on Greenspan speech\n",
            "\n",
            "Sentence 2 (Score: 0.997):\n",
            "The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\n",
            "\n",
            "Sentence 3 (Score: 0.010):\n",
            "And Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it.\n",
            "\n",
            "Sentence 4 (Score: 0.037):\n",
            "In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday.\n",
            "\n",
            "Sentence 5 (Score: 0.295):\n",
            "Market concerns about the deficit has hit the greenback in recent months.\n",
            "\n",
            "Sentence 6 (Score: 0.631):\n",
            "On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data.\n",
            "\n",
            "Sentence 7 (Score: 0.030):\n",
            "\"I think the chairman's taking a much more sanguine view on the current account deficit than he's taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York.\n",
            "\n",
            "Sentence 8 (Score: 0.017):\n",
            "\"He's taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"\n",
            "\n",
            "Sentence 9 (Score: 0.419):\n",
            "Worries about the deficit concerns about China do, however, remain.\n",
            "\n",
            "Sentence 10 (Score: 0.812):\n",
            "China's currency remains pegged to the dollar and the US currency's sharp falls in recent months have therefore made Chinese export prices highly competitive.\n",
            "\n",
            "Sentence 11 (Score: 0.001):\n",
            "But calls for a shift in Beijing's policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg.\n",
            "\n",
            "Sentence 12 (Score: 0.442):\n",
            "The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy.\n",
            "\n",
            "Sentence 13 (Score: 0.014):\n",
            "In the meantime, the US Federal Reserve's decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates.\n",
            "\n",
            "Sentence 14 (Score: 0.002):\n",
            "The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar.\n",
            "\n",
            "Sentence 15 (Score: 0.007):\n",
            "The recent falls have partly been the result of big budget deficits, as well as the US's yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments.\n",
            "\n",
            "Sentence 16 (Score: 0.764):\n",
            "The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.\n",
            "\n",
            "=== GENERATED SUMMARY ===\n",
            "The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. China's currency remains pegged to the dollar and the US currency's sharp falls in recent months have therefore made Chinese export prices highly competitive. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.\n",
            "\n",
            "=== SUMMARY SENTENCES SELECTED ===\n",
            "\n",
            "Sentence 2 (Score: 0.997):\n",
            "The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\n",
            "\n",
            "Sentence 6 (Score: 0.631):\n",
            "On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data.\n",
            "\n",
            "Sentence 10 (Score: 0.812):\n",
            "China's currency remains pegged to the dollar and the US currency's sharp falls in recent months have therefore made Chinese export prices highly competitive.\n",
            "\n",
            "Sentence 16 (Score: 0.764):\n",
            "The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# Alternative using your preprocessing pipeline\n",
        "sample_row = bbc_df.iloc[[1]]  # Get as DataFrame to match your processing function\n",
        "processed = prepare_labeled_sentences_spacy(sample_row)\n",
        "sample_sentences = [item['raw_sentence'] for item in processed]\n",
        "\n",
        "# Generate summary\n",
        "summary, probs = bbc_trainer.summarize(sample_sentences)\n",
        "\n",
        "# Display results\n",
        "print(\"=== ORIGINAL ARTICLE SENTENCES ===\")\n",
        "for i, sent in enumerate(sample_sentences):\n",
        "    print(f\"\\nSentence {i+1} (Score: {probs[i]:.3f}):\")\n",
        "    print(sent)\n",
        "\n",
        "print(\"\\n=== GENERATED SUMMARY ===\")\n",
        "print(summary)\n",
        "\n",
        "print(\"\\n=== SUMMARY SENTENCES SELECTED ===\")\n",
        "for i, (sent, prob) in enumerate(zip(sample_sentences, probs)):\n",
        "    if prob >= bbc_trainer.threshold:  # Default threshold is 0.5\n",
        "        print(f\"\\nSentence {i+1} (Score: {prob:.3f}):\")\n",
        "        print(sent)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 20,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D_JYyHrzV_rC",
        "outputId": "ce4258f5-06aa-4295-9f70-45edffd1c902"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Preprocessing articles: 100%|██████████| 1/1 [00:00<00:00,  8.18it/s]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "ORIGINAL ARTICLE:\n",
            "================================================================================\n",
            "Dollar gains on Greenspan speech\n",
            "\n",
            "The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\n",
            "\n",
            "And Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it. In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday. Market concerns about the deficit has hit the greenback in recent months. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. \"I think the chairman's taking a much more sanguine view on the current account deficit than he's taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York. \"He's taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"\n",
            "\n",
            "Worries about the deficit concerns about China do, however, remain. China's currency remains pegged to the dollar and the US currency's sharp falls in recent months have therefore made Chinese export prices highly competitive. But calls for a shift in Beijing's policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg. The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy. In the meantime, the US Federal Reserve's decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates. The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar. The recent falls have partly been the result of big budget deficits, as well as the US's yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.\n",
            "\n",
            "================================================================================\n",
            "GENERATED SUMMARY:\n",
            "================================================================================\n",
            "The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise. On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data. China's currency remains pegged to the dollar and the US currency's sharp falls in recent months have therefore made Chinese export prices highly competitive. The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.\n",
            "\n",
            "================================================================================\n",
            "SENTENCE SCORES:\n",
            "================================================================================\n",
            "\n",
            "[EXCLUDED] Sentence 1 (Score: 0.001):\n",
            "Dollar gains on Greenspan speech\n",
            "\n",
            "[INCLUDED] Sentence 2 (Score: 0.997):\n",
            "The dollar has hit its highest level against the euro in almost three months after the Federal Reserve head said the US trade deficit is set to stabilise.\n",
            "\n",
            "[EXCLUDED] Sentence 3 (Score: 0.010):\n",
            "And Alan Greenspan highlighted the US government's willingness to curb spending and rising household savings as factors which may help to reduce it.\n",
            "\n",
            "[EXCLUDED] Sentence 4 (Score: 0.037):\n",
            "In late trading in New York, the dollar reached $1.2871 against the euro, from $1.2974 on Thursday.\n",
            "\n",
            "[EXCLUDED] Sentence 5 (Score: 0.295):\n",
            "Market concerns about the deficit has hit the greenback in recent months.\n",
            "\n",
            "[INCLUDED] Sentence 6 (Score: 0.631):\n",
            "On Friday, Federal Reserve chairman Mr Greenspan's speech in London ahead of the meeting of G7 finance ministers sent the dollar higher after it had earlier tumbled on the back of worse-than-expected US jobs data.\n",
            "\n",
            "[EXCLUDED] Sentence 7 (Score: 0.030):\n",
            "\"I think the chairman's taking a much more sanguine view on the current account deficit than he's taken for some time,\" said Robert Sinche, head of currency strategy at Bank of America in New York.\n",
            "\n",
            "[EXCLUDED] Sentence 8 (Score: 0.017):\n",
            "\"He's taking a longer-term view, laying out a set of conditions under which the current account deficit can improve this year and next.\"\n",
            "\n",
            "[EXCLUDED] Sentence 9 (Score: 0.419):\n",
            "Worries about the deficit concerns about China do, however, remain.\n",
            "\n",
            "[INCLUDED] Sentence 10 (Score: 0.812):\n",
            "China's currency remains pegged to the dollar and the US currency's sharp falls in recent months have therefore made Chinese export prices highly competitive.\n",
            "\n",
            "[EXCLUDED] Sentence 11 (Score: 0.001):\n",
            "But calls for a shift in Beijing's policy have fallen on deaf ears, despite recent comments in a major Chinese newspaper that the \"time is ripe\" for a loosening of the peg.\n",
            "\n",
            "[EXCLUDED] Sentence 12 (Score: 0.442):\n",
            "The G7 meeting is thought unlikely to produce any meaningful movement in Chinese policy.\n",
            "\n",
            "[EXCLUDED] Sentence 13 (Score: 0.014):\n",
            "In the meantime, the US Federal Reserve's decision on 2 February to boost interest rates by a quarter of a point - the sixth such move in as many months - has opened up a differential with European rates.\n",
            "\n",
            "[EXCLUDED] Sentence 14 (Score: 0.002):\n",
            "The half-point window, some believe, could be enough to keep US assets looking more attractive, and could help prop up the dollar.\n",
            "\n",
            "[EXCLUDED] Sentence 15 (Score: 0.007):\n",
            "The recent falls have partly been the result of big budget deficits, as well as the US's yawning current account gap, both of which need to be funded by the buying of US bonds and assets by foreign firms and governments.\n",
            "\n",
            "[INCLUDED] Sentence 16 (Score: 0.764):\n",
            "The White House will announce its budget on Monday, and many commentators believe the deficit will remain at close to half a trillion dollars.\n"
          ]
        }
      ],
      "source": [
        "# Get the original article and process it\n",
        "sample_row = bbc_df.iloc[[1]]  # Get second article as DataFrame\n",
        "original_text = sample_row['Article'].values[0]  # Changed from 'text' to 'Article'\n",
        "processed = prepare_labeled_sentences_spacy(sample_row)\n",
        "sample_sentences = [item['raw_sentence'] for item in processed]\n",
        "\n",
        "# Generate summary\n",
        "summary, probs = bbc_trainer.summarize(sample_sentences)\n",
        "\n",
        "# Display the original article\n",
        "print(\"=\"*80)\n",
        "print(\"ORIGINAL ARTICLE:\")\n",
        "print(\"=\"*80)\n",
        "print(original_text)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Display the generated summary\n",
        "print(\"GENERATED SUMMARY:\")\n",
        "print(\"=\"*80)\n",
        "print(summary)\n",
        "print(\"\\n\" + \"=\"*80)\n",
        "\n",
        "# Display sentence scores\n",
        "print(\"SENTENCE SCORES:\")\n",
        "print(\"=\"*80)\n",
        "for i, (sent, score) in enumerate(zip(sample_sentences, probs)):\n",
        "    included = \"[INCLUDED]\" if score >= bbc_trainer.threshold else \"[EXCLUDED]\"\n",
        "    print(f\"\\n{included} Sentence {i+1} (Score: {score:.3f}):\")\n",
        "    print(sent)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
