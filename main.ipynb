{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "89d0b769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mohamedkenya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     /Users/mohamedkenya/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     /Users/mohamedkenya/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/mohamedkenya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger_eng to\n",
      "[nltk_data]     /Users/mohamedkenya/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger_eng is already up-to-\n",
      "[nltk_data]       date!\n",
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/mohamedkenya/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from utils import prepare_labeled_sentences, prepare_labeled_sentences_spacy\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3f019",
   "metadata": {},
   "source": [
    "Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7d4506ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBC Dataset\n",
    "bbc_df = pd.read_csv(\"data/bbc/bbc_dataset.csv\")\n",
    "\n",
    "# CNN Datasets\n",
    "# cnn_train_df = pd.read_csv(\"data/cnn/cnn_dailymail_train.csv\")\n",
    "# cnn_valid_df = pd.read_csv(\"data/cnn/cnn_dailymail_valid.csv\")\n",
    "# cnn_test_df = pd.read_csv(\"data/cnn/cnn_dailymail_test.csv\")\n",
    "\n",
    "imdb_df = pd.read_csv(\"data/imdb/imdb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "01e05b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Musicians to tackle US red tape\\n\\nMusicians' ...</td>\n",
       "      <td>Nigel McCune from the Musicians' Union said Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U2's desire to be number one\\n\\nU2, who have w...</td>\n",
       "      <td>But they still want more.They have to want to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rocker Doherty in on-stage fight\\n\\nRock singe...</td>\n",
       "      <td>Babyshambles, which he formed after his acrimo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Snicket tops US box office chart\\n\\nThe film a...</td>\n",
       "      <td>A Series of Unfortunate Events also stars Scot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ocean's Twelve raids box office\\n\\nOcean's Twe...</td>\n",
       "      <td>Ocean's Twelve, the crime caper sequel starrin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  \\\n",
       "0  Musicians to tackle US red tape\\n\\nMusicians' ...   \n",
       "1  U2's desire to be number one\\n\\nU2, who have w...   \n",
       "2  Rocker Doherty in on-stage fight\\n\\nRock singe...   \n",
       "3  Snicket tops US box office chart\\n\\nThe film a...   \n",
       "4  Ocean's Twelve raids box office\\n\\nOcean's Twe...   \n",
       "\n",
       "                                             Summary  \n",
       "0  Nigel McCune from the Musicians' Union said Br...  \n",
       "1  But they still want more.They have to want to ...  \n",
       "2  Babyshambles, which he formed after his acrimo...  \n",
       "3  A Series of Unfortunate Events also stars Scot...  \n",
       "4  Ocean's Twelve, the crime caper sequel starrin...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview to confirm structure\n",
    "print(\"BBC Sample:\")\n",
    "display(bbc_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "851acf97",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(\"CNN Sample:\")\n",
    "# display(cnn_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8a3b0b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production The filming tech...</td>\n",
       "      <td>A wonderful little production The filming tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was wonderful way to spend time...</td>\n",
       "      <td>I thought it was proof that Woody Allen is sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there a family where little boy Jake...</td>\n",
       "      <td>Basically there a family where little boy Jake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei Love in the Time of Money is vis...</td>\n",
       "      <td>Petter Mattei Love in the Time of Money is vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production The filming tech...   \n",
       "2  I thought this was wonderful way to spend time...   \n",
       "3  Basically there a family where little boy Jake...   \n",
       "4  Petter Mattei Love in the Time of Money is vis...   \n",
       "\n",
       "                                             Summary  \n",
       "0  One of the other reviewers has mentioned that ...  \n",
       "1  A wonderful little production The filming tech...  \n",
       "2  I thought it was proof that Woody Allen is sti...  \n",
       "3  Basically there a family where little boy Jake...  \n",
       "4  Petter Mattei Love in the Time of Money is vis...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"IMDB Sample:\")\n",
    "display(imdb_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c1a94",
   "metadata": {},
   "source": [
    "Preprocess BBC Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b005f2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing articles: 100%|██████████| 2225/2225 [04:50<00:00,  7.66it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process the BBC dataset\n",
    "bbc_labeled_data = prepare_labeled_sentences_spacy(bbc_df)\n",
    "\n",
    "# Convert to DataFrame for modeling\n",
    "bbc_processed_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"article_id\": item[\"article_id\"],\n",
    "            \"article_sentences\": item[\"raw_sentence\"],\n",
    "            \"preprocessed_sentence\": item[\"preprocessed_sentence\"],\n",
    "            \"label\": item[\"label\"],\n",
    "        }\n",
    "        for item in bbc_labeled_data\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3309143b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41677, 4)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_processed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d6a9ee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary sentences: 16543 out of 41677 (39.69%)\n",
      "\n",
      "Example summary sentences:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_sentences</th>\n",
       "      <th>preprocessed_sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Musicians to tackle US red tape  Musicians' gr...</td>\n",
       "      <td>musician tackle u red tape musician group tack...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A singer hoping to perform in the US can expec...</td>\n",
       "      <td>singer hop perform u expect pay simply obtain ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nigel McCune from the Musicians' Union said Br...</td>\n",
       "      <td>nigel mccune musician union say british musici...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                  article_sentences  \\\n",
       "0           0  Musicians to tackle US red tape  Musicians' gr...   \n",
       "1           0  A singer hoping to perform in the US can expec...   \n",
       "4           0  Nigel McCune from the Musicians' Union said Br...   \n",
       "\n",
       "                               preprocessed_sentence  label  \n",
       "0  musician tackle u red tape musician group tack...      1  \n",
       "1  singer hop perform u expect pay simply obtain ...      1  \n",
       "4  nigel mccune musician union say british musici...      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count how many sentences are labeled as summary sentences\n",
    "summary_count = bbc_processed_df['label'].sum()\n",
    "total_count = len(bbc_processed_df)\n",
    "print(f\"Summary sentences: {summary_count} out of {total_count} ({summary_count/total_count:.2%})\")\n",
    "\n",
    "# Show some examples of sentences included in summaries\n",
    "print(\"\\nExample summary sentences:\")\n",
    "display(bbc_processed_df[bbc_processed_df['label'] == 1].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "65bf0a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_sentences</th>\n",
       "      <th>preprocessed_sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Musicians to tackle US red tape  Musicians' gr...</td>\n",
       "      <td>musician tackle u red tape musician group tack...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A singer hoping to perform in the US can expec...</td>\n",
       "      <td>singer hop perform u expect pay simply obtain ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Groups including the Musicians' Union are call...</td>\n",
       "      <td>group include musician union call end raw deal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>US acts are not faced with comparable expense ...</td>\n",
       "      <td>u act face comparable expense bureaucracy visi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nigel McCune from the Musicians' Union said Br...</td>\n",
       "      <td>nigel mccune musician union say british musici...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>A sponsor has to make a petition on their beha...</td>\n",
       "      <td>sponsor make petition behalf form amount nearl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>\"If you make a mistake on your form, you risk ...</td>\n",
       "      <td>make mistake form risk ban thus ability career...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>\"The US is the world's biggest music market, w...</td>\n",
       "      <td>u world big music market mean something creaky...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>\"The current situation is preventing British a...</td>\n",
       "      <td>current situation prevent british act maintain...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>The Musicians' Union stance is being endorsed ...</td>\n",
       "      <td>musician union stance endorse music manager fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>The MMF's general secretary James Seller said:...</td>\n",
       "      <td>mmf general secretary james seller say imagine...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>Every member would have to travel to London to...</td>\n",
       "      <td>every member would travel london visa process</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>\"The US market is seen as the holy grail and o...</td>\n",
       "      <td>u market see holy grail one benchmark success ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>\"It's still very important, but there are othe...</td>\n",
       "      <td>still important market like europe india china...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>A Department for Media, Culture and Sport spok...</td>\n",
       "      <td>department medium culture sport spokeswoman sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>A US Embassy spokesman said: \"We are aware tha...</td>\n",
       "      <td>u embassy spokesman say aware entertainer requ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>\"We are aware of the importance of cultural ex...</td>\n",
       "      <td>aware importance cultural exchange best facili...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>U2's desire to be number one  U2, who have won...</td>\n",
       "      <td>desire number one win three prestigious grammy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>The most popular groups in the history of rock...</td>\n",
       "      <td>popular group history rock several thing common</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>The music must be inspired and appeal across g...</td>\n",
       "      <td>music must inspire appeal across generation di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>But such success is down to more than music.</td>\n",
       "      <td>success music</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>They have to be compelling performers, charism...</td>\n",
       "      <td>compel performer charismatic intelligent enoug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>They also have to want it.</td>\n",
       "      <td>also want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>They have to want to be the biggest band ever ...</td>\n",
       "      <td>want big band ever stop want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>The Beatles had it, the Rolling Stones still h...</td>\n",
       "      <td>beatles roll stone still rem hold onto queen c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>And U2 have it in spades, and keep churning it...</td>\n",
       "      <td>spade keep churn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>Their new album, How To Dismantle An Atomic Bo...</td>\n",
       "      <td>new album dismantle atomic bomb come year scho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>They may have lost some of the edginess and ra...</td>\n",
       "      <td>may lose edginess raw youthful force propel to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>Vertigo, the first single from the new album, ...</td>\n",
       "      <td>vertigo first single new album go straight uk ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>\"The challenge is to be bigger and bolder and ...</td>\n",
       "      <td>challenge big bolder good make record whole wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>Drummer Larry Mullen Jr echoed those sentiment...</td>\n",
       "      <td>drummer larry mullen jr echoed sentiment compe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>We don't want to be thought of as a veteran ba...</td>\n",
       "      <td>want think veteran band</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>The band have done \"everything in their consid...</td>\n",
       "      <td>band everything considerable power ensure rema...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>\"This makes them hugely determined and formida...</td>\n",
       "      <td>make hugely determine formidable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>He added: \"They are equally determined to push...</td>\n",
       "      <td>add equally determine push make music continue...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>\"As such, they've constantly re-invented and c...</td>\n",
       "      <td>constantly challenge</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>They are, perhaps, alone as the only rock band...</td>\n",
       "      <td>perhaps alone rock band get well age</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>The other key ingredient was the fact they wer...</td>\n",
       "      <td>key ingredient fact highly organise mr rees say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>\"They do everything in the right way.\"</td>\n",
       "      <td>everything right way</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>The group were born when Mullen put an appeal ...</td>\n",
       "      <td>group bear mullen put appeal bandmates high sc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>Dick Evans soon dropped out and the four-piece...</td>\n",
       "      <td>dick evans soon drop know feedback hype settle</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>By 1978, they had won a talent contest and got...</td>\n",
       "      <td>win talent contest get notice manager paul mcg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>\"They were brilliant, but very coarse,\" McGuin...</td>\n",
       "      <td>brilliant coarse mcguinness recently say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>\"In a way, they were doing exactly what they d...</td>\n",
       "      <td>way exactly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>Only badly.\"</td>\n",
       "      <td>badly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>They struggled to attract record company atten...</td>\n",
       "      <td>struggle attract record company attention late...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>They released two Ireland-only singles, which ...</td>\n",
       "      <td>release two single top national chart lead dea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>The stadium-filling, anthemic sound was U2's a...</td>\n",
       "      <td>anthemic sound aim start third album war saw m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>Songs like Sunday Bloody Sunday and New Year's...</td>\n",
       "      <td>song like sunday bloody sunday new year day br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>His stage performances - which included flag-w...</td>\n",
       "      <td>stage performance include earn reputation elec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>In 1987, The Joshua Tree broke sales records a...</td>\n",
       "      <td>joshua tree break sale record saw band reach h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>Those songs took the band's epic, atmospheric ...</td>\n",
       "      <td>song take band epic atmospheric sound simple p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>The end of the decade marked a crucial point f...</td>\n",
       "      <td>end decade mark crucial point band reach top s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>These came in the form of explorations of diff...</td>\n",
       "      <td>come form exploration different branch rock fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>The Achtung Baby album in 1991 was followed by...</td>\n",
       "      <td>achtung baby album follow zooropa pop correspo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>He was also building a parallel reputation - n...</td>\n",
       "      <td>also build parallel reputation always pleasure...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>Before the release of How To Dismantle An Atom...</td>\n",
       "      <td>release dismantle atomic bomb sell million alb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>But they still want more.</td>\n",
       "      <td>still want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2</td>\n",
       "      <td>Rocker Doherty in on-stage fight  Rock singer ...</td>\n",
       "      <td>rocker doherty fight rock singer pete doherty ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2</td>\n",
       "      <td>Babyshambles played for 5,000 fans at London's...</td>\n",
       "      <td>babyshambles play fan london brixton academy t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    article_id                                  article_sentences  \\\n",
       "0            0  Musicians to tackle US red tape  Musicians' gr...   \n",
       "1            0  A singer hoping to perform in the US can expec...   \n",
       "2            0  Groups including the Musicians' Union are call...   \n",
       "3            0  US acts are not faced with comparable expense ...   \n",
       "4            0  Nigel McCune from the Musicians' Union said Br...   \n",
       "5            0  A sponsor has to make a petition on their beha...   \n",
       "6            0  \"If you make a mistake on your form, you risk ...   \n",
       "7            0  \"The US is the world's biggest music market, w...   \n",
       "8            0  \"The current situation is preventing British a...   \n",
       "9            0  The Musicians' Union stance is being endorsed ...   \n",
       "10           0  The MMF's general secretary James Seller said:...   \n",
       "11           0  Every member would have to travel to London to...   \n",
       "12           0  \"The US market is seen as the holy grail and o...   \n",
       "13           0  \"It's still very important, but there are othe...   \n",
       "14           0  A Department for Media, Culture and Sport spok...   \n",
       "15           0  A US Embassy spokesman said: \"We are aware tha...   \n",
       "16           0  \"We are aware of the importance of cultural ex...   \n",
       "17           1  U2's desire to be number one  U2, who have won...   \n",
       "18           1  The most popular groups in the history of rock...   \n",
       "19           1  The music must be inspired and appeal across g...   \n",
       "20           1       But such success is down to more than music.   \n",
       "21           1  They have to be compelling performers, charism...   \n",
       "22           1                         They also have to want it.   \n",
       "23           1  They have to want to be the biggest band ever ...   \n",
       "24           1  The Beatles had it, the Rolling Stones still h...   \n",
       "25           1  And U2 have it in spades, and keep churning it...   \n",
       "26           1  Their new album, How To Dismantle An Atomic Bo...   \n",
       "27           1  They may have lost some of the edginess and ra...   \n",
       "28           1  Vertigo, the first single from the new album, ...   \n",
       "29           1  \"The challenge is to be bigger and bolder and ...   \n",
       "30           1  Drummer Larry Mullen Jr echoed those sentiment...   \n",
       "31           1  We don't want to be thought of as a veteran ba...   \n",
       "32           1  The band have done \"everything in their consid...   \n",
       "33           1  \"This makes them hugely determined and formida...   \n",
       "34           1  He added: \"They are equally determined to push...   \n",
       "35           1  \"As such, they've constantly re-invented and c...   \n",
       "36           1  They are, perhaps, alone as the only rock band...   \n",
       "37           1  The other key ingredient was the fact they wer...   \n",
       "38           1             \"They do everything in the right way.\"   \n",
       "39           1  The group were born when Mullen put an appeal ...   \n",
       "40           1  Dick Evans soon dropped out and the four-piece...   \n",
       "41           1  By 1978, they had won a talent contest and got...   \n",
       "42           1  \"They were brilliant, but very coarse,\" McGuin...   \n",
       "43           1  \"In a way, they were doing exactly what they d...   \n",
       "44           1                                       Only badly.\"   \n",
       "45           1  They struggled to attract record company atten...   \n",
       "46           1  They released two Ireland-only singles, which ...   \n",
       "47           1  The stadium-filling, anthemic sound was U2's a...   \n",
       "48           1  Songs like Sunday Bloody Sunday and New Year's...   \n",
       "49           1  His stage performances - which included flag-w...   \n",
       "50           1  In 1987, The Joshua Tree broke sales records a...   \n",
       "51           1  Those songs took the band's epic, atmospheric ...   \n",
       "52           1  The end of the decade marked a crucial point f...   \n",
       "53           1  These came in the form of explorations of diff...   \n",
       "54           1  The Achtung Baby album in 1991 was followed by...   \n",
       "55           1  He was also building a parallel reputation - n...   \n",
       "56           1  Before the release of How To Dismantle An Atom...   \n",
       "57           1                          But they still want more.   \n",
       "58           2  Rocker Doherty in on-stage fight  Rock singer ...   \n",
       "59           2  Babyshambles played for 5,000 fans at London's...   \n",
       "\n",
       "                                preprocessed_sentence  label  \n",
       "0   musician tackle u red tape musician group tack...      1  \n",
       "1   singer hop perform u expect pay simply obtain ...      1  \n",
       "2   group include musician union call end raw deal...      0  \n",
       "3   u act face comparable expense bureaucracy visi...      0  \n",
       "4   nigel mccune musician union say british musici...      1  \n",
       "5   sponsor make petition behalf form amount nearl...      0  \n",
       "6   make mistake form risk ban thus ability career...      0  \n",
       "7   u world big music market mean something creaky...      1  \n",
       "8   current situation prevent british act maintain...      1  \n",
       "9   musician union stance endorse music manager fo...      1  \n",
       "10  mmf general secretary james seller say imagine...      0  \n",
       "11      every member would travel london visa process      0  \n",
       "12  u market see holy grail one benchmark success ...      0  \n",
       "13  still important market like europe india china...      0  \n",
       "14  department medium culture sport spokeswoman sa...      0  \n",
       "15  u embassy spokesman say aware entertainer requ...      1  \n",
       "16  aware importance cultural exchange best facili...      0  \n",
       "17  desire number one win three prestigious grammy...      1  \n",
       "18    popular group history rock several thing common      0  \n",
       "19  music must inspire appeal across generation di...      0  \n",
       "20                                      success music      0  \n",
       "21  compel performer charismatic intelligent enoug...      0  \n",
       "22                                          also want      1  \n",
       "23                       want big band ever stop want      1  \n",
       "24  beatles roll stone still rem hold onto queen c...      0  \n",
       "25                                   spade keep churn      0  \n",
       "26  new album dismantle atomic bomb come year scho...      1  \n",
       "27  may lose edginess raw youthful force propel to...      0  \n",
       "28  vertigo first single new album go straight uk ...      1  \n",
       "29  challenge big bolder good make record whole wo...      1  \n",
       "30  drummer larry mullen jr echoed sentiment compe...      0  \n",
       "31                            want think veteran band      1  \n",
       "32  band everything considerable power ensure rema...      1  \n",
       "33                   make hugely determine formidable      0  \n",
       "34  add equally determine push make music continue...      0  \n",
       "35                               constantly challenge      0  \n",
       "36               perhaps alone rock band get well age      1  \n",
       "37    key ingredient fact highly organise mr rees say      0  \n",
       "38                               everything right way      0  \n",
       "39  group bear mullen put appeal bandmates high sc...      0  \n",
       "40     dick evans soon drop know feedback hype settle      0  \n",
       "41  win talent contest get notice manager paul mcg...      0  \n",
       "42           brilliant coarse mcguinness recently say      0  \n",
       "43                                        way exactly      0  \n",
       "44                                              badly      0  \n",
       "45  struggle attract record company attention late...      0  \n",
       "46  release two single top national chart lead dea...      0  \n",
       "47  anthemic sound aim start third album war saw m...      1  \n",
       "48  song like sunday bloody sunday new year day br...      1  \n",
       "49  stage performance include earn reputation elec...      0  \n",
       "50  joshua tree break sale record saw band reach h...      1  \n",
       "51  song take band epic atmospheric sound simple p...      0  \n",
       "52  end decade mark crucial point band reach top s...      1  \n",
       "53  come form exploration different branch rock fo...      1  \n",
       "54  achtung baby album follow zooropa pop correspo...      0  \n",
       "55  also build parallel reputation always pleasure...      0  \n",
       "56  release dismantle atomic bomb sell million alb...      1  \n",
       "57                                         still want      1  \n",
       "58  rocker doherty fight rock singer pete doherty ...      1  \n",
       "59  babyshambles play fan london brixton academy t...      1  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_processed_df.head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa7f591",
   "metadata": {},
   "source": [
    "Preprocessed IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "4ac0d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing articles:   0%|          | 0/4000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing articles: 100%|██████████| 4000/4000 [03:28<00:00, 19.19it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process the BBC dataset\n",
    "imdb_labeled_df = prepare_labeled_sentences_spacy(imdb_df[:4000])\n",
    "\n",
    "# Convert to DataFrame for modeling\n",
    "imdb_processed_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"article_id\": item[\"article_id\"],\n",
    "            \"article_sentences\": item[\"raw_sentence\"],\n",
    "            \"preprocessed_sentence\": item[\"preprocessed_sentence\"],\n",
    "            \"label\": item[\"label\"],\n",
    "        }\n",
    "        for item in imdb_labeled_df\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a67dd484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13024, 4)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_processed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "64901be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary sentences: 2934 out of 13024 (22.53%)\n",
      "\n",
      "Example summary sentences:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_sentences</th>\n",
       "      <th>preprocessed_sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production The filming tech...</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>Basically there a family where little boy Jake...</td>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei Love in the Time of Money is vis...</td>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    article_id                                  article_sentences  \\\n",
       "2            1  A wonderful little production The filming tech...   \n",
       "9            3  Basically there a family where little boy Jake...   \n",
       "11           4  Petter Mattei Love in the Time of Money is vis...   \n",
       "\n",
       "                                preprocessed_sentence  label  \n",
       "2   wonderful little production filming technique ...      1  \n",
       "9   basically family little boy jake think zombie ...      1  \n",
       "11  petter mattei love time money visually stunnin...      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count how many sentences are labeled as summary sentences\n",
    "summary_count = imdb_processed_df['label'].sum()\n",
    "total_count = len(imdb_processed_df)\n",
    "print(f\"Summary sentences: {summary_count} out of {total_count} ({summary_count/total_count:.2%})\")\n",
    "\n",
    "# Show some examples of sentences included in summaries\n",
    "print(\"\\nExample summary sentences:\")\n",
    "display(imdb_processed_df[imdb_processed_df['label'] == 1].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2a84aae2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(imdb_processed_df[\"raw_sentence\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e1a68bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_sentences</th>\n",
       "      <th>preprocessed_sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>one reviewer mention watch oz episode hook rig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>This show pulls no punches with regards to dru...</td>\n",
       "      <td>show pull punch regard drug sex violence hardc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production The filming tech...</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>but he has all the voices down pat too You can...</td>\n",
       "      <td>voice pat truly see seamless edit guide refere...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>but it is terrificly written and performed pie...</td>\n",
       "      <td>terrificly write perform piece masterful produ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>The realism really comes home with the little ...</td>\n",
       "      <td>realism really come home little thing fantasy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was wonderful way to spend time...</td>\n",
       "      <td>think wonderful way spend time hot summer week...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>The plot is simplistic but the dialogue is wit...</td>\n",
       "      <td>plot simplistic dialogue witty character likab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>While some may be disappointed when they reali...</td>\n",
       "      <td>may disappoint realize match point risk addict...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>Basically there a family where little boy Jake...</td>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>And then we have Jake with his closet which to...</td>\n",
       "      <td>jake closet totally ruin film expect see booge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei Love in the Time of Money is vis...</td>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>This is movie that seems to be telling us what...</td>\n",
       "      <td>movie seem tell u money power success people d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>Kane Michael Imperioli Adrian Grenier and the ...</td>\n",
       "      <td>kane michael imperioli adrian grenier rest tal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>We wish Mr Mattei good luck and await anxiousl...</td>\n",
       "      <td>wish mr mattei good luck await anxiously next ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>Probably my all time favorite movie story of s...</td>\n",
       "      <td>probably time favorite movie story selflessnes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>The kids are as grandma says more like dressed...</td>\n",
       "      <td>kid grandma say like dress midget child make f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>I sure would like to see resurrection of up da...</td>\n",
       "      <td>sure would like see resurrection dated seahunt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>Oh by the way thank you for an outlet like thi...</td>\n",
       "      <td>oh way thank outlet like view many viewpoint t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>So any ole way believe ve got what wanna say W...</td>\n",
       "      <td>ole way believe get wan na say would nice read...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7</td>\n",
       "      <td>This show was an amazing fresh innovative idea...</td>\n",
       "      <td>show amazing fresh innovative idea first air f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>encourage positive comment film look forward w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9</td>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>like original gut wrench laughter like movie y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10</td>\n",
       "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
       "      <td>phil alien one quirky film humour base around ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>At first it was very odd and pretty funny but ...</td>\n",
       "      <td>first odd pretty funny movie progress find jok...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>anymore</td>\n",
       "      <td>anymore</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>Its low budget film thats never problem in its...</td>\n",
       "      <td>low budget film thats never problem pretty int...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11</td>\n",
       "      <td>I saw this movie when was about when it came o...</td>\n",
       "      <td>saw movie come recall scary scene big bird eat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11</td>\n",
       "      <td>The horror As young kid going to these cheesy ...</td>\n",
       "      <td>horror young kid go cheesy film saturday after...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12</td>\n",
       "      <td>So im not big fan of Boll work but then again ...</td>\n",
       "      <td>im big fan boll work many enjoy movie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>12</td>\n",
       "      <td>Postal maybe im the only one Boll apparently b...</td>\n",
       "      <td>postal maybe im one boll apparently buy right ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>12</td>\n",
       "      <td>So the tale goes like this Jack Carver played ...</td>\n",
       "      <td>tale go like jack carver play til schweiger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>12</td>\n",
       "      <td>yes Carver is German all hail the bratwurst ea...</td>\n",
       "      <td>yes carver german hail bratwurst eat dude howe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>12</td>\n",
       "      <td>but we only saw carver in first person perspec...</td>\n",
       "      <td>saw carver first person perspective</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>12</td>\n",
       "      <td>so we don really know what he looked like when...</td>\n",
       "      <td>really know look like kick however storyline f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>12</td>\n",
       "      <td>We see the evil mad scientist Dr Krieger playe...</td>\n",
       "      <td>see evil mad scientist dr krieger play udo kie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>13</td>\n",
       "      <td>The cast played Shakespeare Shakespeare lost a...</td>\n",
       "      <td>cast play shakespeare shakespeare lose appreci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>14</td>\n",
       "      <td>This fantastic movie of three prisoners who be...</td>\n",
       "      <td>fantastic movie three prisoner become famous o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>14</td>\n",
       "      <td>but this roll is not bad Another good thing ab...</td>\n",
       "      <td>roll bad another good thing movie soundtrack m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15</td>\n",
       "      <td>Kind of drawn in by the erotic scenes only to ...</td>\n",
       "      <td>kind drawn erotic scene realize one amateurish...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>15</td>\n",
       "      <td>What was with the bisexual relationship out of...</td>\n",
       "      <td>bisexual relationship nowhere heterosexual enc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>15</td>\n",
       "      <td>And what was with that absurd dance with every...</td>\n",
       "      <td>absurd dance everybody play stereotyped role g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>16</td>\n",
       "      <td>Some films just simply should not be remade Th...</td>\n",
       "      <td>film simply remake one bad film fail capture f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>16</td>\n",
       "      <td>But you will enjoy the friction of terror in t...</td>\n",
       "      <td>enjoy friction terror old version much</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>17</td>\n",
       "      <td>This movie made it into one of my top most awf...</td>\n",
       "      <td>movie make one top awful movie horrible contin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>17</td>\n",
       "      <td>The ghost scene at the end was stolen from the...</td>\n",
       "      <td>ghost scene end steal final scene old star war...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>17</td>\n",
       "      <td>hello</td>\n",
       "      <td>hello</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>17</td>\n",
       "      <td>And the whole machine vs humans theme WAS the ...</td>\n",
       "      <td>whole machine v human theme matrix terminator ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>18</td>\n",
       "      <td>I remember this film it was the first film had...</td>\n",
       "      <td>remember film first film watch cinema picture ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>19</td>\n",
       "      <td>An awful film It must have been up against som...</td>\n",
       "      <td>awful film must real stinker nominate golden g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>19</td>\n",
       "      <td>They ve taken the story of the first famous fe...</td>\n",
       "      <td>take story first famous female renaissance pai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>19</td>\n",
       "      <td>My complaint is not that they ve taken liberti...</td>\n",
       "      <td>complaint take liberty fact story good would p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>19</td>\n",
       "      <td>But it simply bizarre by all accounts the true...</td>\n",
       "      <td>simply bizarre account true story artist would...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>19</td>\n",
       "      <td>so why did they come up with this dishwater du...</td>\n",
       "      <td>come dishwater dull script suppose enough nake...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>20</td>\n",
       "      <td>After the success of Die Hard and it sequels i...</td>\n",
       "      <td>success die hard sequels surprise really glut ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>20</td>\n",
       "      <td>However if you an forget all the nonsense it a...</td>\n",
       "      <td>however forget nonsense actually lovable unden...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>20</td>\n",
       "      <td>And whilst he surely can be it really does loo...</td>\n",
       "      <td>whilst surely really look like ralph waite fra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>20</td>\n",
       "      <td>yes you can help enjoy that bit Hal needed goo...</td>\n",
       "      <td>yes help enjoy bit hal need good kicking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>20</td>\n",
       "      <td>So forget your better judgement who cares if t...</td>\n",
       "      <td>forget good judgement care could never happen ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>20</td>\n",
       "      <td>And if you re looking for Qaulen he the one we...</td>\n",
       "      <td>look qaulen one wear helicopter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    article_id                                  article_sentences  \\\n",
       "0            0  One of the other reviewers has mentioned that ...   \n",
       "1            0  This show pulls no punches with regards to dru...   \n",
       "2            1  A wonderful little production The filming tech...   \n",
       "3            1  but he has all the voices down pat too You can...   \n",
       "4            1  but it is terrificly written and performed pie...   \n",
       "5            1  The realism really comes home with the little ...   \n",
       "6            2  I thought this was wonderful way to spend time...   \n",
       "7            2  The plot is simplistic but the dialogue is wit...   \n",
       "8            2  While some may be disappointed when they reali...   \n",
       "9            3  Basically there a family where little boy Jake...   \n",
       "10           3  And then we have Jake with his closet which to...   \n",
       "11           4  Petter Mattei Love in the Time of Money is vis...   \n",
       "12           4  This is movie that seems to be telling us what...   \n",
       "13           4  Kane Michael Imperioli Adrian Grenier and the ...   \n",
       "14           4  We wish Mr Mattei good luck and await anxiousl...   \n",
       "15           5  Probably my all time favorite movie story of s...   \n",
       "16           5  The kids are as grandma says more like dressed...   \n",
       "17           6  I sure would like to see resurrection of up da...   \n",
       "18           6  Oh by the way thank you for an outlet like thi...   \n",
       "19           6  So any ole way believe ve got what wanna say W...   \n",
       "20           7  This show was an amazing fresh innovative idea...   \n",
       "21           8  Encouraged by the positive comments about this...   \n",
       "22           9  If you like original gut wrenching laughter yo...   \n",
       "23          10  Phil the Alien is one of those quirky films wh...   \n",
       "24          10  At first it was very odd and pretty funny but ...   \n",
       "25          10                                            anymore   \n",
       "26          10  Its low budget film thats never problem in its...   \n",
       "27          11  I saw this movie when was about when it came o...   \n",
       "28          11  The horror As young kid going to these cheesy ...   \n",
       "29          12  So im not big fan of Boll work but then again ...   \n",
       "30          12  Postal maybe im the only one Boll apparently b...   \n",
       "31          12  So the tale goes like this Jack Carver played ...   \n",
       "32          12  yes Carver is German all hail the bratwurst ea...   \n",
       "33          12  but we only saw carver in first person perspec...   \n",
       "34          12  so we don really know what he looked like when...   \n",
       "35          12  We see the evil mad scientist Dr Krieger playe...   \n",
       "36          13  The cast played Shakespeare Shakespeare lost a...   \n",
       "37          14  This fantastic movie of three prisoners who be...   \n",
       "38          14  but this roll is not bad Another good thing ab...   \n",
       "39          15  Kind of drawn in by the erotic scenes only to ...   \n",
       "40          15  What was with the bisexual relationship out of...   \n",
       "41          15  And what was with that absurd dance with every...   \n",
       "42          16  Some films just simply should not be remade Th...   \n",
       "43          16  But you will enjoy the friction of terror in t...   \n",
       "44          17  This movie made it into one of my top most awf...   \n",
       "45          17  The ghost scene at the end was stolen from the...   \n",
       "46          17                                              hello   \n",
       "47          17  And the whole machine vs humans theme WAS the ...   \n",
       "48          18  I remember this film it was the first film had...   \n",
       "49          19  An awful film It must have been up against som...   \n",
       "50          19  They ve taken the story of the first famous fe...   \n",
       "51          19  My complaint is not that they ve taken liberti...   \n",
       "52          19  But it simply bizarre by all accounts the true...   \n",
       "53          19  so why did they come up with this dishwater du...   \n",
       "54          20  After the success of Die Hard and it sequels i...   \n",
       "55          20  However if you an forget all the nonsense it a...   \n",
       "56          20  And whilst he surely can be it really does loo...   \n",
       "57          20  yes you can help enjoy that bit Hal needed goo...   \n",
       "58          20  So forget your better judgement who cares if t...   \n",
       "59          20  And if you re looking for Qaulen he the one we...   \n",
       "\n",
       "                                preprocessed_sentence  label  \n",
       "0   one reviewer mention watch oz episode hook rig...      0  \n",
       "1   show pull punch regard drug sex violence hardc...      0  \n",
       "2   wonderful little production filming technique ...      1  \n",
       "3   voice pat truly see seamless edit guide refere...      0  \n",
       "4   terrificly write perform piece masterful produ...      0  \n",
       "5   realism really come home little thing fantasy ...      0  \n",
       "6   think wonderful way spend time hot summer week...      0  \n",
       "7   plot simplistic dialogue witty character likab...      0  \n",
       "8   may disappoint realize match point risk addict...      0  \n",
       "9   basically family little boy jake think zombie ...      1  \n",
       "10  jake closet totally ruin film expect see booge...      0  \n",
       "11  petter mattei love time money visually stunnin...      1  \n",
       "12  movie seem tell u money power success people d...      0  \n",
       "13  kane michael imperioli adrian grenier rest tal...      1  \n",
       "14  wish mr mattei good luck await anxiously next ...      0  \n",
       "15  probably time favorite movie story selflessnes...      1  \n",
       "16  kid grandma say like dress midget child make f...      0  \n",
       "17  sure would like see resurrection dated seahunt...      1  \n",
       "18  oh way thank outlet like view many viewpoint t...      0  \n",
       "19  ole way believe get wan na say would nice read...      0  \n",
       "20  show amazing fresh innovative idea first air f...      0  \n",
       "21  encourage positive comment film look forward w...      0  \n",
       "22  like original gut wrench laughter like movie y...      1  \n",
       "23  phil alien one quirky film humour base around ...      1  \n",
       "24  first odd pretty funny movie progress find jok...      1  \n",
       "25                                            anymore      0  \n",
       "26  low budget film thats never problem pretty int...      0  \n",
       "27  saw movie come recall scary scene big bird eat...      1  \n",
       "28  horror young kid go cheesy film saturday after...      0  \n",
       "29              im big fan boll work many enjoy movie      0  \n",
       "30  postal maybe im one boll apparently buy right ...      0  \n",
       "31        tale go like jack carver play til schweiger      0  \n",
       "32  yes carver german hail bratwurst eat dude howe...      0  \n",
       "33                saw carver first person perspective      0  \n",
       "34  really know look like kick however storyline f...      0  \n",
       "35  see evil mad scientist dr krieger play udo kie...      0  \n",
       "36  cast play shakespeare shakespeare lose appreci...      0  \n",
       "37  fantastic movie three prisoner become famous o...      1  \n",
       "38  roll bad another good thing movie soundtrack m...      1  \n",
       "39  kind drawn erotic scene realize one amateurish...      0  \n",
       "40  bisexual relationship nowhere heterosexual enc...      0  \n",
       "41  absurd dance everybody play stereotyped role g...      0  \n",
       "42  film simply remake one bad film fail capture f...      0  \n",
       "43             enjoy friction terror old version much      0  \n",
       "44  movie make one top awful movie horrible contin...      0  \n",
       "45  ghost scene end steal final scene old star war...      0  \n",
       "46                                              hello      0  \n",
       "47  whole machine v human theme matrix terminator ...      0  \n",
       "48  remember film first film watch cinema picture ...      0  \n",
       "49  awful film must real stinker nominate golden g...      1  \n",
       "50  take story first famous female renaissance pai...      1  \n",
       "51  complaint take liberty fact story good would p...      0  \n",
       "52  simply bizarre account true story artist would...      0  \n",
       "53  come dishwater dull script suppose enough nake...      0  \n",
       "54  success die hard sequels surprise really glut ...      0  \n",
       "55  however forget nonsense actually lovable unden...      0  \n",
       "56  whilst surely really look like ralph waite fra...      0  \n",
       "57           yes help enjoy bit hal need good kicking      0  \n",
       "58  forget good judgement care could never happen ...      0  \n",
       "59                    look qaulen one wear helicopter      0  "
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_processed_df.head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee6068",
   "metadata": {},
   "source": [
    "BiLSTM + Attention\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdb73e5e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d6b1f4e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aa5ca1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mohamedkenya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running on BBC Dataset ===\n",
      "Train Accuracy for BBC: 0.7290\n",
      "Test Accuracy for BBC: 0.6457\n",
      "\n",
      "Classification Report for BBC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.71      0.72      0.71      7616\n",
      "           1       0.54      0.53      0.54      4810\n",
      "\n",
      "    accuracy                           0.65     12426\n",
      "   macro avg       0.63      0.62      0.62     12426\n",
      "weighted avg       0.64      0.65      0.64     12426\n",
      "\n",
      "\n",
      "Average ROUGE-1 (F1) for BBC sample (200 articles): 0.6494\n",
      "Average ROUGE-2 (F1) for BBC sample (200 articles): 0.5573\n",
      "Average ROUGE-L (F1) for BBC sample (200 articles): 0.6427\n",
      "\n",
      "=== Running on IMDB Dataset ===\n",
      "Train Accuracy for IMDB: 0.9709\n",
      "Test Accuracy for IMDB: 0.9606\n",
      "\n",
      "Classification Report for IMDB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.99      0.97      0.98     11591\n",
      "           1       0.11      0.47      0.18       109\n",
      "\n",
      "    accuracy                           0.96     11700\n",
      "   macro avg       0.55      0.72      0.58     11700\n",
      "weighted avg       0.99      0.96      0.97     11700\n",
      "\n",
      "\n",
      "Average ROUGE-1 (F1) for IMDB sample (200 articles): 0.5617\n",
      "Average ROUGE-2 (F1) for IMDB sample (200 articles): 0.4881\n",
      "Average ROUGE-L (F1) for IMDB sample (200 articles): 0.5612\n"
     ]
    }
   ],
   "source": [
    "#Machine Using Decision Trees\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from rouge import Rouge\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def prepare_dataset(df):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        article = row['article']\n",
    "        summary = row['summary']\n",
    "        sents = sent_tokenize(article)\n",
    "        for sent in sents:\n",
    "            sentences.append(sent)\n",
    "            labels.append(1 if sent in summary else 0)\n",
    "    return sentences, labels\n",
    "\n",
    "def extract_summary(article, clf, vectorizer):\n",
    "    sents = sent_tokenize(article)\n",
    "    X_sents = vectorizer.transform(sents)\n",
    "    preds = clf.predict(X_sents)\n",
    "    extracted = [s for s, p in zip(sents, preds) if p == 1]\n",
    "    # Return first sentence if no sentence predicted\n",
    "    return \" \".join(extracted) if extracted else sents[0]\n",
    "\n",
    "def run_on_dataset(name, df):\n",
    "    print(f\"\\n=== Running on {name} Dataset ===\")\n",
    "    \n",
    "    # Prepare data\n",
    "    sentences, labels = prepare_dataset(df)\n",
    "    vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1,2))\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    y = labels\n",
    "\n",
    "    # Split train/test\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    # Create Decision Tree classifier with tuned hyperparameters\n",
    "    clf = DecisionTreeClassifier(\n",
    "        max_depth=30,\n",
    "        min_samples_leaf=10,\n",
    "        min_samples_split=10,\n",
    "        class_weight='balanced',\n",
    "        random_state=42\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    # Train/Test accuracy\n",
    "    train_acc = clf.score(X_train, y_train)\n",
    "    test_acc = clf.score(X_test, y_test)\n",
    "    print(f\"Train Accuracy for {name}: {train_acc:.4f}\")\n",
    "    print(f\"Test Accuracy for {name}: {test_acc:.4f}\")\n",
    "\n",
    "    # Classification report on test set\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"\\nClassification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    # ROUGE evaluation on 200 samples\n",
    "    sample_df = df.sample(n=200, random_state=42)\n",
    "    rouge = Rouge()\n",
    "    rouge_1_scores = []\n",
    "    rouge_2_scores = []\n",
    "    rouge_l_scores = []\n",
    "\n",
    "    for _, row in sample_df.iterrows():\n",
    "        pred_summary = extract_summary(row['article'], clf, vectorizer)\n",
    "        true_summary = row['summary']\n",
    "        scores = rouge.get_scores(pred_summary, true_summary)[0]\n",
    "        rouge_1_scores.append(scores['rouge-1']['f'])\n",
    "        rouge_2_scores.append(scores['rouge-2']['f'])\n",
    "        rouge_l_scores.append(scores['rouge-l']['f'])\n",
    "\n",
    "    print(f\"\\nAverage ROUGE-1 (F1) for {name} sample (200 articles): {sum(rouge_1_scores)/len(rouge_1_scores):.4f}\")\n",
    "    print(f\"Average ROUGE-2 (F1) for {name} sample (200 articles): {sum(rouge_2_scores)/len(rouge_2_scores):.4f}\")\n",
    "    print(f\"Average ROUGE-L (F1) for {name} sample (200 articles): {sum(rouge_l_scores)/len(rouge_l_scores):.4f}\")\n",
    "\n",
    "# Make sure your datasets columns are correctly named\n",
    "bbc_df = bbc_df.rename(columns={\"Article\": \"article\", \"Summary\": \"summary\"})\n",
    "imdb_df = imdb_df.rename(columns={\"Article\": \"article\", \"Summary\": \"summary\"})\n",
    "\n",
    "# Run on both datasets\n",
    "run_on_dataset(\"BBC\", bbc_df)\n",
    "run_on_dataset(\"IMDB\", imdb_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "34d1d017",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Extractive Summarization on BBC Dataset ===\n",
      "Epoch 1 Loss: 1.0810\n",
      "Epoch 2 Loss: 0.9956\n",
      "Epoch 3 Loss: 0.8756\n",
      "Epoch 4 Loss: 0.6977\n",
      "Epoch 5 Loss: 0.4841\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.75      0.62      0.68      5081\n",
      "         1.0       0.53      0.68      0.60      3255\n",
      "\n",
      "    accuracy                           0.64      8336\n",
      "   macro avg       0.64      0.65      0.64      8336\n",
      "weighted avg       0.66      0.64      0.64      8336\n",
      "\n",
      "Accuracy: 0.6402351247600768\n",
      "\n",
      "Tuning threshold for best F1...\n",
      "Best Threshold: 0.30, F1: 0.6094\n",
      "\n",
      "Computing ROUGE Scores...\n",
      "\n",
      "ROUGE-1 F1: 0.3606\n",
      "ROUGE-2 F1: 0.2528\n",
      "ROUGE-L F1: 0.3061\n",
      "Execution Time: 355.20 seconds\n",
      "\n",
      "=== Running Extractive Summarization on IMDB Dataset ===\n",
      "Epoch 1 Loss: 0.9255\n",
      "Epoch 2 Loss: 0.8912\n",
      "Epoch 3 Loss: 0.8349\n",
      "Epoch 4 Loss: 0.7562\n",
      "Epoch 5 Loss: 0.6411\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.85      0.66      0.75      2021\n",
      "         1.0       0.34      0.60      0.43       584\n",
      "\n",
      "    accuracy                           0.65      2605\n",
      "   macro avg       0.60      0.63      0.59      2605\n",
      "weighted avg       0.74      0.65      0.68      2605\n",
      "\n",
      "Accuracy: 0.6491362763915547\n",
      "\n",
      "Tuning threshold for best F1...\n",
      "Best Threshold: 0.40, F1: 0.4373\n",
      "\n",
      "Computing ROUGE Scores...\n",
      "\n",
      "ROUGE-1 F1: 0.4503\n",
      "ROUGE-2 F1: 0.3995\n",
      "ROUGE-L F1: 0.4373\n",
      "Execution Time: 111.21 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "import time\n",
    "from collections import defaultdict\n",
    "from rouge import Rouge\n",
    "\n",
    "# =====================\n",
    "# Dataset Class\n",
    "# =====================\n",
    "class ExtractiveDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.sentences = df['preprocessed_sentence'].values\n",
    "        self.labels = df['label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.labels[idx]\n",
    "\n",
    "# =====================\n",
    "# Tokenizer + Vocab\n",
    "# =====================\n",
    "class Vocab:\n",
    "    def __init__(self, texts, min_freq=1):\n",
    "        from collections import Counter\n",
    "        self.token2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        counter = Counter(word for text in texts for word in text.split())\n",
    "        for word, freq in counter.items():\n",
    "            if freq >= min_freq:\n",
    "                self.token2idx[word] = len(self.token2idx)\n",
    "        self.idx2token = {i: t for t, i in self.token2idx.items()}\n",
    "\n",
    "    def encode(self, text, max_len=100):\n",
    "        tokens = text.split()\n",
    "        ids = [self.token2idx.get(t, self.token2idx['<UNK>']) for t in tokens]\n",
    "        ids = ids[:max_len] + [0] * (max_len - len(ids))\n",
    "        return torch.tensor(ids)\n",
    "\n",
    "# =====================\n",
    "# Additive Attention\n",
    "# =====================\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, lstm_out):\n",
    "        energy = torch.tanh(self.W(lstm_out))\n",
    "        scores = self.v(energy).squeeze(2)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = torch.sum(lstm_out * weights.unsqueeze(2), dim=1)\n",
    "        return context, weights\n",
    "\n",
    "# =====================\n",
    "# BiLSTM + Attention Model\n",
    "# =====================\n",
    "class BiLSTMAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.bilstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attn_layer = AdditiveAttention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        lstm_out, _ = self.bilstm(embedded)\n",
    "        context, _ = self.attn_layer(lstm_out)\n",
    "        out = self.fc(context).squeeze(1)  # raw logits\n",
    "        return out\n",
    "\n",
    "# =====================\n",
    "# Collate Function for Padding\n",
    "# =====================\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    inputs = torch.stack([vocab.encode(t, max_len=100) for t in texts])\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    return inputs, labels\n",
    "\n",
    "# =====================\n",
    "# Training / Evaluation\n",
    "# =====================\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_preds, all_labels, all_texts, all_probs = [], [], [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = torch.sigmoid(model(inputs))  # Convert logits to probs\n",
    "            all_probs.extend(outputs.cpu().tolist())\n",
    "            all_preds.extend((outputs > 0.5).int().cpu().tolist())\n",
    "            all_labels.extend(labels.tolist())\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "    return all_probs, all_preds, all_labels\n",
    "\n",
    "# =====================\n",
    "# Run Pipeline\n",
    "# =====================\n",
    "def run_pipeline(name, df):\n",
    "    print(f\"\\n=== Running Extractive Summarization on {name} Dataset ===\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    if 'reference_summary' not in df.columns:\n",
    "        df['reference_summary'] = df.groupby('article_id')['preprocessed_sentence'].transform(\n",
    "            lambda x: ' '.join(x[df.loc[x.index, 'label'] == 1])\n",
    "        )\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "    global vocab\n",
    "    vocab = Vocab(train_df['preprocessed_sentence'].tolist())\n",
    "\n",
    "    train_dataset = ExtractiveDataset(train_df)\n",
    "    test_dataset = ExtractiveDataset(test_df)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32, collate_fn=collate_fn)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BiLSTMAttention(len(vocab.token2idx)).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "    pos_weight = torch.tensor([3.0]).to(device)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=pos_weight)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "        print(f\"Epoch {epoch+1} Loss: {loss:.4f}\")\n",
    "\n",
    "    all_probs, all_preds, all_labels = evaluate(model, test_loader, device)\n",
    "\n",
    "    # Find optimal threshold\n",
    "    print(\"\\nTuning threshold for best F1...\")\n",
    "    best_thresh, best_f1 = 0.5, 0\n",
    "    for t in [i * 0.05 for i in range(1, 20)]:\n",
    "        preds_t = [1 if p > t else 0 for p in all_probs]\n",
    "        score = f1_score(all_labels, preds_t)\n",
    "        if score > best_f1:\n",
    "            best_f1, best_thresh = score, t\n",
    "    print(f\"Best Threshold: {best_thresh:.2f}, F1: {best_f1:.4f}\")\n",
    "\n",
    "    # Apply best threshold for prediction\n",
    "    final_preds = [1 if p > best_thresh else 0 for p in all_probs]\n",
    "\n",
    "    # ROUGE Evaluation\n",
    "    print(\"\\nComputing ROUGE Scores...\")\n",
    "    pred_by_article = defaultdict(list)\n",
    "    for i, (row, pred) in enumerate(zip(test_df.itertuples(), final_preds)):\n",
    "        if pred == 1:\n",
    "            pred_by_article[row.article_id].append(row.preprocessed_sentence)\n",
    "\n",
    "    rouge = Rouge()\n",
    "    scores_1, scores_2, scores_l = [], [], []\n",
    "    grouped_refs = test_df[['article_id', 'reference_summary']].drop_duplicates().set_index('article_id')['reference_summary']\n",
    "\n",
    "    for aid, pred_sents in pred_by_article.items():\n",
    "        if aid in grouped_refs:\n",
    "            pred_summary = ' '.join(pred_sents)\n",
    "            ref_summary = grouped_refs[aid]\n",
    "            try:\n",
    "                score = rouge.get_scores(pred_summary, ref_summary)[0]\n",
    "                scores_1.append(score['rouge-1']['f'])\n",
    "                scores_2.append(score['rouge-2']['f'])\n",
    "                scores_l.append(score['rouge-l']['f'])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    if scores_1:\n",
    "        print(f\"\\nROUGE-1 F1: {sum(scores_1)/len(scores_1):.4f}\")\n",
    "        print(f\"ROUGE-2 F1: {sum(scores_2)/len(scores_2):.4f}\")\n",
    "        print(f\"ROUGE-L F1: {sum(scores_l)/len(scores_l):.4f}\")\n",
    "    else:\n",
    "        print(\"No valid ROUGE scores could be calculated.\")\n",
    "\n",
    "    print(\"Execution Time: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    return final_preds\n",
    "\n",
    "# Example call\n",
    "preds_bbc = run_pipeline(\"BBC\", bbc_processed_df)\n",
    "preds_imdb = run_pipeline(\"IMDB\", imdb_processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "445c9057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running Extractive Summarization on BBC Dataset ===\n",
      "Epoch 1: Train Loss=1.0831, Val Loss=1.0590\n",
      "Epoch 2: Train Loss=0.9987, Val Loss=1.0609\n",
      "Epoch 3: Train Loss=0.8756, Val Loss=1.0986\n",
      "Epoch 4: Train Loss=0.6644, Val Loss=1.2436\n",
      "Epoch 5: Train Loss=0.3821, Val Loss=1.5655\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.74      0.59      0.66      5081\n",
      "         1.0       0.52      0.68      0.59      3255\n",
      "\n",
      "    accuracy                           0.63      8336\n",
      "   macro avg       0.63      0.64      0.63      8336\n",
      "weighted avg       0.66      0.63      0.63      8336\n",
      "\n",
      "Accuracy: 0.628478886756238\n",
      "\n",
      "Best Threshold: 0.14, F1: 0.6003\n",
      "\n",
      "Computing ROUGE Scores...\n",
      "\n",
      "ROUGE-1 F1: 0.36842557298372297\n",
      "ROUGE-2 F1: 0.2553879411912053\n",
      "ROUGE-L F1: 0.30800864265444367\n",
      "Execution Time: 364.21 seconds\n",
      "\n",
      "=== Running Extractive Summarization on IMDB Dataset ===\n",
      "Epoch 1: Train Loss=0.9281, Val Loss=0.9079\n",
      "Epoch 2: Train Loss=0.8853, Val Loss=0.9044\n",
      "Epoch 3: Train Loss=0.8220, Val Loss=0.9070\n",
      "Epoch 4: Train Loss=0.7089, Val Loss=1.1719\n",
      "Epoch 5: Train Loss=0.5626, Val Loss=1.1418\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0       0.86      0.61      0.72      2021\n",
      "         1.0       0.33      0.65      0.43       584\n",
      "\n",
      "    accuracy                           0.62      2605\n",
      "   macro avg       0.59      0.63      0.58      2605\n",
      "weighted avg       0.74      0.62      0.65      2605\n",
      "\n",
      "Accuracy: 0.6218809980806143\n",
      "\n",
      "Best Threshold: 0.46, F1: 0.4369\n",
      "\n",
      "Computing ROUGE Scores...\n",
      "\n",
      "ROUGE-1 F1: 0.4419547538307266\n",
      "ROUGE-2 F1: 0.3931828028689194\n",
      "ROUGE-L F1: 0.43106943049814533\n",
      "Execution Time: 114.25 seconds\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score, f1_score\n",
    "from collections import defaultdict\n",
    "from rouge import Rouge\n",
    "import time\n",
    "\n",
    "# =====================\n",
    "# Constants & Configs\n",
    "# =====================\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 5\n",
    "LR = 1e-3\n",
    "MAX_LEN = 100\n",
    "POS_WEIGHT = 3.0\n",
    "\n",
    "# =====================\n",
    "# Dataset Class\n",
    "# =====================\n",
    "class ExtractiveDataset(Dataset):\n",
    "    def __init__(self, df):\n",
    "        self.sentences = df['preprocessed_sentence'].values\n",
    "        self.labels = df['label'].values\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.sentences)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return self.sentences[idx], self.labels[idx]\n",
    "\n",
    "# =====================\n",
    "# Tokenizer + Vocabulary\n",
    "# =====================\n",
    "class Vocab:\n",
    "    def __init__(self, texts, min_freq=1):\n",
    "        from collections import Counter\n",
    "        self.token2idx = {'<PAD>': 0, '<UNK>': 1}\n",
    "        counter = Counter(word for text in texts for word in text.split())\n",
    "        for word, freq in counter.items():\n",
    "            if freq >= min_freq:\n",
    "                self.token2idx[word] = len(self.token2idx)\n",
    "        self.idx2token = {i: t for t, i in self.token2idx.items()}\n",
    "\n",
    "    def encode(self, text, max_len=MAX_LEN):\n",
    "        ids = [self.token2idx.get(t, 1) for t in text.split()]\n",
    "        ids = ids[:max_len] + [0] * (max_len - len(ids))\n",
    "        return torch.tensor(ids)\n",
    "\n",
    "# =====================\n",
    "# Additive Attention Layer\n",
    "# =====================\n",
    "class AdditiveAttention(nn.Module):\n",
    "    def __init__(self, hidden_dim):\n",
    "        super().__init__()\n",
    "        self.W = nn.Linear(hidden_dim * 2, hidden_dim)\n",
    "        self.v = nn.Linear(hidden_dim, 1)\n",
    "\n",
    "    def forward(self, lstm_out):\n",
    "        energy = torch.tanh(self.W(lstm_out))\n",
    "        scores = self.v(energy).squeeze(2)\n",
    "        weights = torch.softmax(scores, dim=1)\n",
    "        context = torch.sum(lstm_out * weights.unsqueeze(2), dim=1)\n",
    "        return context, weights\n",
    "\n",
    "# =====================\n",
    "# BiLSTM + Attention Model\n",
    "# =====================\n",
    "class BiLSTMAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embed_dim=100, hidden_dim=128):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embed_dim, padding_idx=0)\n",
    "        self.bilstm = nn.LSTM(embed_dim, hidden_dim, batch_first=True, bidirectional=True)\n",
    "        self.attn = AdditiveAttention(hidden_dim)\n",
    "        self.fc = nn.Linear(hidden_dim * 2, 1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding(x)\n",
    "        lstm_out, _ = self.bilstm(x)\n",
    "        context, _ = self.attn(lstm_out)\n",
    "        return self.fc(context).squeeze(1)  # logits\n",
    "\n",
    "# =====================\n",
    "# Collate Function\n",
    "# =====================\n",
    "def collate_fn(batch):\n",
    "    texts, labels = zip(*batch)\n",
    "    inputs = torch.stack([vocab.encode(t) for t in texts])\n",
    "    labels = torch.tensor(labels, dtype=torch.float32)\n",
    "    return inputs, labels\n",
    "\n",
    "# =====================\n",
    "# Training Function\n",
    "# =====================\n",
    "def train_epoch(model, loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for inputs, labels in loader:\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# =====================\n",
    "# Evaluation Function\n",
    "# =====================\n",
    "def evaluate(model, loader, device):\n",
    "    model.eval()\n",
    "    all_probs, all_preds, all_labels = [], [], []\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs = inputs.to(device)\n",
    "            outputs = torch.sigmoid(model(inputs))\n",
    "            all_probs.extend(outputs.cpu().numpy())\n",
    "            all_preds.extend((outputs > 0.5).int().cpu().numpy())\n",
    "            all_labels.extend(labels.numpy())\n",
    "    print(classification_report(all_labels, all_preds))\n",
    "    print(\"Accuracy:\", accuracy_score(all_labels, all_preds))\n",
    "    return all_probs, all_preds, all_labels\n",
    "\n",
    "# =====================\n",
    "# Main Pipeline\n",
    "# =====================\n",
    "def run_pipeline(name, df):\n",
    "    print(f\"\\n=== Running Extractive Summarization on {name} Dataset ===\")\n",
    "    start_time = time.time()\n",
    "\n",
    "    if 'reference_summary' not in df.columns:\n",
    "        df['reference_summary'] = df.groupby('article_id')['preprocessed_sentence'].transform(\n",
    "            lambda x: ' '.join(x[df.loc[x.index, 'label'] == 1])\n",
    "        )\n",
    "\n",
    "    train_df, test_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "\n",
    "    global vocab\n",
    "    vocab = Vocab(train_df['preprocessed_sentence'].tolist())\n",
    "\n",
    "    train_dataset = ExtractiveDataset(train_df)\n",
    "    test_dataset = ExtractiveDataset(test_df)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=BATCH_SIZE, collate_fn=collate_fn)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = BiLSTMAttention(len(vocab.token2idx)).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=LR)\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', patience=2, factor=0.5)\n",
    "    criterion = nn.BCEWithLogitsLoss(pos_weight=torch.tensor([POS_WEIGHT]).to(device))\n",
    "\n",
    "    for epoch in range(EPOCHS):\n",
    "        train_loss = train_epoch(model, train_loader, optimizer, criterion, device)\n",
    "\n",
    "        # Validation loss\n",
    "        model.eval()\n",
    "        val_loss = 0\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in test_loader:\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "                val_loss += criterion(model(inputs), labels).item()\n",
    "        val_loss /= len(test_loader)\n",
    "        scheduler.step(val_loss)\n",
    "        print(f\"Epoch {epoch+1}: Train Loss={train_loss:.4f}, Val Loss={val_loss:.4f}\")\n",
    "\n",
    "    # Threshold tuning\n",
    "    all_probs, _, all_labels = evaluate(model, test_loader, device)\n",
    "    thresholds = [i/100 for i in range(10, 90, 2)]\n",
    "    best_thresh, best_f1 = max(((t, f1_score(all_labels, [1 if p > t else 0 for p in all_probs]))\n",
    "                                for t in thresholds), key=lambda x: x[1])\n",
    "    print(f\"\\nBest Threshold: {best_thresh:.2f}, F1: {best_f1:.4f}\")\n",
    "\n",
    "    final_preds = [1 if p > best_thresh else 0 for p in all_probs]\n",
    "\n",
    "    # ROUGE scoring\n",
    "    print(\"\\nComputing ROUGE Scores...\")\n",
    "    pred_by_article = defaultdict(list)\n",
    "    for row, pred in zip(test_df.itertuples(), final_preds):\n",
    "        if pred == 1:\n",
    "            pred_by_article[row.article_id].append(row.preprocessed_sentence)\n",
    "\n",
    "    grouped_refs = test_df[['article_id', 'reference_summary']].drop_duplicates().set_index('article_id')['reference_summary']\n",
    "    rouge = Rouge()\n",
    "    rouge_scores = {\"rouge-1\": [], \"rouge-2\": [], \"rouge-l\": []}\n",
    "\n",
    "    for aid, preds in pred_by_article.items():\n",
    "        ref = grouped_refs.get(aid)\n",
    "        if ref:\n",
    "            try:\n",
    "                scores = rouge.get_scores(' '.join(preds), ref)[0]\n",
    "                for key in rouge_scores:\n",
    "                    rouge_scores[key].append(scores[key]['f'])\n",
    "            except:\n",
    "                continue\n",
    "\n",
    "    if rouge_scores[\"rouge-1\"]:\n",
    "        print(\"\\nROUGE-1 F1:\", sum(rouge_scores[\"rouge-1\"]) / len(rouge_scores[\"rouge-1\"]))\n",
    "        print(\"ROUGE-2 F1:\", sum(rouge_scores[\"rouge-2\"]) / len(rouge_scores[\"rouge-2\"]))\n",
    "        print(\"ROUGE-L F1:\", sum(rouge_scores[\"rouge-l\"]) / len(rouge_scores[\"rouge-l\"]))\n",
    "    else:\n",
    "        print(\"No valid ROUGE scores could be calculated.\")\n",
    "\n",
    "    print(\"Execution Time: {:.2f} seconds\".format(time.time() - start_time))\n",
    "    return final_preds\n",
    "\n",
    "# =====================\n",
    "# Example Usage\n",
    "# =====================\n",
    "preds_bbc = run_pipeline(\"BBC\", bbc_processed_df)\n",
    "preds_imdb = run_pipeline(\"IMDB\", imdb_processed_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "317011a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== BBC Dataset Sample Summaries ===\n",
      "\n",
      "--- Article ID: 1542 ---\n",
      "Predicted Summary:\n",
      "say minister would consult proposal could see father allow take partner maternity pay leave period extend right flexible work carers parent old child say party would boost maternity pay first six month allow woman stay home time hewitt also stress plan would pay taxpayer employer plan include let maternity pay give father extend right parent old child far detail government plan outline monday\n",
      "\n",
      "Reference Summary:\n",
      "She said her party would boost maternity pay in the first six months to allow more women to stay at home in that time.She said new mothers were already entitled to 12 months leave, but that many women could not take it as only six of those months were paid.The Tories dismissed the maternity pay plan as \"desperate\", while the Liberal Democrats said it was misdirected.She said ministers would consult on other proposals that could see fathers being allowed to take some of their partner's maternity pay or leave period, or extending the rights of flexible working to carers or parents of older children.Liberal Democrat spokeswoman for women Sandra Gidley said: \"While mothers would welcome any extra maternity pay the Liberal Democrats feel this money is being misdirected.\"We will definitely extend the maternity pay, from the six months where it now is to nine months, that's the extra Â£1,400.\"Ms Hewitt said: \"We have already doubled the length of maternity pay, it was 13 weeks when we were elected, we have already taken it up to 26 weeks.Other plans include letting maternity pay be given to fathers and extending rights to parents of older children.\n",
      "\n",
      "============================================================\n",
      "\n",
      "--- Article ID: 287 ---\n",
      "Predicted Summary:\n",
      "wasp make tv presenter andrea arnold little terrorist work ashvin kumar also debut nomination staunton tell bbc news think film would appeal academy voter owen already make bookmaker favourite best support actor role closer already clinch golden globe award nominate film release ago feel honoured overwhelm say first nomination actress okonedo chosen performance hotel rwanda rwandan genocide winslet say ecstatic fourth nomination career\n",
      "\n",
      "Reference Summary:\n",
      "Kate Winslet was also nominated in the best actress category for her role in Eternal Sunshine of the Spotless Mind.To be nominated for a film that was released a while ago, I feel so honoured and overwhelmed,\" she said.\"It was an extraordinary time making the film and I can't believe what has happened this morning,\" she said.Imelda Staunton was nominated for best actress for her role in the abortion drama, while Leigh received nods for best director and original screenplay.Winslet said she was \"ecstatic\" about the fourth nomination of her career.Cinematographer John Mathieson, who was nominated for Gladiator in 2001, is also up for The Phantom of the Opera.It is also a debut nomination for Staunton, 49, who told BBC News 24 she had not thought the film would appeal to Academy voters.The UK has two contenders in the best live action short film category.John Woodward, chief executive of the UK Film Council, said it was \"extremely heartening\" to see British filmmaking talent recognised on the global stage.Owen has already been made bookmakers' favourite for best supporting actor for the role in Closer that has already clinched him a Golden Globe award.\"Britain has a hugely talented industry and these nominations show why National Lottery investment in film pays major dividends for our culture and economy.\"\n",
      "\n",
      "============================================================\n",
      "\n",
      "--- Article ID: 62 ---\n",
      "Predicted Summary:\n",
      "protagonist harvard professor liken contemporary indiana jones also appear brown first book angel demon da vinci film star tom hank actor tom hank director ron howard reunite da vinci code adaptation international novel dan brown hank play robert langdon try solve murder member ancient society protect dark secret century\n",
      "\n",
      "Reference Summary:\n",
      "It will be Hanks' third collaboration with Howard.Brown's book has become a publishing phenomenon, consistently topping book charts in the UK and US.Hanks will play Robert Langdon, who is trying to solve the murder of a member of an ancient society that has protected dark secrets for centuries.Actor Tom Hanks and director Ron Howard are reuniting for The Da Vinci Code, an adaptation of the international best-selling novel by Dan Brown.It is a classic whodunit, which centres on a global conspiracy surrounding the Holy Grail mythology and places heavy emphasis on symbols and cryptography.\n",
      "\n",
      "============================================================\n",
      "\n",
      "--- Article ID: 1779 ---\n",
      "Predicted Summary:\n",
      "add last year see many statement senior figure government insist public must right choice\n",
      "\n",
      "Reference Summary:\n",
      "Their plan would see the House of Lords being renamed the Second Chamber of Parliament, and its members would be known as MSCPs.The group says the British public and a clear majority of MPs support replacing the Lords with a largely-elected second chamber.The cross-party group has unveiled a draft bill proposing a smaller second chamber in which 70% of members would be elected.A group of MPs has tried to raise the pressure on Tony Blair over reform to the House of Lords by publishing a detailed blueprint for change.The Elect the Lords Campaign said the draft bill was an important contribution to the debate.The government postponed plans to remove the remaining hereditary peers because they said they were unlikely to succeed after opposition in the Lords.But the all-party group, including Tories Ken Clarke and Sir George Young, Labour's Robin Cook and Tony Wright and Liberal Democrat Paul Tyler, is confident its plan would win support from a \"large majority\".And Mr Tyler said the prime minister's view that there was no agreement on the shape of the future of the Lords was flawed.\"The problem, I think, in the prime minister's mind is there doesn't appear to be a consensus that includes him,\" he said.The group says it can win support for removing the last 92 hereditaries.There would be 385 MSCPs, including 270 elected members, 87 appointed members and 16 bishops.\n",
      "\n",
      "============================================================\n",
      "\n",
      "--- Article ID: 678 ---\n",
      "Predicted Summary:\n",
      "regret ministry take decision say ivan kulakov deputy chairman highland gold mining firm motto bring russia gold market u oil giant exxon say plan take part new tender project previously sign preliminary agreement\n",
      "\n",
      "Reference Summary:\n",
      "Exxon, the world's largest oil company, has signed preliminary agreements to develop the Sakhalin 3 field.International oil and mining companies have reacted cautiously to Russia's decision to bar foreign firms from natural resource tenders in 2005.\"We regret the ministry has taken such a decision,\" said Ivan Kulakov, deputy chairman of Highland Gold - a mining firm that has the motto \"Bringing Russia's Gold to Market\".Company spokesman Glenn Waller said Exxon still considered the deal valid, despite Russia inviting new offers for the land block.What has surprised observers is that since the collapse of communism Russia has been courting foreign investment.\"It would be a shame if that has a negative impact on the investment climate.\"US oil giant Exxon said it did not plan to take part in a new tender on a project for which it had previously signed a preliminary agreement.BP spent $7.5bn to create Russian-registered oil company TNK-BP, and has a partnership to develop the Sakhalin 5 petroleum field with state-owned Rosneft.Other firms that have been linked with investment in Russia include France's Total, the US-based ChevronTexaco, and miner Barrick Gold.According to Mr Waller, Exxon \"were not planning to bid at a new tender anyway\".\n",
      "\n",
      "============================================================\n",
      "\n",
      "\n",
      "=== IMDB Dataset Sample Summaries ===\n",
      "\n",
      "--- Article ID: 1543 ---\n",
      "Predicted Summary:\n",
      "oh man want give internal crow robot real workout movie pop ol vcr\n",
      "\n",
      "Reference Summary:\n",
      "Traci is girl with problem Psychology has developed names for it when child develops sexual crush on the opposite sex parent But this girl seems to have one for her same sex one and don't think there a term for that. Her mother Dana is played by Rosanna Arquette whose cute overbite neo flowerchild sexuality and luscious figure makes me forgive her any number of bad movies or unsympathetic characters.\n",
      "\n",
      "============================================================\n",
      "\n",
      "--- Article ID: 1420 ---\n",
      "Predicted Summary:\n",
      "one care everyone pca love zoey would anything even mean give right arm wardrobe good mine mine pretty freak decent character paper flat hard enjoy\n",
      "\n",
      "Reference Summary:\n",
      "The characters are Stupid They re ALL stereotypical and yet have nice clothes and are always skinny Don't even get me started on Jamie Lynn role as ZOEY Zoey is pretty popular tan blonde young teen who everyone just LOVES She has rebellious great personality that everyone agrees with no matter how dumb or extreme it is.\n",
      "\n",
      "============================================================\n",
      "\n",
      "--- Article ID: 889 ---\n",
      "Predicted Summary:\n",
      "movie keep laugh hysterically dunno maybe like bad relationship get ridiculous guilty pleasure either way single underrated movie behind stunt man robert stack\n",
      "\n",
      "Reference Summary:\n",
      "The only conceivable flaw of this film is it title Please stop comparing it to the first did in my previous review only to separate it from the first If you haven't seen the movie and are curious TOTALLY forget about the first and invent different name for this. There is nothing alike and has mood all its own.\n",
      "\n",
      "============================================================\n",
      "\n",
      "--- Article ID: 659 ---\n",
      "Predicted Summary:\n",
      "also pretty disgust remember dinner scene think almost felt sorry ritter yasbeck warden\n",
      "\n",
      "Reference Summary:\n",
      "The sequel is like its predecessor completely brain dead It also pretty disgusting remember the dinner scene. To think almost felt sorry for Ritter Yasbeck and Warden Did they need the money that much \n",
      "\n",
      "============================================================\n",
      "\n",
      "--- Article ID: 3511 ---\n",
      "Predicted Summary:\n",
      "mind keep thinking guy noir sketch music\n",
      "\n",
      "Reference Summary:\n",
      "It must have been excruciating to attend the dailies as the shooting continued on this failure of film Probably Cruise the Exec Prod saw what was happening and had Towne use much much more of the nude footage in the final cut then Towne wanted to. Colin Farrell can act his way out of paper bag But he one of the flavors of the decade producer darling and one is forced to avoid the embarrassment of watching him by not attending his films.\n",
      "\n",
      "============================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "random.seed(42)  # for reproducibility\n",
    "\n",
    "# --- BBC ---\n",
    "\n",
    "# Recreate test split for BBC\n",
    "_, test_df_bbc = train_test_split(bbc_processed_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build predicted summaries dict for BBC\n",
    "pred_by_article_bbc = defaultdict(list)\n",
    "for pred, text, row in zip(preds_bbc, pred_texts_bbc, test_df_bbc.itertuples()):\n",
    "    if pred == 1:\n",
    "        pred_by_article_bbc[row.article_id].append(text)\n",
    "\n",
    "predicted_summaries_bbc = {aid: ' '.join(sents) for aid, sents in pred_by_article_bbc.items()}\n",
    "\n",
    "# Filter and sample article_ids for BBC\n",
    "filtered_df_bbc = bbc_processed_df[bbc_processed_df['label'] == 1]\n",
    "candidate_article_ids_bbc = filtered_df_bbc['article_id'].unique()\n",
    "candidate_article_ids_bbc = [aid for aid in candidate_article_ids_bbc if aid in predicted_summaries_bbc]\n",
    "sampled_article_ids_bbc = random.sample(list(candidate_article_ids_bbc), 5)\n",
    "\n",
    "# Ensure article_id column in bbc_df\n",
    "if 'article_id' not in bbc_df.columns:\n",
    "    bbc_df = bbc_df.reset_index().rename(columns={'index': 'article_id'})\n",
    "\n",
    "print(\"\\n=== BBC Dataset Sample Summaries ===\\n\")\n",
    "for aid in sampled_article_ids_bbc:\n",
    "    pred_sum = predicted_summaries_bbc.get(aid, \"[No predicted summary]\")\n",
    "    ref_sum_row = bbc_df.loc[bbc_df['article_id'] == aid, 'Summary']\n",
    "    ref_sum = ref_sum_row.values[0] if len(ref_sum_row) > 0 else \"[No original summary]\"\n",
    "    \n",
    "    print(f\"--- Article ID: {aid} ---\")\n",
    "    print(\"Predicted Summary:\")\n",
    "    print(pred_sum)\n",
    "    print(\"\\nReference Summary:\")\n",
    "    print(ref_sum)\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "\n",
    "\n",
    "# --- IMDB ---\n",
    "\n",
    "# Recreate test split for IMDB\n",
    "_, test_df_imdb = train_test_split(imdb_processed_df, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build predicted summaries dict for IMDB\n",
    "pred_by_article_imdb = defaultdict(list)\n",
    "for pred, text, row in zip(preds_imdb, pred_texts_imdb, test_df_imdb.itertuples()):\n",
    "    if pred == 1:\n",
    "        pred_by_article_imdb[row.article_id].append(text)\n",
    "\n",
    "predicted_summaries_imdb = {aid: ' '.join(sents) for aid, sents in pred_by_article_imdb.items()}\n",
    "\n",
    "# Filter and sample article_ids for IMDB\n",
    "filtered_df_imdb = imdb_processed_df[imdb_processed_df['label'] == 1]\n",
    "candidate_article_ids_imdb = filtered_df_imdb['article_id'].unique()\n",
    "candidate_article_ids_imdb = [aid for aid in candidate_article_ids_imdb if aid in predicted_summaries_imdb]\n",
    "sampled_article_ids_imdb = random.sample(list(candidate_article_ids_imdb), 5)\n",
    "\n",
    "# Ensure article_id column in imdb_df\n",
    "if 'article_id' not in imdb_df.columns:\n",
    "    imdb_df = imdb_df.reset_index().rename(columns={'index': 'article_id'})\n",
    "\n",
    "print(\"\\n=== IMDB Dataset Sample Summaries ===\\n\")\n",
    "for aid in sampled_article_ids_imdb:\n",
    "    pred_sum = predicted_summaries_imdb.get(aid, \"[No predicted summary]\")\n",
    "    ref_sum_row = imdb_df.loc[imdb_df['article_id'] == aid, 'Summary']\n",
    "    ref_sum = ref_sum_row.values[0] if len(ref_sum_row) > 0 else \"[No original summary]\"\n",
    "    \n",
    "    print(f\"--- Article ID: {aid} ---\")\n",
    "    print(\"Predicted Summary:\")\n",
    "    print(pred_sum)\n",
    "    print(\"\\nReference Summary:\")\n",
    "    print(ref_sum)\n",
    "    print(\"\\n\" + \"=\"*60 + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "206d91ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     /Users/mohamedkenya/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Running on BBC Dataset with Random Forest ===\n",
      "Train Accuracy for BBC: 0.7017\n",
      "Test Accuracy for BBC: 0.6632\n",
      "\n",
      "Classification Report for BBC:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.72      0.74      0.73      7616\n",
      "           1       0.57      0.54      0.56      4810\n",
      "\n",
      "    accuracy                           0.66     12426\n",
      "   macro avg       0.64      0.64      0.64     12426\n",
      "weighted avg       0.66      0.66      0.66     12426\n",
      "\n",
      "\n",
      "Average ROUGE-1 (F1) for BBC sample (200 articles): 0.6474\n",
      "Average ROUGE-2 (F1) for BBC sample (200 articles): 0.5601\n",
      "Average ROUGE-L (F1) for BBC sample (200 articles): 0.6415\n",
      "\n",
      "=== Running on IMDB Dataset with Random Forest ===\n",
      "Train Accuracy for IMDB: 0.9892\n",
      "Test Accuracy for IMDB: 0.9840\n",
      "\n",
      "Classification Report for IMDB:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      0.99      0.99     11591\n",
      "           1       0.29      0.50      0.37       109\n",
      "\n",
      "    accuracy                           0.98     11700\n",
      "   macro avg       0.64      0.75      0.68     11700\n",
      "weighted avg       0.99      0.98      0.99     11700\n",
      "\n",
      "\n",
      "Average ROUGE-1 (F1) for IMDB sample (200 articles): 0.5617\n",
      "Average ROUGE-2 (F1) for IMDB sample (200 articles): 0.4881\n",
      "Average ROUGE-L (F1) for IMDB sample (200 articles): 0.5612\n",
      "\n",
      "=== Sample Predicted vs Reference Summaries for BBC ===\n",
      "\n",
      "--- Article 1 ---\n",
      "Predicted Summary:\n",
      "Mr Brown has just returned from a tour of African nations. The chancellor was speaking at an event jointly organised by the UK's Department for International Development and the UN Development Programme on Wednesday. Mr Brown welcomed news that the Bill Gates Foundation and Norway are joining up to put an extra Â£0.53bn ($1bn ) into the Global Alliance for Vaccines and Immunisation (Gavi). If Gavi could increase its funding for immunisation by an extra Â£4bn ($7.4bn) over 10 years, then an extra five million lives could have been saved by 2015 and five million thereafter, Mr Brown argued. Campaign groups including Friends of the Earth, the World Development Movement, and War on Want said UK government policy on free trade was a major barrier to fighting poverty. \"As long as Mr Blair and Mr Brown continue to push free trade and privatisation on developing countries, more and more people will be pushed deeper into poverty, not lifted out of it.\"\n",
      "\n",
      "Reference Summary:\n",
      "Mr Brown welcomed news that the Bill Gates Foundation and Norway are joining up to put an extra Â£0.53bn ($1bn ) into the Global Alliance for Vaccines and Immunisation (Gavi).UK Chancellor Gordon Brown has offered Â£960m ($1.8bn) over 15 years to an international scheme aiming to boost vaccination and immunisation schemes.If Gavi could increase its funding for immunisation by an extra Â£4bn ($7.4bn) over 10 years, then an extra five million lives could have been saved by 2015 and five million thereafter, Mr Brown argued.\"As long as Mr Blair and Mr Brown continue to push free trade and privatisation on developing countries, more and more people will be pushed deeper into poverty, not lifted out of it.\"Britain, France, Gavi and the Gates Foundation have drawn up proposals to apply the principles of the International Finance Facility (IFF) to the area of immunisation.Campaign groups including Friends of the Earth, the World Development Movement, and War on Want said UK government policy on free trade was a major barrier to fighting poverty.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Article 2 ---\n",
      "Predicted Summary:\n",
      "Lindsay Davenport, top seed in the women's draw, has been handed a first-round bye and plays France's Dechy in the second round on Tuesday.\n",
      "\n",
      "Reference Summary:\n",
      "Defending women's champion Justine Henin-Hardenne is also out of the Sydney event because of a knee injury.Number one men's seed Lleyton Hewitt begins his quest for a fourth Sydney title on Tuesday when he plays Karol Beck.Lindsay Davenport, top seed in the women's draw, has been handed a first-round bye and plays France's Dechy in the second round on Tuesday.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Article 3 ---\n",
      "Predicted Summary:\n",
      "The US budget and its trade deficit are both deep in the red, helping to push the dollar to lows against the euro and fuelling fears about the economy. Mr Bush indicated there would be \"strict discipline\" on non-defence spending in the budget. \"We will submit a budget that fits the times,\" Mr Bush said. The US has said it is committed to a strong dollar. But the dollar's weakness has hit European and Asian exporters and lead to calls for US intervention to boost the currency. Mr Bush, however, has said the best way to halt the dollar's slide is to deal with the US deficit. \"It's a budget that I think will send the right signal to the financial markets and to those concerned about our short-term deficits,\" Mr Bush added.\n",
      "\n",
      "Reference Summary:\n",
      "Mr Bush, however, has said the best way to halt the dollar's slide is to deal with the US deficit.US president George W Bush has pledged to introduce a \"tough\" federal budget next February in a bid to halve the country's deficit in five years.\"We will submit a budget that fits the times,\" Mr Bush said.The US budget and its trade deficit are both deep in the red, helping to push the dollar to lows against the euro and fuelling fears about the economy.Mr Bush indicated there would be \"strict discipline\" on non-defence spending in the budget.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Article 4 ---\n",
      "Predicted Summary:\n",
      "Minimum rate for foster parents\n",
      "\n",
      "Foster carers are to be guaranteed a minimum allowance to help cover their costs, the government has announced. Minister for Children, Young People and Families Margaret Hodge said new plans will ensure fosterers' allowances would be as fair as possible. About 50,000 children live with foster families in the UK and carers have said they need more money to make ends meet. Ms Hodge said: \"Foster carers must not be out of pocket when meeting the costs of caring for a looked after child - a crucial role in society. \"Our proposal for a national minimum rate shows we are serious about creating a better deal for foster carers and about encouraging more people to come forward and consider fostering as a worthwhile and rewarding opportunity.\" The government is seeking to amend the Children Bill, which passes through the Commons next week, to establish a national minimum payment. The Association of Directors of Social Services (ADSS) said it agreed in principle with the government's plans.\n",
      "\n",
      "Reference Summary:\n",
      "About 50,000 children live with foster families in the UK and carers have said they need more money to make ends meet.\"And with a shortage of over 8,000 foster carers in England, it's not a sustainable situation to expect carers to fund foster care from their own pockets.\"\"We need to make sure that arrangements for paying foster carers are as fair and transparent as possible.Foster carers are to be guaranteed a minimum allowance to help cover their costs, the government has announced.\"But ADSS fully supports proper remuneration for valued foster carers and looks forward to working with ministers, local government and the fostering organisations themselves in order to make sure a sensible and practicable policy emerges.\"Ms Hodge said: \"Foster carers must not be out of pocket when meeting the costs of caring for a looked after child - a crucial role in society.\"Our proposal for a national minimum rate shows we are serious about creating a better deal for foster carers and about encouraging more people to come forward and consider fostering as a worthwhile and rewarding opportunity.\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Article 5 ---\n",
      "Predicted Summary:\n",
      "The Scot, who led GB to World Cross Country bronze earlier this year, moved away from the field with Ines Monteiro halfway into the 6.6km race. Meanwhile, Briton Karl Keska battled bravely to finish seventh in the men's 10.6km race in a time of 31:41. Kenenisa Bekele of Ethiopia - the reigning world long and short course champion - was never troubled by any of the opposition, winning leisurely in 30.26. Butler said of her success: \"I felt great throughout the race and hope this is a good beginning for a marvellous 2005 season for me.\"\n",
      "\n",
      "Reference Summary:\n",
      "Gelete Burka then crowned a great day for Ethiopia by claiming victory in the women's race.Elsewhere, Abebe Dinkessa of Ethiopia won the Brussels IAAF cross-country race on Sunday, completing the 10,500m course in 33.22.The Scot, who led GB to World Cross Country bronze earlier this year, moved away from the field with Ines Monteiro halfway into the 6.6km race.Butler said of her success: \"I felt great throughout the race and hope this is a good beginning for a marvellous 2005 season for me.\"\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "=== Sample Predicted vs Reference Summaries for IMDB ===\n",
      "\n",
      "--- Article 1 ---\n",
      "Predicted Summary:\n",
      "Two old men sitting on park bench don really have problem with this scene Only problem is that it not scene it the entire movieYup movies don get anymore low concept than this They also don get anymore boring than this either but there worse to come because these two old men are chalk and cheese One is Nat Moyer who is Yiddish communist while the other is Midge Carter former golden gloves champion who also black Let me see now Jew and black man sitting on park bench getting along fine Well guess it possible though unlikely but if this film has such an inoffensive scenario why play up to the Jewish stereotype Why make them loud tribilistic rabble rousers who take hebrew oaths Slightly ironic that the Jews seen at the start of the movie are exactly the type of Jews seen in Nazi propaganda films in the sStereotypes aside moi dearz the problem with M NOT RAPPAPORT is that it written for an entirely different meduim than cinema it based on stage play and it shows Walter Matthau sleepwalks through his role as Nat while this commentator almost slept through the whole movie\n",
      "\n",
      "Reference Summary:\n",
      "Two old men sitting on park bench don't really have problem with this scene. Only problem is that it not scene it the entire movie. Walter Matthau sleepwalks through his role as Nat while this commentator almost slept through the whole movie.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Article 2 ---\n",
      "Predicted Summary:\n",
      "Dark Harbor is moody little excursion into murky emotional waters that run extremely deep It basically character piece featuring finely layered performance by the always great Rickman with Polly Walker and Norman Reedus also excellent forming the other two sides of this strange triangle perfect late night cable film with surprise ending to boot\n",
      "\n",
      "Reference Summary:\n",
      "Dark Harbor is moody little excursion into murky emotional waters that run extremely deep. It basically character piece featuring finely layered performance by the always great Rickman. Polly Walker and Norman Reedus also excellent forming the other two sides of this strange triangle.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Article 3 ---\n",
      "Predicted Summary:\n",
      "After long period in the space looking for the remains of planet Krypton Superman Brandon Routh returns to Earth He misses Lois Lane Kate Bosworth who got married and has son with Richard White James Marsden Meanwhile Lex Luthor Kevin Spacey plots an evil plan using crystals he stole from the Fortress of Solitude to create new land and submerge the USA After so many delightful movies of Superman with the unforgettable Christopher Reeve or TV shows like Lois and Clark and Teri Hatcher or Smallville great expectation was created for the return of Superman in this Bryan Singer version Unfortunately the awful story is too long and boring with many unnecessary parts lack of emotion and overrated in IMDb In addition the romance between Lois Lane and Superman is something shamefully ridiculous The twenty two years old actress Kate Bosworth is wrongly miscast playing the role of mature reporter and experienced mother of five years old boy Brandon Routh is two years younger than Tom Welling who plays teenager Clark Kent in Smallville The character of Parker Posey Kitty Kowalski is actually silly caricature Last but not the least and in spite of being terrific Lex Luthor Kevin Spacey is forty five years old therefore older and older than the rest of the lead cast The corny conclusion looks like soap opera and is terrible My vote is four Title Brazil Superman Returns\n",
      "\n",
      "Reference Summary:\n",
      "After long period in the space looking for the remains of planet Krypton Superman Brandon Routh returns to Earth He misses Lois Lane Kate Bosworth who got married and has son with Richard White James Marsden Meanwhile Lex Luthor Kevin Spacey plots an evil plan using crystals he stole from the Fortress of Solitude to create new land and submerge the USA.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Article 4 ---\n",
      "Predicted Summary:\n",
      "There were lot of things going against this movie for me before watched it First was typical high school senior in Shakespeare class didn really even like much less understood half of Shakespeare would be no more than UNINTELLIGIBLE without me pouring ALL my concentration into his almost encrypted plays encrypted with his extremely difficult to understand language and then still wouldn get most of it Second it was hours long never thought that could be good thing Well let me tell you something This movie was so masterful so beautiful actually understood all the language as it was being performed Now the script was followed to the letter in this movie the same script that was incomprehensible to me in Shakespeare class And here was my mind opening and me understanding it was doubting myself while watching the movie almost But lo and behold when performed and only then Shakespeare comes to life So this version of Hamlet showed me that Shakespeare is indeed master who wrote great stories When saw it on the big screen especially in the high budget major motion picture style with beautiful cinematography and photography and acted amazingly by Brannagh and cast somehow understood what was going on What was being said The language is awesome and passionate It allows for more raw emotion when words can describe something maybe Shakespeare words can still hold to this day that Fist of The North Star animated english dub is the greatest movie ever made No movie provides more sheer entertainment But for movie to come close to dethroning Fist from that position which Hamlet did it came close is truly amazing awe inspiring It wasn a movie It was an event Even more amazing it made me appreciate shakespeare Wow Powerful Powerful is the word One of the rare TRULY powerful movies out there This gets hundred trillion stars out of infinity stars Yes yes By the way all you kids out there in Shakespeare class forget it You re wasting you re time You have to see the plays performed Only then will justice be done to them\n",
      "\n",
      "Reference Summary:\n",
      "There were lot of things going against this movie for me before watched it. This movie was so masterful so beautiful actually understood all the language as it was being performed. The script was followed to the letter in this movie the same script that was incomprehensible to me in Shakespeare class.\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "--- Article 5 ---\n",
      "Predicted Summary:\n",
      "I sorry but even TJ Hooker Adrian Zmed couldn save this sequel went through half the movie thinking that this was spoof of the original Then came that wild and wacky motorcycle scene notice that this is the only movie that Patricia Birch directs and sadly realized they were trying to be serious did get kick out of the fact that the opposing gang having lost their wheels due to their gambling habits in the original Grease were forced to use motorcycles in the second movie Being shamed by that putz character Carrington d hate to see what they would resort to later maybe Mopeds also never bought the hackneyed theme hunky Australian boy can fit into Outsiders dominated school ergo goes for tough guy with stupid biker helmet look It was Disney story gone horribly awry So it looks like you CAN ruin good thing by placing bubble gum smacking Michelle Pfeiffer in musical The only thing took away from this movie was an idea of how many points out of ten to give it\n",
      "\n",
      "Reference Summary:\n",
      "I sorry but even TJ Hooker Adrian Zmed couldn't save this sequel. It was Disney story gone horribly awry. So it looks like you CAN ruin good thing by placing bubble gum smacking Michelle Pfeiffer in musical.\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from rouge import Rouge\n",
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Prepare dataset into sentence-level samples\n",
    "def prepare_dataset(df):\n",
    "    sentences = []\n",
    "    labels = []\n",
    "    for _, row in df.iterrows():\n",
    "        article = row['article']\n",
    "        summary = row['summary']\n",
    "        sents = sent_tokenize(article)\n",
    "        for sent in sents:\n",
    "            sentences.append(sent)\n",
    "            labels.append(1 if sent in summary else 0)\n",
    "    return sentences, labels\n",
    "\n",
    "# Use the model to extract predicted summary from an article\n",
    "def extract_summary(article, clf, vectorizer):\n",
    "    sents = sent_tokenize(article)\n",
    "    X_sents = vectorizer.transform(sents)\n",
    "    preds = clf.predict(X_sents)\n",
    "    extracted = [s for s, p in zip(sents, preds) if p == 1]\n",
    "    return \" \".join(extracted) if extracted else sents[0]\n",
    "\n",
    "# Main training + evaluation function\n",
    "def run_on_dataset(name, df):\n",
    "    print(f\"\\n=== Running on {name} Dataset with Random Forest ===\")\n",
    "\n",
    "    sentences, labels = prepare_dataset(df)\n",
    "    vectorizer = TfidfVectorizer(max_features=10000, ngram_range=(1, 2))\n",
    "    X = vectorizer.fit_transform(sentences)\n",
    "    y = labels\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "    clf = RandomForestClassifier(\n",
    "        n_estimators=100,\n",
    "        max_depth=30,\n",
    "        min_samples_leaf=10,\n",
    "        min_samples_split=10,\n",
    "        class_weight='balanced',\n",
    "        random_state=42,\n",
    "        n_jobs=-1\n",
    "    )\n",
    "    clf.fit(X_train, y_train)\n",
    "\n",
    "    train_acc = clf.score(X_train, y_train)\n",
    "    test_acc = clf.score(X_test, y_test)\n",
    "    print(f\"Train Accuracy for {name}: {train_acc:.4f}\")\n",
    "    print(f\"Test Accuracy for {name}: {test_acc:.4f}\")\n",
    "\n",
    "    y_pred = clf.predict(X_test)\n",
    "    print(f\"\\nClassification Report for {name}:\")\n",
    "    print(classification_report(y_test, y_pred))\n",
    "\n",
    "    sample_df = df.sample(n=200, random_state=42)\n",
    "    rouge = Rouge()\n",
    "    rouge_1_scores, rouge_2_scores, rouge_l_scores = [], [], []\n",
    "\n",
    "    for _, row in sample_df.iterrows():\n",
    "        pred_summary = extract_summary(row['article'], clf, vectorizer)\n",
    "        true_summary = row['summary']\n",
    "        try:\n",
    "            scores = rouge.get_scores(pred_summary, true_summary)[0]\n",
    "            rouge_1_scores.append(scores['rouge-1']['f'])\n",
    "            rouge_2_scores.append(scores['rouge-2']['f'])\n",
    "            rouge_l_scores.append(scores['rouge-l']['f'])\n",
    "        except:\n",
    "            continue\n",
    "\n",
    "    print(f\"\\nAverage ROUGE-1 (F1) for {name} sample (200 articles): {sum(rouge_1_scores)/len(rouge_1_scores):.4f}\")\n",
    "    print(f\"Average ROUGE-2 (F1) for {name} sample (200 articles): {sum(rouge_2_scores)/len(rouge_2_scores):.4f}\")\n",
    "    print(f\"Average ROUGE-L (F1) for {name} sample (200 articles): {sum(rouge_l_scores)/len(rouge_l_scores):.4f}\")\n",
    "\n",
    "    return clf, vectorizer\n",
    "\n",
    "# Function to display predicted vs reference summaries\n",
    "def show_predictions(df, name, clf, vectorizer, n=5):\n",
    "    print(f\"\\n=== Sample Predicted vs Reference Summaries for {name} ===\")\n",
    "    sample_df = df.sample(n=n, random_state=1).reset_index(drop=True)\n",
    "    for i, row in sample_df.iterrows():\n",
    "        article = row['article']\n",
    "        true_summary = row['summary']\n",
    "        pred_summary = extract_summary(article, clf, vectorizer)\n",
    "\n",
    "        print(f\"\\n--- Article {i+1} ---\")\n",
    "        print(f\"Predicted Summary:\\n{pred_summary}\\n\")\n",
    "        print(f\"Reference Summary:\\n{true_summary}\\n\")\n",
    "        print(\"-\" * 80)\n",
    "\n",
    "# Ensure correct column names\n",
    "bbc_df = bbc_df.rename(columns={\"Article\": \"article\", \"Summary\": \"summary\"})\n",
    "imdb_df = imdb_df.rename(columns={\"Article\": \"article\", \"Summary\": \"summary\"})\n",
    "\n",
    "# Run and capture models\n",
    "bbc_clf, bbc_vectorizer = run_on_dataset(\"BBC\", bbc_df)\n",
    "imdb_clf, imdb_vectorizer = run_on_dataset(\"IMDB\", imdb_df)\n",
    "\n",
    "# Show sample predictions\n",
    "show_predictions(bbc_df, \"BBC\", bbc_clf, bbc_vectorizer, n=5)\n",
    "show_predictions(imdb_df, \"IMDB\", imdb_clf, imdb_vectorizer, n=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad028d46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
