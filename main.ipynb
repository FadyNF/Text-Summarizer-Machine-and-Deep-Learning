{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "89d0b769",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     /Users/mohamedkenya/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from utils import prepare_labeled_sentences, prepare_labeled_sentences_spacy\n",
    "import nltk\n",
    "nltk.download('punkt_tab')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fd3f019",
   "metadata": {},
   "source": [
    "Read Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7d4506ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BBC Dataset\n",
    "bbc_df = pd.read_csv(\"data/bbc/bbc_dataset.csv\")\n",
    "\n",
    "# CNN Datasets\n",
    "# cnn_train_df = pd.read_csv(\"data/cnn/cnn_dailymail_train.csv\")\n",
    "# cnn_valid_df = pd.read_csv(\"data/cnn/cnn_dailymail_valid.csv\")\n",
    "# cnn_test_df = pd.read_csv(\"data/cnn/cnn_dailymail_test.csv\")\n",
    "\n",
    "imdb_df = pd.read_csv(\"data/imdb/imdb.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "01e05b12",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BBC Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Musicians to tackle US red tape\\n\\nMusicians' ...</td>\n",
       "      <td>Nigel McCune from the Musicians' Union said Br...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>U2's desire to be number one\\n\\nU2, who have w...</td>\n",
       "      <td>But they still want more.They have to want to ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Rocker Doherty in on-stage fight\\n\\nRock singe...</td>\n",
       "      <td>Babyshambles, which he formed after his acrimo...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Snicket tops US box office chart\\n\\nThe film a...</td>\n",
       "      <td>A Series of Unfortunate Events also stars Scot...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Ocean's Twelve raids box office\\n\\nOcean's Twe...</td>\n",
       "      <td>Ocean's Twelve, the crime caper sequel starrin...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  \\\n",
       "0  Musicians to tackle US red tape\\n\\nMusicians' ...   \n",
       "1  U2's desire to be number one\\n\\nU2, who have w...   \n",
       "2  Rocker Doherty in on-stage fight\\n\\nRock singe...   \n",
       "3  Snicket tops US box office chart\\n\\nThe film a...   \n",
       "4  Ocean's Twelve raids box office\\n\\nOcean's Twe...   \n",
       "\n",
       "                                             Summary  \n",
       "0  Nigel McCune from the Musicians' Union said Br...  \n",
       "1  But they still want more.They have to want to ...  \n",
       "2  Babyshambles, which he formed after his acrimo...  \n",
       "3  A Series of Unfortunate Events also stars Scot...  \n",
       "4  Ocean's Twelve, the crime caper sequel starrin...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Preview to confirm structure\n",
    "print(\"BBC Sample:\")\n",
    "display(bbc_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "851acf97",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CNN Sample:\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cnn_train_df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCNN Sample:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 2\u001b[0m display(\u001b[43mcnn_train_df\u001b[49m\u001b[38;5;241m.\u001b[39mhead())\n",
      "\u001b[0;31mNameError\u001b[0m: name 'cnn_train_df' is not defined"
     ]
    }
   ],
   "source": [
    "# print(\"CNN Sample:\")\n",
    "# display(cnn_train_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a3b0b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "IMDB Sample:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Article</th>\n",
       "      <th>Summary</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production The filming tech...</td>\n",
       "      <td>A wonderful little production The filming tech...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was wonderful way to spend time...</td>\n",
       "      <td>I thought it was proof that Woody Allen is sti...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there a family where little boy Jake...</td>\n",
       "      <td>Basically there a family where little boy Jake...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei Love in the Time of Money is vis...</td>\n",
       "      <td>Petter Mattei Love in the Time of Money is vis...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                             Article  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production The filming tech...   \n",
       "2  I thought this was wonderful way to spend time...   \n",
       "3  Basically there a family where little boy Jake...   \n",
       "4  Petter Mattei Love in the Time of Money is vis...   \n",
       "\n",
       "                                             Summary  \n",
       "0  One of the other reviewers has mentioned that ...  \n",
       "1  A wonderful little production The filming tech...  \n",
       "2  I thought it was proof that Woody Allen is sti...  \n",
       "3  Basically there a family where little boy Jake...  \n",
       "4  Petter Mattei Love in the Time of Money is vis...  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"IMDB Sample:\")\n",
    "display(imdb_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed3c1a94",
   "metadata": {},
   "source": [
    "Preprocess BBC Datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b005f2d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing articles: 100%|██████████| 2225/2225 [04:42<00:00,  7.87it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process the BBC dataset\n",
    "bbc_labeled_data = prepare_labeled_sentences_spacy(bbc_df)\n",
    "\n",
    "# Convert to DataFrame for modeling\n",
    "bbc_processed_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"article_id\": item[\"article_id\"],\n",
    "            \"article_sentences\": item[\"raw_sentence\"],\n",
    "            \"preprocessed_sentence\": item[\"preprocessed_sentence\"],\n",
    "            \"label\": item[\"label\"],\n",
    "        }\n",
    "        for item in bbc_labeled_data\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3309143b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(41677, 4)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_processed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d6a9ee18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary sentences: 16543 out of 41677 (39.69%)\n",
      "\n",
      "Example summary sentences:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_sentences</th>\n",
       "      <th>preprocessed_sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Musicians to tackle US red tape  Musicians' gr...</td>\n",
       "      <td>musician tackle u red tape musician group tack...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A singer hoping to perform in the US can expec...</td>\n",
       "      <td>singer hop perform u expect pay simply obtain ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nigel McCune from the Musicians' Union said Br...</td>\n",
       "      <td>nigel mccune musician union say british musici...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   article_id                                  article_sentences  \\\n",
       "0           0  Musicians to tackle US red tape  Musicians' gr...   \n",
       "1           0  A singer hoping to perform in the US can expec...   \n",
       "4           0  Nigel McCune from the Musicians' Union said Br...   \n",
       "\n",
       "                               preprocessed_sentence  label  \n",
       "0  musician tackle u red tape musician group tack...      1  \n",
       "1  singer hop perform u expect pay simply obtain ...      1  \n",
       "4  nigel mccune musician union say british musici...      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count how many sentences are labeled as summary sentences\n",
    "summary_count = bbc_processed_df['label'].sum()\n",
    "total_count = len(bbc_processed_df)\n",
    "print(f\"Summary sentences: {summary_count} out of {total_count} ({summary_count/total_count:.2%})\")\n",
    "\n",
    "# Show some examples of sentences included in summaries\n",
    "print(\"\\nExample summary sentences:\")\n",
    "display(bbc_processed_df[bbc_processed_df['label'] == 1].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "65bf0a12",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_sentences</th>\n",
       "      <th>preprocessed_sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>Musicians to tackle US red tape  Musicians' gr...</td>\n",
       "      <td>musician tackle u red tape musician group tack...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>A singer hoping to perform in the US can expec...</td>\n",
       "      <td>singer hop perform u expect pay simply obtain ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>Groups including the Musicians' Union are call...</td>\n",
       "      <td>group include musician union call end raw deal...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>US acts are not faced with comparable expense ...</td>\n",
       "      <td>u act face comparable expense bureaucracy visi...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>Nigel McCune from the Musicians' Union said Br...</td>\n",
       "      <td>nigel mccune musician union say british musici...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>A sponsor has to make a petition on their beha...</td>\n",
       "      <td>sponsor make petition behalf form amount nearl...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>\"If you make a mistake on your form, you risk ...</td>\n",
       "      <td>make mistake form risk ban thus ability career...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>\"The US is the world's biggest music market, w...</td>\n",
       "      <td>u world big music market mean something creaky...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>\"The current situation is preventing British a...</td>\n",
       "      <td>current situation prevent british act maintain...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>The Musicians' Union stance is being endorsed ...</td>\n",
       "      <td>musician union stance endorse music manager fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>The MMF's general secretary James Seller said:...</td>\n",
       "      <td>mmf general secretary james seller say imagine...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>Every member would have to travel to London to...</td>\n",
       "      <td>every member would travel london visa process</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>\"The US market is seen as the holy grail and o...</td>\n",
       "      <td>u market see holy grail one benchmark success ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>\"It's still very important, but there are othe...</td>\n",
       "      <td>still important market like europe india china...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>A Department for Media, Culture and Sport spok...</td>\n",
       "      <td>department medium culture sport spokeswoman sa...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0</td>\n",
       "      <td>A US Embassy spokesman said: \"We are aware tha...</td>\n",
       "      <td>u embassy spokesman say aware entertainer requ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>\"We are aware of the importance of cultural ex...</td>\n",
       "      <td>aware importance cultural exchange best facili...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1</td>\n",
       "      <td>U2's desire to be number one  U2, who have won...</td>\n",
       "      <td>desire number one win three prestigious grammy...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1</td>\n",
       "      <td>The most popular groups in the history of rock...</td>\n",
       "      <td>popular group history rock several thing common</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>The music must be inspired and appeal across g...</td>\n",
       "      <td>music must inspire appeal across generation di...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1</td>\n",
       "      <td>But such success is down to more than music.</td>\n",
       "      <td>success music</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1</td>\n",
       "      <td>They have to be compelling performers, charism...</td>\n",
       "      <td>compel performer charismatic intelligent enoug...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1</td>\n",
       "      <td>They also have to want it.</td>\n",
       "      <td>also want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1</td>\n",
       "      <td>They have to want to be the biggest band ever ...</td>\n",
       "      <td>want big band ever stop want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1</td>\n",
       "      <td>The Beatles had it, the Rolling Stones still h...</td>\n",
       "      <td>beatles roll stone still rem hold onto queen c...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1</td>\n",
       "      <td>And U2 have it in spades, and keep churning it...</td>\n",
       "      <td>spade keep churn</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1</td>\n",
       "      <td>Their new album, How To Dismantle An Atomic Bo...</td>\n",
       "      <td>new album dismantle atomic bomb come year scho...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1</td>\n",
       "      <td>They may have lost some of the edginess and ra...</td>\n",
       "      <td>may lose edginess raw youthful force propel to...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1</td>\n",
       "      <td>Vertigo, the first single from the new album, ...</td>\n",
       "      <td>vertigo first single new album go straight uk ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>1</td>\n",
       "      <td>\"The challenge is to be bigger and bolder and ...</td>\n",
       "      <td>challenge big bolder good make record whole wo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1</td>\n",
       "      <td>Drummer Larry Mullen Jr echoed those sentiment...</td>\n",
       "      <td>drummer larry mullen jr echoed sentiment compe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1</td>\n",
       "      <td>We don't want to be thought of as a veteran ba...</td>\n",
       "      <td>want think veteran band</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1</td>\n",
       "      <td>The band have done \"everything in their consid...</td>\n",
       "      <td>band everything considerable power ensure rema...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1</td>\n",
       "      <td>\"This makes them hugely determined and formida...</td>\n",
       "      <td>make hugely determine formidable</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>1</td>\n",
       "      <td>He added: \"They are equally determined to push...</td>\n",
       "      <td>add equally determine push make music continue...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>1</td>\n",
       "      <td>\"As such, they've constantly re-invented and c...</td>\n",
       "      <td>constantly challenge</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>1</td>\n",
       "      <td>They are, perhaps, alone as the only rock band...</td>\n",
       "      <td>perhaps alone rock band get well age</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>1</td>\n",
       "      <td>The other key ingredient was the fact they wer...</td>\n",
       "      <td>key ingredient fact highly organise mr rees say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>1</td>\n",
       "      <td>\"They do everything in the right way.\"</td>\n",
       "      <td>everything right way</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>1</td>\n",
       "      <td>The group were born when Mullen put an appeal ...</td>\n",
       "      <td>group bear mullen put appeal bandmates high sc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>1</td>\n",
       "      <td>Dick Evans soon dropped out and the four-piece...</td>\n",
       "      <td>dick evans soon drop know feedback hype settle</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>1</td>\n",
       "      <td>By 1978, they had won a talent contest and got...</td>\n",
       "      <td>win talent contest get notice manager paul mcg...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>1</td>\n",
       "      <td>\"They were brilliant, but very coarse,\" McGuin...</td>\n",
       "      <td>brilliant coarse mcguinness recently say</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>1</td>\n",
       "      <td>\"In a way, they were doing exactly what they d...</td>\n",
       "      <td>way exactly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>1</td>\n",
       "      <td>Only badly.\"</td>\n",
       "      <td>badly</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>1</td>\n",
       "      <td>They struggled to attract record company atten...</td>\n",
       "      <td>struggle attract record company attention late...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>1</td>\n",
       "      <td>They released two Ireland-only singles, which ...</td>\n",
       "      <td>release two single top national chart lead dea...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>1</td>\n",
       "      <td>The stadium-filling, anthemic sound was U2's a...</td>\n",
       "      <td>anthemic sound aim start third album war saw m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>1</td>\n",
       "      <td>Songs like Sunday Bloody Sunday and New Year's...</td>\n",
       "      <td>song like sunday bloody sunday new year day br...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>1</td>\n",
       "      <td>His stage performances - which included flag-w...</td>\n",
       "      <td>stage performance include earn reputation elec...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>1</td>\n",
       "      <td>In 1987, The Joshua Tree broke sales records a...</td>\n",
       "      <td>joshua tree break sale record saw band reach h...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>1</td>\n",
       "      <td>Those songs took the band's epic, atmospheric ...</td>\n",
       "      <td>song take band epic atmospheric sound simple p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>1</td>\n",
       "      <td>The end of the decade marked a crucial point f...</td>\n",
       "      <td>end decade mark crucial point band reach top s...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>1</td>\n",
       "      <td>These came in the form of explorations of diff...</td>\n",
       "      <td>come form exploration different branch rock fo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>1</td>\n",
       "      <td>The Achtung Baby album in 1991 was followed by...</td>\n",
       "      <td>achtung baby album follow zooropa pop correspo...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>1</td>\n",
       "      <td>He was also building a parallel reputation - n...</td>\n",
       "      <td>also build parallel reputation always pleasure...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>1</td>\n",
       "      <td>Before the release of How To Dismantle An Atom...</td>\n",
       "      <td>release dismantle atomic bomb sell million alb...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>1</td>\n",
       "      <td>But they still want more.</td>\n",
       "      <td>still want</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>2</td>\n",
       "      <td>Rocker Doherty in on-stage fight  Rock singer ...</td>\n",
       "      <td>rocker doherty fight rock singer pete doherty ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>2</td>\n",
       "      <td>Babyshambles played for 5,000 fans at London's...</td>\n",
       "      <td>babyshambles play fan london brixton academy t...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    article_id                                  article_sentences  \\\n",
       "0            0  Musicians to tackle US red tape  Musicians' gr...   \n",
       "1            0  A singer hoping to perform in the US can expec...   \n",
       "2            0  Groups including the Musicians' Union are call...   \n",
       "3            0  US acts are not faced with comparable expense ...   \n",
       "4            0  Nigel McCune from the Musicians' Union said Br...   \n",
       "5            0  A sponsor has to make a petition on their beha...   \n",
       "6            0  \"If you make a mistake on your form, you risk ...   \n",
       "7            0  \"The US is the world's biggest music market, w...   \n",
       "8            0  \"The current situation is preventing British a...   \n",
       "9            0  The Musicians' Union stance is being endorsed ...   \n",
       "10           0  The MMF's general secretary James Seller said:...   \n",
       "11           0  Every member would have to travel to London to...   \n",
       "12           0  \"The US market is seen as the holy grail and o...   \n",
       "13           0  \"It's still very important, but there are othe...   \n",
       "14           0  A Department for Media, Culture and Sport spok...   \n",
       "15           0  A US Embassy spokesman said: \"We are aware tha...   \n",
       "16           0  \"We are aware of the importance of cultural ex...   \n",
       "17           1  U2's desire to be number one  U2, who have won...   \n",
       "18           1  The most popular groups in the history of rock...   \n",
       "19           1  The music must be inspired and appeal across g...   \n",
       "20           1       But such success is down to more than music.   \n",
       "21           1  They have to be compelling performers, charism...   \n",
       "22           1                         They also have to want it.   \n",
       "23           1  They have to want to be the biggest band ever ...   \n",
       "24           1  The Beatles had it, the Rolling Stones still h...   \n",
       "25           1  And U2 have it in spades, and keep churning it...   \n",
       "26           1  Their new album, How To Dismantle An Atomic Bo...   \n",
       "27           1  They may have lost some of the edginess and ra...   \n",
       "28           1  Vertigo, the first single from the new album, ...   \n",
       "29           1  \"The challenge is to be bigger and bolder and ...   \n",
       "30           1  Drummer Larry Mullen Jr echoed those sentiment...   \n",
       "31           1  We don't want to be thought of as a veteran ba...   \n",
       "32           1  The band have done \"everything in their consid...   \n",
       "33           1  \"This makes them hugely determined and formida...   \n",
       "34           1  He added: \"They are equally determined to push...   \n",
       "35           1  \"As such, they've constantly re-invented and c...   \n",
       "36           1  They are, perhaps, alone as the only rock band...   \n",
       "37           1  The other key ingredient was the fact they wer...   \n",
       "38           1             \"They do everything in the right way.\"   \n",
       "39           1  The group were born when Mullen put an appeal ...   \n",
       "40           1  Dick Evans soon dropped out and the four-piece...   \n",
       "41           1  By 1978, they had won a talent contest and got...   \n",
       "42           1  \"They were brilliant, but very coarse,\" McGuin...   \n",
       "43           1  \"In a way, they were doing exactly what they d...   \n",
       "44           1                                       Only badly.\"   \n",
       "45           1  They struggled to attract record company atten...   \n",
       "46           1  They released two Ireland-only singles, which ...   \n",
       "47           1  The stadium-filling, anthemic sound was U2's a...   \n",
       "48           1  Songs like Sunday Bloody Sunday and New Year's...   \n",
       "49           1  His stage performances - which included flag-w...   \n",
       "50           1  In 1987, The Joshua Tree broke sales records a...   \n",
       "51           1  Those songs took the band's epic, atmospheric ...   \n",
       "52           1  The end of the decade marked a crucial point f...   \n",
       "53           1  These came in the form of explorations of diff...   \n",
       "54           1  The Achtung Baby album in 1991 was followed by...   \n",
       "55           1  He was also building a parallel reputation - n...   \n",
       "56           1  Before the release of How To Dismantle An Atom...   \n",
       "57           1                          But they still want more.   \n",
       "58           2  Rocker Doherty in on-stage fight  Rock singer ...   \n",
       "59           2  Babyshambles played for 5,000 fans at London's...   \n",
       "\n",
       "                                preprocessed_sentence  label  \n",
       "0   musician tackle u red tape musician group tack...      1  \n",
       "1   singer hop perform u expect pay simply obtain ...      1  \n",
       "2   group include musician union call end raw deal...      0  \n",
       "3   u act face comparable expense bureaucracy visi...      0  \n",
       "4   nigel mccune musician union say british musici...      1  \n",
       "5   sponsor make petition behalf form amount nearl...      0  \n",
       "6   make mistake form risk ban thus ability career...      0  \n",
       "7   u world big music market mean something creaky...      1  \n",
       "8   current situation prevent british act maintain...      1  \n",
       "9   musician union stance endorse music manager fo...      1  \n",
       "10  mmf general secretary james seller say imagine...      0  \n",
       "11      every member would travel london visa process      0  \n",
       "12  u market see holy grail one benchmark success ...      0  \n",
       "13  still important market like europe india china...      0  \n",
       "14  department medium culture sport spokeswoman sa...      0  \n",
       "15  u embassy spokesman say aware entertainer requ...      1  \n",
       "16  aware importance cultural exchange best facili...      0  \n",
       "17  desire number one win three prestigious grammy...      1  \n",
       "18    popular group history rock several thing common      0  \n",
       "19  music must inspire appeal across generation di...      0  \n",
       "20                                      success music      0  \n",
       "21  compel performer charismatic intelligent enoug...      0  \n",
       "22                                          also want      1  \n",
       "23                       want big band ever stop want      1  \n",
       "24  beatles roll stone still rem hold onto queen c...      0  \n",
       "25                                   spade keep churn      0  \n",
       "26  new album dismantle atomic bomb come year scho...      1  \n",
       "27  may lose edginess raw youthful force propel to...      0  \n",
       "28  vertigo first single new album go straight uk ...      1  \n",
       "29  challenge big bolder good make record whole wo...      1  \n",
       "30  drummer larry mullen jr echoed sentiment compe...      0  \n",
       "31                            want think veteran band      1  \n",
       "32  band everything considerable power ensure rema...      1  \n",
       "33                   make hugely determine formidable      0  \n",
       "34  add equally determine push make music continue...      0  \n",
       "35                               constantly challenge      0  \n",
       "36               perhaps alone rock band get well age      1  \n",
       "37    key ingredient fact highly organise mr rees say      0  \n",
       "38                               everything right way      0  \n",
       "39  group bear mullen put appeal bandmates high sc...      0  \n",
       "40     dick evans soon drop know feedback hype settle      0  \n",
       "41  win talent contest get notice manager paul mcg...      0  \n",
       "42           brilliant coarse mcguinness recently say      0  \n",
       "43                                        way exactly      0  \n",
       "44                                              badly      0  \n",
       "45  struggle attract record company attention late...      0  \n",
       "46  release two single top national chart lead dea...      0  \n",
       "47  anthemic sound aim start third album war saw m...      1  \n",
       "48  song like sunday bloody sunday new year day br...      1  \n",
       "49  stage performance include earn reputation elec...      0  \n",
       "50  joshua tree break sale record saw band reach h...      1  \n",
       "51  song take band epic atmospheric sound simple p...      0  \n",
       "52  end decade mark crucial point band reach top s...      1  \n",
       "53  come form exploration different branch rock fo...      1  \n",
       "54  achtung baby album follow zooropa pop correspo...      0  \n",
       "55  also build parallel reputation always pleasure...      0  \n",
       "56  release dismantle atomic bomb sell million alb...      1  \n",
       "57                                         still want      1  \n",
       "58  rocker doherty fight rock singer pete doherty ...      1  \n",
       "59  babyshambles play fan london brixton academy t...      1  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bbc_processed_df.head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "afa7f591",
   "metadata": {},
   "source": [
    "Preprocessed IMDB Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "4ac0d9c3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Preprocessing articles: 100%|██████████| 4000/4000 [03:21<00:00, 19.80it/s]\n"
     ]
    }
   ],
   "source": [
    "# Process the BBC dataset\n",
    "imdb_labeled_df = prepare_labeled_sentences_spacy(imdb_df[:4000])\n",
    "\n",
    "# Convert to DataFrame for modeling\n",
    "imdb_processed_df = pd.DataFrame(\n",
    "    [\n",
    "        {\n",
    "            \"article_id\": item[\"article_id\"],\n",
    "            \"article_sentences\": item[\"raw_sentence\"],\n",
    "            \"preprocessed_sentence\": item[\"preprocessed_sentence\"],\n",
    "            \"label\": item[\"label\"],\n",
    "        }\n",
    "        for item in imdb_labeled_df\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a67dd484",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(13024, 4)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_processed_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "64901be6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Summary sentences: 2934 out of 13024 (22.53%)\n",
      "\n",
      "Example summary sentences:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_sentences</th>\n",
       "      <th>preprocessed_sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production The filming tech...</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>Basically there a family where little boy Jake...</td>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei Love in the Time of Money is vis...</td>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    article_id                                  article_sentences  \\\n",
       "2            1  A wonderful little production The filming tech...   \n",
       "9            3  Basically there a family where little boy Jake...   \n",
       "11           4  Petter Mattei Love in the Time of Money is vis...   \n",
       "\n",
       "                                preprocessed_sentence  label  \n",
       "2   wonderful little production filming technique ...      1  \n",
       "9   basically family little boy jake think zombie ...      1  \n",
       "11  petter mattei love time money visually stunnin...      1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Count how many sentences are labeled as summary sentences\n",
    "summary_count = imdb_processed_df['label'].sum()\n",
    "total_count = len(imdb_processed_df)\n",
    "print(f\"Summary sentences: {summary_count} out of {total_count} ({summary_count/total_count:.2%})\")\n",
    "\n",
    "# Show some examples of sentences included in summaries\n",
    "print(\"\\nExample summary sentences:\")\n",
    "display(imdb_processed_df[imdb_processed_df['label'] == 1].head(3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a84aae2",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'raw_sentence'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3805\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3804\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 3805\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcasted_key\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3806\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m err:\n",
      "File \u001b[0;32mindex.pyx:167\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mindex.pyx:196\u001b[0m, in \u001b[0;36mpandas._libs.index.IndexEngine.get_loc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7081\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mpandas/_libs/hashtable_class_helper.pxi:7089\u001b[0m, in \u001b[0;36mpandas._libs.hashtable.PyObjectHashTable.get_item\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'raw_sentence'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[43mimdb_processed_df\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mraw_sentence\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m]\u001b[49m[\u001b[38;5;241m2\u001b[39m])\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/frame.py:4102\u001b[0m, in \u001b[0;36mDataFrame.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   4100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcolumns\u001b[38;5;241m.\u001b[39mnlevels \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m   4101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_getitem_multilevel(key)\n\u001b[0;32m-> 4102\u001b[0m indexer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcolumns\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_loc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   4103\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m is_integer(indexer):\n\u001b[1;32m   4104\u001b[0m     indexer \u001b[38;5;241m=\u001b[39m [indexer]\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/pandas/core/indexes/base.py:3812\u001b[0m, in \u001b[0;36mIndex.get_loc\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3807\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(casted_key, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (\n\u001b[1;32m   3808\u001b[0m         \u001b[38;5;28misinstance\u001b[39m(casted_key, abc\u001b[38;5;241m.\u001b[39mIterable)\n\u001b[1;32m   3809\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28many\u001b[39m(\u001b[38;5;28misinstance\u001b[39m(x, \u001b[38;5;28mslice\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m x \u001b[38;5;129;01min\u001b[39;00m casted_key)\n\u001b[1;32m   3810\u001b[0m     ):\n\u001b[1;32m   3811\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m InvalidIndexError(key)\n\u001b[0;32m-> 3812\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(key) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01merr\u001b[39;00m\n\u001b[1;32m   3813\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m:\n\u001b[1;32m   3814\u001b[0m     \u001b[38;5;66;03m# If we have a listlike key, _check_indexing_error will raise\u001b[39;00m\n\u001b[1;32m   3815\u001b[0m     \u001b[38;5;66;03m#  InvalidIndexError. Otherwise we fall through and re-raise\u001b[39;00m\n\u001b[1;32m   3816\u001b[0m     \u001b[38;5;66;03m#  the TypeError.\u001b[39;00m\n\u001b[1;32m   3817\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_indexing_error(key)\n",
      "\u001b[0;31mKeyError\u001b[0m: 'raw_sentence'"
     ]
    }
   ],
   "source": [
    "# print(imdb_processed_df[\"raw_sentence\"][2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "2e1a68bd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>article_id</th>\n",
       "      <th>article_sentences</th>\n",
       "      <th>preprocessed_sentence</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>one reviewer mention watch oz episode hook rig...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>This show pulls no punches with regards to dru...</td>\n",
       "      <td>show pull punch regard drug sex violence hardc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>A wonderful little production The filming tech...</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1</td>\n",
       "      <td>but he has all the voices down pat too You can...</td>\n",
       "      <td>voice pat truly see seamless edit guide refere...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>but it is terrificly written and performed pie...</td>\n",
       "      <td>terrificly write perform piece masterful produ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1</td>\n",
       "      <td>The realism really comes home with the little ...</td>\n",
       "      <td>realism really come home little thing fantasy ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>2</td>\n",
       "      <td>I thought this was wonderful way to spend time...</td>\n",
       "      <td>think wonderful way spend time hot summer week...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>2</td>\n",
       "      <td>The plot is simplistic but the dialogue is wit...</td>\n",
       "      <td>plot simplistic dialogue witty character likab...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>2</td>\n",
       "      <td>While some may be disappointed when they reali...</td>\n",
       "      <td>may disappoint realize match point risk addict...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>Basically there a family where little boy Jake...</td>\n",
       "      <td>basically family little boy jake think zombie ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>3</td>\n",
       "      <td>And then we have Jake with his closet which to...</td>\n",
       "      <td>jake closet totally ruin film expect see booge...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4</td>\n",
       "      <td>Petter Mattei Love in the Time of Money is vis...</td>\n",
       "      <td>petter mattei love time money visually stunnin...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4</td>\n",
       "      <td>This is movie that seems to be telling us what...</td>\n",
       "      <td>movie seem tell u money power success people d...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4</td>\n",
       "      <td>Kane Michael Imperioli Adrian Grenier and the ...</td>\n",
       "      <td>kane michael imperioli adrian grenier rest tal...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>4</td>\n",
       "      <td>We wish Mr Mattei good luck and await anxiousl...</td>\n",
       "      <td>wish mr mattei good luck await anxiously next ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5</td>\n",
       "      <td>Probably my all time favorite movie story of s...</td>\n",
       "      <td>probably time favorite movie story selflessnes...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5</td>\n",
       "      <td>The kids are as grandma says more like dressed...</td>\n",
       "      <td>kid grandma say like dress midget child make f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>6</td>\n",
       "      <td>I sure would like to see resurrection of up da...</td>\n",
       "      <td>sure would like see resurrection dated seahunt...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>6</td>\n",
       "      <td>Oh by the way thank you for an outlet like thi...</td>\n",
       "      <td>oh way thank outlet like view many viewpoint t...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>6</td>\n",
       "      <td>So any ole way believe ve got what wanna say W...</td>\n",
       "      <td>ole way believe get wan na say would nice read...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>7</td>\n",
       "      <td>This show was an amazing fresh innovative idea...</td>\n",
       "      <td>show amazing fresh innovative idea first air f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>8</td>\n",
       "      <td>Encouraged by the positive comments about this...</td>\n",
       "      <td>encourage positive comment film look forward w...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>9</td>\n",
       "      <td>If you like original gut wrenching laughter yo...</td>\n",
       "      <td>like original gut wrench laughter like movie y...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>10</td>\n",
       "      <td>Phil the Alien is one of those quirky films wh...</td>\n",
       "      <td>phil alien one quirky film humour base around ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>10</td>\n",
       "      <td>At first it was very odd and pretty funny but ...</td>\n",
       "      <td>first odd pretty funny movie progress find jok...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>10</td>\n",
       "      <td>anymore</td>\n",
       "      <td>anymore</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>10</td>\n",
       "      <td>Its low budget film thats never problem in its...</td>\n",
       "      <td>low budget film thats never problem pretty int...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>11</td>\n",
       "      <td>I saw this movie when was about when it came o...</td>\n",
       "      <td>saw movie come recall scary scene big bird eat...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>11</td>\n",
       "      <td>The horror As young kid going to these cheesy ...</td>\n",
       "      <td>horror young kid go cheesy film saturday after...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>12</td>\n",
       "      <td>So im not big fan of Boll work but then again ...</td>\n",
       "      <td>im big fan boll work many enjoy movie</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>12</td>\n",
       "      <td>Postal maybe im the only one Boll apparently b...</td>\n",
       "      <td>postal maybe im one boll apparently buy right ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>12</td>\n",
       "      <td>So the tale goes like this Jack Carver played ...</td>\n",
       "      <td>tale go like jack carver play til schweiger</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>12</td>\n",
       "      <td>yes Carver is German all hail the bratwurst ea...</td>\n",
       "      <td>yes carver german hail bratwurst eat dude howe...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>12</td>\n",
       "      <td>but we only saw carver in first person perspec...</td>\n",
       "      <td>saw carver first person perspective</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>12</td>\n",
       "      <td>so we don really know what he looked like when...</td>\n",
       "      <td>really know look like kick however storyline f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>12</td>\n",
       "      <td>We see the evil mad scientist Dr Krieger playe...</td>\n",
       "      <td>see evil mad scientist dr krieger play udo kie...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>13</td>\n",
       "      <td>The cast played Shakespeare Shakespeare lost a...</td>\n",
       "      <td>cast play shakespeare shakespeare lose appreci...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>14</td>\n",
       "      <td>This fantastic movie of three prisoners who be...</td>\n",
       "      <td>fantastic movie three prisoner become famous o...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>14</td>\n",
       "      <td>but this roll is not bad Another good thing ab...</td>\n",
       "      <td>roll bad another good thing movie soundtrack m...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>15</td>\n",
       "      <td>Kind of drawn in by the erotic scenes only to ...</td>\n",
       "      <td>kind drawn erotic scene realize one amateurish...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>15</td>\n",
       "      <td>What was with the bisexual relationship out of...</td>\n",
       "      <td>bisexual relationship nowhere heterosexual enc...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>15</td>\n",
       "      <td>And what was with that absurd dance with every...</td>\n",
       "      <td>absurd dance everybody play stereotyped role g...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>16</td>\n",
       "      <td>Some films just simply should not be remade Th...</td>\n",
       "      <td>film simply remake one bad film fail capture f...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>16</td>\n",
       "      <td>But you will enjoy the friction of terror in t...</td>\n",
       "      <td>enjoy friction terror old version much</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>17</td>\n",
       "      <td>This movie made it into one of my top most awf...</td>\n",
       "      <td>movie make one top awful movie horrible contin...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>17</td>\n",
       "      <td>The ghost scene at the end was stolen from the...</td>\n",
       "      <td>ghost scene end steal final scene old star war...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>17</td>\n",
       "      <td>hello</td>\n",
       "      <td>hello</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>17</td>\n",
       "      <td>And the whole machine vs humans theme WAS the ...</td>\n",
       "      <td>whole machine v human theme matrix terminator ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>18</td>\n",
       "      <td>I remember this film it was the first film had...</td>\n",
       "      <td>remember film first film watch cinema picture ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>19</td>\n",
       "      <td>An awful film It must have been up against som...</td>\n",
       "      <td>awful film must real stinker nominate golden g...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>19</td>\n",
       "      <td>They ve taken the story of the first famous fe...</td>\n",
       "      <td>take story first famous female renaissance pai...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>19</td>\n",
       "      <td>My complaint is not that they ve taken liberti...</td>\n",
       "      <td>complaint take liberty fact story good would p...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>19</td>\n",
       "      <td>But it simply bizarre by all accounts the true...</td>\n",
       "      <td>simply bizarre account true story artist would...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>19</td>\n",
       "      <td>so why did they come up with this dishwater du...</td>\n",
       "      <td>come dishwater dull script suppose enough nake...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>20</td>\n",
       "      <td>After the success of Die Hard and it sequels i...</td>\n",
       "      <td>success die hard sequels surprise really glut ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>20</td>\n",
       "      <td>However if you an forget all the nonsense it a...</td>\n",
       "      <td>however forget nonsense actually lovable unden...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>20</td>\n",
       "      <td>And whilst he surely can be it really does loo...</td>\n",
       "      <td>whilst surely really look like ralph waite fra...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>20</td>\n",
       "      <td>yes you can help enjoy that bit Hal needed goo...</td>\n",
       "      <td>yes help enjoy bit hal need good kicking</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>20</td>\n",
       "      <td>So forget your better judgement who cares if t...</td>\n",
       "      <td>forget good judgement care could never happen ...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>20</td>\n",
       "      <td>And if you re looking for Qaulen he the one we...</td>\n",
       "      <td>look qaulen one wear helicopter</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    article_id                                  article_sentences  \\\n",
       "0            0  One of the other reviewers has mentioned that ...   \n",
       "1            0  This show pulls no punches with regards to dru...   \n",
       "2            1  A wonderful little production The filming tech...   \n",
       "3            1  but he has all the voices down pat too You can...   \n",
       "4            1  but it is terrificly written and performed pie...   \n",
       "5            1  The realism really comes home with the little ...   \n",
       "6            2  I thought this was wonderful way to spend time...   \n",
       "7            2  The plot is simplistic but the dialogue is wit...   \n",
       "8            2  While some may be disappointed when they reali...   \n",
       "9            3  Basically there a family where little boy Jake...   \n",
       "10           3  And then we have Jake with his closet which to...   \n",
       "11           4  Petter Mattei Love in the Time of Money is vis...   \n",
       "12           4  This is movie that seems to be telling us what...   \n",
       "13           4  Kane Michael Imperioli Adrian Grenier and the ...   \n",
       "14           4  We wish Mr Mattei good luck and await anxiousl...   \n",
       "15           5  Probably my all time favorite movie story of s...   \n",
       "16           5  The kids are as grandma says more like dressed...   \n",
       "17           6  I sure would like to see resurrection of up da...   \n",
       "18           6  Oh by the way thank you for an outlet like thi...   \n",
       "19           6  So any ole way believe ve got what wanna say W...   \n",
       "20           7  This show was an amazing fresh innovative idea...   \n",
       "21           8  Encouraged by the positive comments about this...   \n",
       "22           9  If you like original gut wrenching laughter yo...   \n",
       "23          10  Phil the Alien is one of those quirky films wh...   \n",
       "24          10  At first it was very odd and pretty funny but ...   \n",
       "25          10                                            anymore   \n",
       "26          10  Its low budget film thats never problem in its...   \n",
       "27          11  I saw this movie when was about when it came o...   \n",
       "28          11  The horror As young kid going to these cheesy ...   \n",
       "29          12  So im not big fan of Boll work but then again ...   \n",
       "30          12  Postal maybe im the only one Boll apparently b...   \n",
       "31          12  So the tale goes like this Jack Carver played ...   \n",
       "32          12  yes Carver is German all hail the bratwurst ea...   \n",
       "33          12  but we only saw carver in first person perspec...   \n",
       "34          12  so we don really know what he looked like when...   \n",
       "35          12  We see the evil mad scientist Dr Krieger playe...   \n",
       "36          13  The cast played Shakespeare Shakespeare lost a...   \n",
       "37          14  This fantastic movie of three prisoners who be...   \n",
       "38          14  but this roll is not bad Another good thing ab...   \n",
       "39          15  Kind of drawn in by the erotic scenes only to ...   \n",
       "40          15  What was with the bisexual relationship out of...   \n",
       "41          15  And what was with that absurd dance with every...   \n",
       "42          16  Some films just simply should not be remade Th...   \n",
       "43          16  But you will enjoy the friction of terror in t...   \n",
       "44          17  This movie made it into one of my top most awf...   \n",
       "45          17  The ghost scene at the end was stolen from the...   \n",
       "46          17                                              hello   \n",
       "47          17  And the whole machine vs humans theme WAS the ...   \n",
       "48          18  I remember this film it was the first film had...   \n",
       "49          19  An awful film It must have been up against som...   \n",
       "50          19  They ve taken the story of the first famous fe...   \n",
       "51          19  My complaint is not that they ve taken liberti...   \n",
       "52          19  But it simply bizarre by all accounts the true...   \n",
       "53          19  so why did they come up with this dishwater du...   \n",
       "54          20  After the success of Die Hard and it sequels i...   \n",
       "55          20  However if you an forget all the nonsense it a...   \n",
       "56          20  And whilst he surely can be it really does loo...   \n",
       "57          20  yes you can help enjoy that bit Hal needed goo...   \n",
       "58          20  So forget your better judgement who cares if t...   \n",
       "59          20  And if you re looking for Qaulen he the one we...   \n",
       "\n",
       "                                preprocessed_sentence  label  \n",
       "0   one reviewer mention watch oz episode hook rig...      0  \n",
       "1   show pull punch regard drug sex violence hardc...      0  \n",
       "2   wonderful little production filming technique ...      1  \n",
       "3   voice pat truly see seamless edit guide refere...      0  \n",
       "4   terrificly write perform piece masterful produ...      0  \n",
       "5   realism really come home little thing fantasy ...      0  \n",
       "6   think wonderful way spend time hot summer week...      0  \n",
       "7   plot simplistic dialogue witty character likab...      0  \n",
       "8   may disappoint realize match point risk addict...      0  \n",
       "9   basically family little boy jake think zombie ...      1  \n",
       "10  jake closet totally ruin film expect see booge...      0  \n",
       "11  petter mattei love time money visually stunnin...      1  \n",
       "12  movie seem tell u money power success people d...      0  \n",
       "13  kane michael imperioli adrian grenier rest tal...      1  \n",
       "14  wish mr mattei good luck await anxiously next ...      0  \n",
       "15  probably time favorite movie story selflessnes...      1  \n",
       "16  kid grandma say like dress midget child make f...      0  \n",
       "17  sure would like see resurrection dated seahunt...      1  \n",
       "18  oh way thank outlet like view many viewpoint t...      0  \n",
       "19  ole way believe get wan na say would nice read...      0  \n",
       "20  show amazing fresh innovative idea first air f...      0  \n",
       "21  encourage positive comment film look forward w...      0  \n",
       "22  like original gut wrench laughter like movie y...      1  \n",
       "23  phil alien one quirky film humour base around ...      1  \n",
       "24  first odd pretty funny movie progress find jok...      1  \n",
       "25                                            anymore      0  \n",
       "26  low budget film thats never problem pretty int...      0  \n",
       "27  saw movie come recall scary scene big bird eat...      1  \n",
       "28  horror young kid go cheesy film saturday after...      0  \n",
       "29              im big fan boll work many enjoy movie      0  \n",
       "30  postal maybe im one boll apparently buy right ...      0  \n",
       "31        tale go like jack carver play til schweiger      0  \n",
       "32  yes carver german hail bratwurst eat dude howe...      0  \n",
       "33                saw carver first person perspective      0  \n",
       "34  really know look like kick however storyline f...      0  \n",
       "35  see evil mad scientist dr krieger play udo kie...      0  \n",
       "36  cast play shakespeare shakespeare lose appreci...      0  \n",
       "37  fantastic movie three prisoner become famous o...      1  \n",
       "38  roll bad another good thing movie soundtrack m...      1  \n",
       "39  kind drawn erotic scene realize one amateurish...      0  \n",
       "40  bisexual relationship nowhere heterosexual enc...      0  \n",
       "41  absurd dance everybody play stereotyped role g...      0  \n",
       "42  film simply remake one bad film fail capture f...      0  \n",
       "43             enjoy friction terror old version much      0  \n",
       "44  movie make one top awful movie horrible contin...      0  \n",
       "45  ghost scene end steal final scene old star war...      0  \n",
       "46                                              hello      0  \n",
       "47  whole machine v human theme matrix terminator ...      0  \n",
       "48  remember film first film watch cinema picture ...      0  \n",
       "49  awful film must real stinker nominate golden g...      1  \n",
       "50  take story first famous female renaissance pai...      1  \n",
       "51  complaint take liberty fact story good would p...      0  \n",
       "52  simply bizarre account true story artist would...      0  \n",
       "53  come dishwater dull script suppose enough nake...      0  \n",
       "54  success die hard sequels surprise really glut ...      0  \n",
       "55  however forget nonsense actually lovable unden...      0  \n",
       "56  whilst surely really look like ralph waite fra...      0  \n",
       "57           yes help enjoy bit hal need good kicking      0  \n",
       "58  forget good judgement care could never happen ...      0  \n",
       "59                    look qaulen one wear helicopter      0  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imdb_processed_df.head(60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ee6068",
   "metadata": {},
   "source": [
    "BiLSTM + Attention\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16289cd7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training on BBC Dataset ===\n",
      "Epoch 1 | Loss: 8.0298\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[41], line 160\u001b[0m\n\u001b[1;32m    157\u001b[0m imdb_df \u001b[38;5;241m=\u001b[39m imdb_df\u001b[38;5;241m.\u001b[39mrename(columns\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mArticle\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124marticle\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSummary\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msummary\u001b[39m\u001b[38;5;124m\"\u001b[39m})\n\u001b[1;32m    159\u001b[0m \u001b[38;5;66;03m# Run the pipeline\u001b[39;00m\n\u001b[0;32m--> 160\u001b[0m \u001b[43mrun_summarization_pipeline\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mBBC\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbbc_df\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m run_summarization_pipeline(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mimdb\u001b[39m\u001b[38;5;124m\"\u001b[39m, imdb_df)\n",
      "Cell \u001b[0;32mIn[41], line 147\u001b[0m, in \u001b[0;36mrun_summarization_pipeline\u001b[0;34m(name, df)\u001b[0m\n\u001b[1;32m    144\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m optim\u001b[38;5;241m.\u001b[39mAdam(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m)\n\u001b[1;32m    146\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m--> 147\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    148\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch\u001b[38;5;241m+\u001b[39m\u001b[38;5;241m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m | Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mloss\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.4f\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    150\u001b[0m rouge_scores \u001b[38;5;241m=\u001b[39m evaluate(model, test_loader, idx2word, device)\n",
      "Cell \u001b[0;32mIn[41], line 101\u001b[0m, in \u001b[0;36mtrain\u001b[0;34m(model, loader, criterion, optimizer, device, clip)\u001b[0m\n\u001b[1;32m     99\u001b[0m output \u001b[38;5;241m=\u001b[39m model(src, tgt[:, :\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    100\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(output\u001b[38;5;241m.\u001b[39mview(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, output\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]), tgt[:, \u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m--> 101\u001b[0m \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    102\u001b[0m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mutils\u001b[38;5;241m.\u001b[39mclip_grad_norm_(model\u001b[38;5;241m.\u001b[39mparameters(), clip)\n\u001b[1;32m    103\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/_tensor.py:648\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    638\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    639\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    640\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    641\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    646\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    647\u001b[0m     )\n\u001b[0;32m--> 648\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    649\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    650\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/__init__.py:353\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    348\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    350\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    351\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    352\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 353\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    354\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    355\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    356\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    357\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    358\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    359\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    360\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    361\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/Library/Frameworks/Python.framework/Versions/3.12/lib/python3.12/site-packages/torch/autograd/graph.py:824\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    822\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    823\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 824\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    825\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    826\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    827\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    828\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Required imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "\n",
    "# Tokenizer utility\n",
    "def tokenize(text):\n",
    "    return text.lower().split()\n",
    "\n",
    "# Create vocabulary\n",
    "def build_vocab(texts, min_freq=2):\n",
    "    counter = Counter(word for text in texts for word in tokenize(text))\n",
    "    vocab = ['<PAD>', '<UNK>', '<SOS>', '<EOS>'] + [word for word, count in counter.items() if count >= min_freq]\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "# Dataset class for summarization\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, articles, summaries, word2idx, max_len=100):\n",
    "        self.articles = articles\n",
    "        self.summaries = summaries\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def encode(self, text, add_sos_eos=False):\n",
    "        tokens = tokenize(text)\n",
    "        if add_sos_eos:\n",
    "            tokens = ['<SOS>'] + tokens + ['<EOS>']\n",
    "        ids = [self.word2idx.get(t, self.word2idx['<UNK>']) for t in tokens]\n",
    "        padded = ids[:self.max_len] + [self.word2idx['<PAD>']] * (self.max_len - len(ids))\n",
    "        return torch.tensor(padded)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = self.encode(self.articles[idx])\n",
    "        summary = self.encode(self.summaries[idx], add_sos_eos=True)\n",
    "        return article, summary\n",
    "\n",
    "# Bahdanau Attention\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, enc_dim, dec_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_dim + dec_dim, dec_dim)\n",
    "        self.v = nn.Linear(dec_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden):\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((encoder_outputs, hidden), dim=2)))\n",
    "        attn_weights = torch.softmax(self.v(energy).squeeze(2), dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        return context\n",
    "\n",
    "# BiLSTM + Attention model\n",
    "class Seq2SeqAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, batch_first=True, bidirectional=True, num_layers=2, dropout=0.3)\n",
    "        self.decoder = nn.LSTMCell(embedding_dim + hidden_dim * 2, hidden_dim * 2)\n",
    "        self.attn = BahdanauAttention(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 4, vocab_size)\n",
    "\n",
    "    def forward(self, src, tgt):\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        embedded_src = self.dropout(self.embedding(src))\n",
    "        encoder_outputs, _ = self.encoder(embedded_src)\n",
    "        embedded_tgt = self.dropout(self.embedding(tgt))\n",
    "\n",
    "        h = encoder_outputs.mean(dim=1)\n",
    "        c = h\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(tgt_len):\n",
    "            context = self.attn(encoder_outputs, h)\n",
    "            rnn_input = torch.cat((embedded_tgt[:, t], context), dim=1)\n",
    "            h, c = self.decoder(rnn_input, (h, c))\n",
    "            output = self.fc_out(torch.cat((h, context), dim=1))\n",
    "            outputs.append(output.unsqueeze(1))\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# Training function\n",
    "def train(model, loader, criterion, optimizer, device, clip=1):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for src, tgt in loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt[:, :-1])\n",
    "        loss = criterion(output.view(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        torch.nn.utils.clip_grad_norm_(model.parameters(), clip)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Evaluation function\n",
    "def evaluate(model, loader, idx2word, device):\n",
    "    model.eval()\n",
    "    rouge = Rouge()\n",
    "    predictions = []\n",
    "    references = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in loader:\n",
    "            src = src.to(device)\n",
    "            output = model(src, tgt[:, :-1].to(device))\n",
    "            pred_ids = output.argmax(dim=2).cpu().numpy()\n",
    "            tgt_ids = tgt[:, 1:].cpu().numpy()\n",
    "\n",
    "            for pred, ref in zip(pred_ids, tgt_ids):\n",
    "                pred_words = [idx2word.get(i, '') for i in pred if i not in (0, 1, 0)]\n",
    "                ref_words = [idx2word.get(i, '') for i in ref if i not in (0, 1, 0)]\n",
    "                predictions.append(\" \".join(pred_words))\n",
    "                references.append(\" \".join(ref_words))\n",
    "\n",
    "    scores = rouge.get_scores(predictions, references, avg=True)\n",
    "    return scores\n",
    "\n",
    "# Pipeline\n",
    "def run_summarization_pipeline(name, df):\n",
    "    print(f\"\\n=== Training on {name} Dataset ===\")\n",
    "    word2idx, idx2word = build_vocab(df['article'].tolist() + df['summary'].tolist())\n",
    "    pad_idx = word2idx['<PAD>']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['article'], df['summary'], test_size=0.2)\n",
    "\n",
    "    train_dataset = SummarizationDataset(X_train.tolist(), y_train.tolist(), word2idx)\n",
    "    test_dataset = SummarizationDataset(X_test.tolist(), y_test.tolist(), word2idx)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=32)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model = Seq2SeqAttention(vocab_size=len(word2idx), embedding_dim=256, hidden_dim=256, pad_idx=pad_idx).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(10):\n",
    "        loss = train(model, train_loader, criterion, optimizer, device)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.4f}\")\n",
    "\n",
    "    rouge_scores = evaluate(model, test_loader, idx2word, device)\n",
    "    print(f\"\\nROUGE Scores on {name} Test Set:\")\n",
    "    for key, score in rouge_scores.items():\n",
    "        print(f\"{key}: {score}\")\n",
    "\n",
    "# Rename columns for compatibility\n",
    "bbc_df = bbc_df.rename(columns={\"Article\": \"article\", \"Summary\": \"summary\"})\n",
    "imdb_df = imdb_df.rename(columns={\"Article\": \"article\", \"Summary\": \"summary\"})\n",
    "\n",
    "# Run the pipeline\n",
    "run_summarization_pipeline(\"BBC\", bbc_df)\n",
    "run_summarization_pipeline(\"imdb\", imdb_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "8fdb6109",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['article_id', 'article_sentences', 'preprocessed_sentence', 'label'], dtype='object')\n",
      "Index(['article_id', 'article_sentences', 'preprocessed_sentence', 'label'], dtype='object')\n"
     ]
    }
   ],
   "source": [
    "print(bbc_processed_df.columns)\n",
    "print(imdb_processed_df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cdfabe91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training on BBC Dataset ===\n",
      "Epoch 1 | Loss: 8.2085\n",
      "Epoch 2 | Loss: 7.4750\n"
     ]
    }
   ],
   "source": [
    "# Imports\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from rouge import Rouge\n",
    "import random\n",
    "import numpy as np\n",
    "\n",
    "# Tokenization\n",
    "def tokenize(text):\n",
    "    return text.lower().strip().split()\n",
    "\n",
    "# Vocabulary\n",
    "def build_vocab(texts, min_freq=2):\n",
    "    counter = Counter(word for text in texts for word in tokenize(text))\n",
    "    vocab = ['<PAD>', '<UNK>', '<SOS>', '<EOS>'] + [word for word, count in counter.items() if count >= min_freq]\n",
    "    word2idx = {word: idx for idx, word in enumerate(vocab)}\n",
    "    idx2word = {idx: word for word, idx in word2idx.items()}\n",
    "    return word2idx, idx2word\n",
    "\n",
    "# Dataset\n",
    "class SummarizationDataset(Dataset):\n",
    "    def __init__(self, articles, summaries, word2idx, max_len=80):\n",
    "        self.articles = articles\n",
    "        self.summaries = summaries\n",
    "        self.word2idx = word2idx\n",
    "        self.max_len = max_len\n",
    "\n",
    "    def encode(self, text, add_sos_eos=False):\n",
    "        tokens = tokenize(text)\n",
    "        if add_sos_eos:\n",
    "            tokens = ['<SOS>'] + tokens + ['<EOS>']\n",
    "        ids = [self.word2idx.get(t, self.word2idx['<UNK>']) for t in tokens]\n",
    "        padded = ids[:self.max_len] + [self.word2idx['<PAD>']] * (self.max_len - len(ids))\n",
    "        return torch.tensor(padded[:self.max_len])\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.articles)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        article = self.encode(self.articles[idx])\n",
    "        summary = self.encode(self.summaries[idx], add_sos_eos=True)\n",
    "        return article, summary\n",
    "\n",
    "# Attention\n",
    "class BahdanauAttention(nn.Module):\n",
    "    def __init__(self, enc_dim, dec_dim):\n",
    "        super().__init__()\n",
    "        self.attn = nn.Linear(enc_dim + dec_dim, dec_dim)\n",
    "        self.v = nn.Linear(dec_dim, 1, bias=False)\n",
    "\n",
    "    def forward(self, encoder_outputs, hidden):\n",
    "        src_len = encoder_outputs.shape[1]\n",
    "        hidden = hidden.unsqueeze(1).repeat(1, src_len, 1)\n",
    "        energy = torch.tanh(self.attn(torch.cat((encoder_outputs, hidden), dim=2)))\n",
    "        attn_weights = torch.softmax(self.v(energy).squeeze(2), dim=1)\n",
    "        context = torch.bmm(attn_weights.unsqueeze(1), encoder_outputs).squeeze(1)\n",
    "        return context\n",
    "\n",
    "# Seq2Seq Model\n",
    "class Seq2SeqAttention(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, hidden_dim, pad_idx):\n",
    "        super().__init__()\n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim, padding_idx=pad_idx)\n",
    "        self.encoder = nn.LSTM(embedding_dim, hidden_dim, bidirectional=True, batch_first=True)\n",
    "        self.decoder = nn.LSTMCell(embedding_dim + hidden_dim * 2, hidden_dim * 2)\n",
    "        self.attn = BahdanauAttention(hidden_dim * 2, hidden_dim * 2)\n",
    "        self.fc_out = nn.Linear(hidden_dim * 4, vocab_size)\n",
    "        self.dropout = nn.Dropout(0.4)\n",
    "\n",
    "    def forward(self, src, tgt, teacher_forcing_ratio=0.5):\n",
    "        batch_size, tgt_len = tgt.shape\n",
    "        embedded_src = self.dropout(self.embedding(src))\n",
    "        encoder_outputs, _ = self.encoder(embedded_src)\n",
    "        embedded_tgt = self.dropout(self.embedding(tgt))\n",
    "\n",
    "        h = encoder_outputs.mean(dim=1)\n",
    "        c = h\n",
    "        inputs = embedded_tgt[:, 0]\n",
    "        outputs = []\n",
    "\n",
    "        for t in range(1, tgt_len):\n",
    "            context = self.attn(encoder_outputs, h)\n",
    "            rnn_input = torch.cat((inputs, context), dim=1)\n",
    "            h, c = self.decoder(rnn_input, (h, c))\n",
    "            output = self.fc_out(torch.cat((h, context), dim=1))\n",
    "            outputs.append(output.unsqueeze(1))\n",
    "            teacher_force = random.random() < teacher_forcing_ratio\n",
    "            inputs = embedded_tgt[:, t] if teacher_force else self.embedding(output.argmax(dim=1))\n",
    "\n",
    "        return torch.cat(outputs, dim=1)\n",
    "\n",
    "# Training\n",
    "def train(model, loader, criterion, optimizer, device, epoch):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    tf_ratio = max(0.6 - 0.05 * epoch, 0.3)\n",
    "    for src, tgt in loader:\n",
    "        src, tgt = src.to(device), tgt.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        output = model(src, tgt, teacher_forcing_ratio=tf_ratio)\n",
    "        loss = criterion(output.view(-1, output.shape[-1]), tgt[:, 1:].reshape(-1))\n",
    "        loss.backward()\n",
    "        nn.utils.clip_grad_norm_(model.parameters(), 1)\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(loader)\n",
    "\n",
    "# Evaluation\n",
    "def evaluate(model, loader, idx2word, device):\n",
    "    model.eval()\n",
    "    rouge = Rouge()\n",
    "    predictions, references = [], []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for src, tgt in loader:\n",
    "            src = src.to(device)\n",
    "            batch_size = src.size(0)\n",
    "            embedded = model.embedding(src)\n",
    "            encoder_outputs, _ = model.encoder(embedded)\n",
    "            h = encoder_outputs.mean(dim=1)\n",
    "            c = h\n",
    "            inputs = model.embedding(torch.tensor([2]*batch_size).to(device))  # <SOS>\n",
    "            preds = [[] for _ in range(batch_size)]\n",
    "\n",
    "            for _ in range(30):\n",
    "                context = model.attn(encoder_outputs, h)\n",
    "                rnn_input = torch.cat((inputs, context), dim=1)\n",
    "                h, c = model.decoder(rnn_input, (h, c))\n",
    "                output = model.fc_out(torch.cat((h, context), dim=1))\n",
    "                top1 = output.argmax(dim=1)\n",
    "                inputs = model.embedding(top1)\n",
    "                for i, token in enumerate(top1.tolist()):\n",
    "                    preds[i].append(token)\n",
    "\n",
    "            for pred, ref in zip(preds, tgt[:, 1:].cpu().numpy()):\n",
    "                pred_words = [idx2word.get(i, '') for i in pred if i > 3]\n",
    "                ref_words = [idx2word.get(i, '') for i in ref if i > 3]\n",
    "                predictions.append(\" \".join(pred_words))\n",
    "                references.append(\" \".join(ref_words))\n",
    "\n",
    "    return rouge.get_scores(predictions, references, avg=True)\n",
    "\n",
    "# Main pipeline\n",
    "def run_pipeline(name, df, max_epochs=8):\n",
    "    print(f\"\\n=== Training on {name} Dataset ===\")\n",
    "    word2idx, idx2word = build_vocab(df['article'].tolist() + df['summary'].tolist())\n",
    "    pad_idx = word2idx['<PAD>']\n",
    "\n",
    "    X_train, X_test, y_train, y_test = train_test_split(df['article'], df['summary'], test_size=0.2)\n",
    "    train_dataset = SummarizationDataset(X_train.tolist(), y_train.tolist(), word2idx)\n",
    "    test_dataset = SummarizationDataset(X_test.tolist(), y_test.tolist(), word2idx)\n",
    "    train_loader = DataLoader(train_dataset, batch_size=16, shuffle=True)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=16)\n",
    "\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "    model = Seq2SeqAttention(len(word2idx), 128, 128, pad_idx).to(device)\n",
    "    criterion = nn.CrossEntropyLoss(ignore_index=pad_idx)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        loss = train(model, train_loader, criterion, optimizer, device, epoch)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {loss:.4f}\")\n",
    "\n",
    "    rouge_scores = evaluate(model, test_loader, idx2word, device)\n",
    "    print(f\"\\nROUGE Scores on {name} Test Set:\")\n",
    "    for key, score in rouge_scores.items():\n",
    "        print(f\"{key}: {score}\")\n",
    "\n",
    "# === Load your data ===\n",
    "# Rename your preprocessed DataFrames to match:\n",
    "bbc_df = bbc_df.rename(columns={\"Article\": \"article\", \"Summary\": \"summary\"})\n",
    "imdb_df = imdb_df.rename(columns={\"Article\": \"article\", \"Summary\": \"summary\"})\n",
    "\n",
    "# Run pipeline\n",
    "run_pipeline(\"BBC\", bbc_df)\n",
    "run_pipeline(\"IMDb\", imdb_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
